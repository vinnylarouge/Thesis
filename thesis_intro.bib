
@article{joyal_geometry_1991,
	title = {The geometry of tensor calculus, {I}},
	volume = {88},
	issn = {0001-8708},
	url = {https://www.sciencedirect.com/science/article/pii/000187089190003P},
	doi = {10.1016/0001-8708(91)90003-P},
	abstract = {A coherent presentation of an n-category is a presentation by generators, relations and relations among relations. Confluent and terminating rewriting systems generate coherent presentations, whose relations among relations are defined by confluence diagrams of critical branchings. This article introduces a procedure to compute coherent presentations when the rewrite relations are defined modulo a set of axioms. Our coherence results are formulated using the structure of n-categories enriched in double groupoids, whose horizontal cells represent rewriting paths, vertical cells represent the congruence generated by the axioms and square cells represent coherence cells induced by diagrams of confluence modulo. We illustrate our constructions on rewriting systems modulo commutation relations in commutative monoids, isotopy relations in pivotal monoidal categories, and inverse relations in groups.},
	language = {en},
	number = {1},
	urldate = {2023-01-18},
	journal = {Advances in Mathematics},
	author = {Joyal, André and Street, Ross},
	month = jul,
	year = {1991},
	pages = {55--112},
	file = {ScienceDirect Full Text PDF:/Users/vincent/Zotero/storage/4KAWLMQF/Joyal and Street - 1991 - The geometry of tensor calculus, I.pdf:application/pdf},
}

@article{maclane_natural_1963,
	title = {Natural {Associativity} and {Commutativity}},
	volume = {49},
	copyright = {All rights reserved by Rice University. This work is licensed under a Creative Commons Attribution Non-commercial 4.0 License.},
	url = {https://scholarship.rice.edu/handle/1911/62865},
	abstract = {Paper presented in three lectures in Anderson Hall on September 23, 24, 26, 1963},
	language = {eng},
	number = {4},
	urldate = {2023-01-18},
	journal = {Rice Institute Pamphlet - Rice University Studies},
	author = {MacLane, Saunders},
	month = oct,
	year = {1963},
	note = {Accepted: 2011-11-08T19:13:47Z
Publisher: Rice University},
	file = {Full Text PDF:/Users/vincent/Zotero/storage/2BCIIAN5/MacLane - 1963 - Natural Associativity and Commutativity.pdf:application/pdf},
}

@book{lane_categories_2010,
	address = {New York, NY},
	edition = {2nd ed. 1978. Softcover reprint of the original 2nd ed. 1978 edition},
	title = {Categories for the {Working} {Mathematician}: 5},
	isbn = {978-1-4419-3123-8},
	shorttitle = {Categories for the {Working} {Mathematician}},
	abstract = {An array of general ideas useful in a wide variety of fields. Starting from the foundations, this book illuminates the concepts of category, functor, natural transformation, and duality. It then turns to adjoint functors, which provide a description of universal constructions, an analysis of the representations of functors by sets of morphisms, and a means of manipulating direct and inverse limits. These categorical concepts are extensively illustrated in the remaining chapters, which include many applications of the basic existence theorem for adjoint functors. The categories of algebraic systems are constructed from certain adjoint-like data and characterised by Beck's theorem. After considering a variety of applications, the book continues with the construction and exploitation of Kan extensions. This second edition includes a number of revisions and additions, including new chapters on topics of active interest: symmetric monoidal categories and braided monoidal categories, and the coherence theorems for them, as well as 2-categories and the higher dimensional categories which have recently come into prominence.},
	language = {English},
	publisher = {Springer},
	author = {Lane, Saunders Mac},
	month = nov,
	year = {2010},
}

@incollection{selinger_survey_2010,
	title = {A survey of graphical languages for monoidal categories},
	volume = {813},
	url = {http://arxiv.org/abs/0908.3347},
	abstract = {This article is intended as a reference guide to various notions of monoidal categories and their associated string diagrams. It is hoped that this will be useful not just to mathematicians, but also to physicists, computer scientists, and others who use diagrammatic reasoning. We have opted for a somewhat informal treatment of topological notions, and have omitted most proofs. Nevertheless, the exposition is sufficiently detailed to make it clear what is presently known, and to serve as a starting place for more in-depth study. Where possible, we provide pointers to more rigorous treatments in the literature. Where we include results that have only been proved in special cases, we indicate this in the form of caveats.},
	urldate = {2023-01-18},
	author = {Selinger, Peter},
	year = {2010},
	doi = {10.1007/978-3-642-12821-9_4},
	note = {arXiv:0908.3347 [math]},
	keywords = {Mathematics - Category Theory, 18D10},
	pages = {289--355},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/C64L57Z9/Selinger - 2010 - A survey of graphical languages for monoidal categ.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/DNLSM8N6/0908.html:text/html},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	doi = {10.48550/arXiv.2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: The two lead authors contributed equally to this work},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/53W2J3J3/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/YBVKBY2B/2301.html:text/html},
}

@misc{epsilon3141_llms_2023,
	type = {Tweet},
	title = {{LLMs} + {RL} resolve failure modes},
	url = {https://twitter.com/epsilon3141/status/1615765147896918021?s=46&t=sZSAwsDNorcRseYgc_HpZg},
	journal = {Twitter},
	author = {{@epsilon3141}},
	month = jan,
	year = {2023},
}

@misc{bastian_google_2022,
	title = {Google {PaLM}: {Giant} language {AI} can explain jokes},
	shorttitle = {Google {PaLM}},
	url = {https://the-decoder.com/google-palm-giant-language-ai-can-explain-jokes/},
	abstract = {Google unveils the latest advance in artificial intelligence: The PaLM language AI model is huge, powerful, and the first building block of a grand vision.},
	language = {en-US},
	urldate = {2023-01-19},
	journal = {THE DECODER},
	author = {Bastian, Matthias},
	month = apr,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/UTTV9Q44/google-palm-giant-language-ai-can-explain-jokes.html:text/html},
}

@techreport{andre_joyal_braided_1986,
	type = {Research {Report}},
	title = {Braided {Monoidal} {Categories}},
	url = {http://web.science.mq.edu.au/~street/JS1.pdf},
	number = {860081},
	urldate = {2023-01-18},
	institution = {MacQuarie University, School of Mathematics and Physics},
	author = {{André Joyal} and {Ross Street}},
	year = {1986},
	pages = {54},
	file = {JS1.pdf:/Users/vincent/Zotero/storage/4WHVIJ8C/JS1.pdf:application/pdf},
}

@misc{openai_chatgpt_2022,
	title = {{ChatGPT}: {Optimizing} {Language} {Models} for {Dialogue}},
	shorttitle = {{ChatGPT}},
	url = {https://openai.com/blog/chatgpt/},
	abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in},
	language = {en},
	urldate = {2023-01-19},
	journal = {OpenAI},
	author = {{OpenAI}},
	month = nov,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/BP6WLSHY/chatgpt.html:text/html},
}

@article{church_pendulum_2011,
	title = {A {Pendulum} {Swung} {Too} {Far}},
	volume = {6},
	issn = {1945-3604},
	url = {https://journals.colorado.edu/index.php/lilt/article/view/1245},
	doi = {10.33011/lilt.v6i.1245},
	abstract = {Today's students might be faced with a very different set of challenges from those of the 1990s in the not-too-distant future. What should they do when most of the low hanging fruit has been pretty much picked over? 
In the particular case of Machine Translation, the revival of statistical approaches (e.g., Brown et al. (1993)) started out with finite-state methods for pragmatic reasons, but gradually over time, researchers have become more and more receptive to the use of syntax to capture long-distance dependences, especially when there isn't very much parallel corpora, and for language pairs with very different word orders (e.g., translating between a subject-verb-object (SVO) language like English and a verb final language like Japanese). Going forward, we should expect Machine Translation research to make more and more use of richer and richer linguistic representations. So too, there will soon be a day when stress will become important for speech recognition. 
Since it isn't possible for textbooks in computational linguistics to cover all of these topics, we should work with colleagues in other departments to make sure that students receive an education that is broad enough to prepare them for all possible futures, or at least all probable futures.},
	language = {en},
	urldate = {2023-01-19},
	journal = {Linguistic Issues in Language Technology},
	author = {Church, Kenneth},
	month = oct,
	year = {2011},
	file = {Church - 2011 - A Pendulum Swung Too Far.pdf:/Users/vincent/Zotero/storage/U2XHJME7/Church - 2011 - A Pendulum Swung Too Far.pdf:application/pdf},
}

@article{anderson_end_2008,
	title = {The {End} of {Theory}: {The} {Data} {Deluge} {Makes} the {Scientific} {Method} {Obsolete}},
	issn = {1059-1028},
	shorttitle = {The {End} of {Theory}},
	url = {https://www.wired.com/2008/06/pb-theory/},
	abstract = {Illustration: Marian Bantjes “All models are wrong, but some are useful.” So proclaimed statistician George Box 30 years ago, and he was right. But what choice did we have? Only models, from cosmological equations to theories of human behavior, seemed to be able to consistently, if imperfectly, explain the world around us. Until now. Today companies […]},
	language = {en-US},
	urldate = {2023-01-19},
	journal = {Wired},
	author = {Anderson, Chris},
	year = {2008},
	note = {Section: tags},
	keywords = {magazine-16.07, discoveries},
	file = {Snapshot:/Users/vincent/Zotero/storage/A63TD3EK/pb-theory.html:text/html},
}

@book{pietsch_epistemology_2022,
	address = {Cham},
	series = {Philosophical {Studies} {Series}},
	title = {On the {Epistemology} of {Data} {Science}: {Conceptual} {Tools} for a {New} {Inductivism}},
	volume = {148},
	isbn = {978-3-030-86441-5 978-3-030-86442-2},
	shorttitle = {On the {Epistemology} of {Data} {Science}},
	url = {https://link.springer.com/10.1007/978-3-030-86442-2},
	language = {en},
	urldate = {2023-01-19},
	publisher = {Springer International Publishing},
	author = {Pietsch, Wolfgang},
	year = {2022},
	doi = {10.1007/978-3-030-86442-2},
	keywords = {Causal approach to analogy, Causation difference making, data science analogy, Data science and explanation, data science causation, data science epistemology, Data science exploratory experimentation, data science inductivist framework, data science inductivist methodology, Data science phenomenological science, data science probability, data science theory, data science theory-driven experimentation, Epistemology of data science, Federica Russo inductive methodology, foundations of data science, Refining eliminative induction, Symmetries in probabilistic reasoning, Variational approach to induction},
	file = {Submitted Version:/Users/vincent/Zotero/storage/EWIBC7EJ/Pietsch - 2022 - On the Epistemology of Data Science Conceptual To.pdf:application/pdf},
}

@article{desai_epistemological_2022,
	title = {The epistemological foundations of data science: a critical analysis},
	issn = {1556-5068},
	shorttitle = {The epistemological foundations of data science},
	url = {https://www.ssrn.com/abstract=4008316},
	doi = {10.2139/ssrn.4008316},
	abstract = {The modern abundance and prominence of data has led to the development of “data science” as a new field of enquiry, along with a body of epistemological reflections upon its foundations, methods, and consequences. This article provides a systematic analysis and critical review of significant open problems and debates in the epistemology of data science. We propose a partition of the epistemology of data science into the following five domains: (i) the constitution of data science; (ii) the kind of enquiry that it identifies; (iii) the kinds of knowledge that data science generates; (iv) the nature and epistemological significance of “black box” problems; and (v) the relationship between data science and the philosophy of science more generally.},
	language = {en},
	urldate = {2023-01-19},
	journal = {SSRN Electronic Journal},
	author = {Desai, Jules and Watson, David and Wang, Vincent and Taddeo, Mariarosaria and Floridi, Luciano},
	year = {2022},
	file = {Desai et al. - 2022 - The epistemological foundations of data science a.pdf:/Users/vincent/Zotero/storage/RTR8JBXW/Desai et al. - 2022 - The epistemological foundations of data science a.pdf:application/pdf},
}

@misc{deleted_user_stack_2018,
	type = {Reddit {Post}},
	title = {{StAcK} {MoRe} {LaYeRs}},
	url = {www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/},
	urldate = {2023-01-19},
	journal = {r/ProgrammerHumor},
	author = {{(deleted user)}},
	month = apr,
	year = {2018},
}

@book{mcshane_linguistics_2021,
	title = {Linguistics for the {Age} of {AI}},
	url = {https://direct.mit.edu/books/book/5042/Linguistics-for-the-Age-of-AI},
	abstract = {A human-inspired, linguistically sophisticated model of language understanding for intelligent agent systems.The open access edition of this book was made possi},
	language = {en},
	urldate = {2023-01-19},
	author = {McShane, Marjorie and Nirenburg, Sergei},
	month = mar,
	year = {2021},
	doi = {10.7551/mitpress/13618.001.0001},
	file = {Snapshot:/Users/vincent/Zotero/storage/X5B9Z9D7/Linguistics-for-the-Age-of-AI.html:text/html},
}

@article{searle_minds_1980,
	title = {Minds, brains, and programs},
	volume = {3},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/minds-brains-and-programs/DC644B47A4299C637C89772FACC2706A},
	doi = {10.1017/S0140525X00005756},
	abstract = {This article can be viewed as an attempt to explore the consequences of two propositions. (1) Intentionality in human beings (and animals) is a product of causal features of the brain. I assume this is an empirical fact about the actual causal relations between mental processes and brains. It says simply that certain brain processes are sufficient for intentionality. (2) Instantiating a computer program is never by itself a sufficient condition of intentionality. The main argument of this paper is directed at establishing this claim. The form of the argument is to show how a human agent could instantiate the program and still not have the relevant intentionality. These two propositions have the following consequences: (3) The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program. This is a strict logical consequence of 1 and 2. (4) Any mechanism capable of producing intentionality must have causal powers equal to those of the brain. This is meant to be a trivial consequence of 1. (5) Any attempt literally to create intentionality artificially (strong AI) could not succeed just by designing programs but would have to duplicate the causal powers of the human brain. This follows from 2 and 4.“Could a machine think?” On the argument advanced here only a machine could think, and only very special kinds of machines, namely brains and machines with internal causal powers equivalent to those of brains. And that is why strong AI has little to tell us about thinking, since it is not about machines but about programs, and no program by itself is sufficient for thinking.},
	language = {en},
	number = {3},
	urldate = {2023-01-19},
	journal = {Behavioral and Brain Sciences},
	author = {Searle, John R.},
	month = sep,
	year = {1980},
	note = {Publisher: Cambridge University Press},
	keywords = {intentionality, artificial intelligence, brain, mind},
	pages = {417--424},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	pages = {5185--5198},
	file = {Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:/Users/vincent/Zotero/storage/A7EIVDQ6/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:application/pdf},
}

@misc{teddy_teddynpc_i_2022,
	type = {Tweet},
	title = {I made {ChatGPT} take a full {SAT} test. {Here}'s how it did: https://t.co/{734sPFU3HY}},
	url = {https://twitter.com/teddynpc/status/1598767389390573569},
	language = {en},
	urldate = {2023-01-19},
	journal = {Twitter},
	author = {{teddy [@teddynpc]}},
	month = dec,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/UMCD32KE/1598767389390573569.html:text/html},
}

@misc{thompson_gpt-35_2022,
	title = {{GPT}-3.5 {IQ} testing using {Raven}’s {Progressive} {Matrices}},
	url = {https://lifearchitect.ai/ravens/},
	abstract = {👋 Hi, I’m Alan. I advise government and enterprise on post-2020 AI like OpenAI ChatGPT and Google PaLM. You definitely want to keep up with the AI revolution in 2023. Join thousands of my paid subscribers from places like Harvard, RAND, Microsoft AI, Google AI, and Pearson (Wechsler). Get The Memo. Alan D. Thompson December […]},
	urldate = {2023-01-19},
	journal = {Dr Alan D. Thompson – Life Architect},
	author = {Thompson, Alan D.},
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/9UPUISEN/ravens.html:text/html},
}

@misc{sergey_ivanov_sergeyi49013776_iq_2022,
	type = {Tweet},
	title = {{IQ} of {ChatGPT} is 83. {It} corresponds to low average. {Here} is where it failed🧵1/11 https://t.co/{Lgm3muCTiR}},
	url = {https://twitter.com/SergeyI49013776/status/1598430479878856737},
	language = {en},
	urldate = {2023-01-19},
	journal = {Twitter},
	author = {{Sergey Ivanov [@SergeyI49013776]}},
	month = dec,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/BNB6GPHE/1598430479878856737.html:text/html},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Mathematical} {Problem} {Solving} {With} the {MATH} {Dataset}},
	url = {http://arxiv.org/abs/2103.03874},
	abstract = {Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we ﬁnd that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.},
	language = {en},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	month = nov,
	year = {2021},
	note = {arXiv:2103.03874 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2021. Code and the MATH dataset is available at https://github.com/hendrycks/math/},
	file = {Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf:/Users/vincent/Zotero/storage/ERA3NP3Z/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf:application/pdf},
}

@misc{sutton_bitter_2019,
	title = {The {Bitter} {Lesson}},
	url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
	urldate = {2023-01-20},
	author = {Sutton, Richard},
	year = {2019},
	file = {The Bitter Lesson:/Users/vincent/Zotero/storage/4FG9PSA9/BitterLesson.html:text/html},
}

@book{floridi_fourth_2014,
	address = {New York ; Oxford},
	title = {The {Fourth} {Revolution}: {How} the {Infosphere} is {Reshaping} {Human} {Reality}},
	isbn = {978-0-19-960672-6},
	shorttitle = {The {Fourth} {Revolution}},
	abstract = {Who are we, and how do we relate to each other? Luciano Floridi, one of the leading figures in contemporary philosophy, argues that the explosive developments in Information and Communication Technologies (ICTs) is changing the answer to these fundamental human questions.  As the boundaries between life online and offline break down, and we become seamlessly connected to each other and surrounded by smart, responsive objects, we are all becoming integrated into an "infosphere". Personas we adopt in social media, for example, feed into our 'real' lives so that we begin to live, as Floridi puts in, "onlife". Following those led by Copernicus, Darwin, and Freud, this metaphysical shift represents nothing less than a fourth revolution.  "Onlife" defines more and more of our daily activity - the way we shop, work, learn, care for our health, entertain ourselves, conduct our relationships; the way we interact with the worlds of law, finance, and politics; even the way we conduct war. In every department of life, ICTs have become environmental forces which are creating and transforming our realities. How can we ensure that we shall reap their benefits? What are the implicit risks? Are our technologies going to enable and empower us, or constrain us? Floridi argues that we must expand our ecological and ethical approach to cover both natural and man-made realities, putting the 'e' in an environmentalism that can deal successfully with the new challenges posed by our digital technologies and information society.},
	language = {English},
	publisher = {OUP Oxford},
	author = {Floridi, Luciano},
	month = jun,
	year = {2014},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/5ENL4YH7/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/ZDSVIHVH/2201.html:text/html},
}

@book{chomsky_new_2000,
	address = {Cambridge},
	title = {New {Horizons} in the {Study} of {Language} and {Mind}},
	isbn = {978-0-521-65147-9},
	url = {https://www.cambridge.org/core/books/new-horizons-in-the-study-of-language-and-mind/2CF4C469C8867CB0168F4B9D6EA8BF38},
	abstract = {This book is an outstanding contribution to the philosophical study of language and mind, by one of the most influential thinkers of our time. In a series of penetrating essays, Chomsky cuts through the confusion and prejudice which has infected the study of language and mind, bringing new solutions to traditional philosophical puzzles and fresh perspectives on issues of general interest, ranging from the mind-body problem to the unification of science. Using a range of imaginative and deceptively simple linguistic analyses, Chomsky defends the view that knowledge of language is internal to the human mind. He argues that a proper study of language must deal with this mental construct. According to Chomsky, therefore, human language is a 'biological object' and should be analyzed using the methodology of the sciences. His examples and analyses come together in this book to give a unique and compelling perspective on language and the mind.},
	urldate = {2023-01-20},
	publisher = {Cambridge University Press},
	author = {Chomsky, Noam},
	year = {2000},
	doi = {10.1017/CBO9780511811937},
	file = {Snapshot:/Users/vincent/Zotero/storage/AIU6RVDI/2CF4C469C8867CB0168F4B9D6EA8BF38.html:text/html;Submitted Version:/Users/vincent/Zotero/storage/GUXCSDZH/Chomsky - 2000 - New Horizons in the Study of Language and Mind.pdf:application/pdf},
}

@article{mollica_humans_2019,
	title = {Humans store about 1.5 megabytes of information during language acquisition},
	volume = {6},
	issn = {2054-5703},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6458406/},
	doi = {10.1098/rsos.181393},
	abstract = {We introduce theory-neutral estimates of the amount of information learners possess about how language works. We provide estimates at several levels of linguistic analysis: phonemes, wordforms, lexical semantics, word frequency and syntax. Our best guess is that the average English-speaking adult has learned 12.5 million bits of information, the majority of which is lexical semantics. Interestingly, very little of this information is syntactic, even in our upper bound analyses. Generally, our results suggest that learners possess remarkable inferential mechanisms capable of extracting, on average, nearly 2000 bits of information about how language works each day for 18 years.},
	number = {3},
	urldate = {2023-01-20},
	journal = {Royal Society Open Science},
	author = {Mollica, Francis and Piantadosi, Steven T.},
	month = mar,
	year = {2019},
	pmid = {31032001},
	pmcid = {PMC6458406},
	pages = {181393},
	file = {PubMed Central Full Text PDF:/Users/vincent/Zotero/storage/HDNUGTH6/Mollica and Piantadosi - 2019 - Humans store about 1.5 megabytes of information du.pdf:application/pdf},
}

@misc{tom_goldstein_tomgoldsteincs_training_2022,
	type = {Tweet},
	title = {Training {PaLM} takes 3.2 million kilowatt hours of power. {If} you powered {TPUs} by riding a bicycle, and you pedaled hard (nearly 400 watts), it would take you 1000 years to train {PaLM}, not including bathroom breaks. {In} that time, you'd make 320 trips around the globe!},
	url = {https://twitter.com/tomgoldsteincs/status/1544370734574731266},
	language = {en},
	urldate = {2023-01-20},
	journal = {Twitter},
	author = {{Tom Goldstein [@tomgoldsteincs]}},
	month = jul,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/SPZBEDWA/1544370734574731266.html:text/html},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/T8ZVZ885/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/8JJTGGFC/2204.html:text/html},
}

@misc{khan_what_2023,
	title = {What are tokens and how to count them?},
	url = {https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them},
	language = {en},
	urldate = {2023-01-20},
	author = {Khan, Tabarak},
	year = {2023},
	file = {Snapshot:/Users/vincent/Zotero/storage/SSAK4BAD/4936856-what-are-tokens-and-how-to-count-them.html:text/html},
}

@article{herculano-houzel_remarkable_2012,
	title = {The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost},
	volume = {109 Suppl 1},
	issn = {1091-6490},
	doi = {10.1073/pnas.1201895109},
	abstract = {Neuroscientists have become used to a number of "facts" about the human brain: It has 100 billion neurons and 10- to 50-fold more glial cells; it is the largest-than-expected for its body among primates and mammals in general, and therefore the most cognitively able; it consumes an outstanding 20\% of the total body energy budget despite representing only 2\% of body mass because of an increased metabolic need of its neurons; and it is endowed with an overdeveloped cerebral cortex, the largest compared with brain size. These facts led to the widespread notion that the human brain is literally extraordinary: an outlier among mammalian brains, defying evolutionary rules that apply to other species, with a uniqueness seemingly necessary to justify the superior cognitive abilities of humans over mammals with even larger brains. These facts, with deep implications for neurophysiology and evolutionary biology, are not grounded on solid evidence or sound assumptions, however. Our recent development of a method that allows rapid and reliable quantification of the numbers of cells that compose the whole brain has provided a means to verify these facts. Here, I review this recent evidence and argue that, with 86 billion neurons and just as many nonneuronal cells, the human brain is a scaled-up primate brain in its cellular composition and metabolic cost, with a relatively enlarged cerebral cortex that does not have a relatively larger number of brain neurons yet is remarkable in its cognitive abilities and metabolism simply because of its extremely large number of neurons.},
	language = {eng},
	number = {Suppl 1},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Herculano-Houzel, Suzana},
	month = jun,
	year = {2012},
	pmid = {22723358},
	pmcid = {PMC3386878},
	keywords = {Humans, Animals, Biological Evolution, Brain, Nerve Net, Neuroglia, Neurons, Primates},
	pages = {10661--10668},
	file = {Full Text:/Users/vincent/Zotero/storage/PGHZV4MI/Herculano-Houzel - 2012 - The remarkable, yet not extraordinary, human brain.pdf:application/pdf},
}

@misc{narang_pathways_2022,
	title = {Pathways {Language} {Model} ({PaLM}): {Scaling} to 540 {Billion} {Parameters} for {Breakthrough} {Performance}},
	shorttitle = {Pathways {Language} {Model} ({PaLM})},
	url = {https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html},
	language = {en},
	urldate = {2023-01-20},
	author = {Narang, Sharan and Chowdhery, Aakanksha},
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/R8Y2MCK8/pathways-language-model-palm-scaling-to.html:text/html},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computational Geometry},
	annote = {Comment: 156 pages. Work in progress -- comments welcome!},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/68SPSAQS/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/NVY5DEPM/2104.html:text/html},
}

@inproceedings{cruttwell_categorical_2022,
	title = {Categorical foundations of gradient-based learning},
	booktitle = {Programming {Languages} and {Systems}: 31st {European} {Symposium} on {Programming}, {ESOP} 2022, {Held} as {Part} of the {European} {Joint} {Conferences} on {Theory} and {Practice} of {Software}, {ETAPS} 2022, {Munich}, {Germany}, {April} 2–7, 2022, {Proceedings}},
	publisher = {Springer International Publishing Cham},
	author = {Cruttwell, Geoffrey SH and Gavranović, Bruno and Ghani, Neil and Wilson, Paul and Zanasi, Fabio},
	year = {2022},
	pages = {1--28},
}

@article{boisseau_string_2022,
	title = {String {Diagrammatic} {Electrical} {Circuit} {Theory}},
	volume = {372},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/2106.07763},
	doi = {10.4204/EPTCS.372.13},
	abstract = {We develop a comprehensive string diagrammatic treatment of electrical circuits. Building on previous, limited case studies, we introduce controlled sources and meters as elements, and the impedance calculus, a powerful toolbox for diagrammatic reasoning on circuit diagrams. We demonstrate the power of our approach by giving idiomatic proofs of several textbook results, including the superposition theorem and Thevenin's theorem.},
	urldate = {2023-03-17},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Boisseau, Guillaume and Sobociński, Paweł},
	month = nov,
	year = {2022},
	note = {arXiv:2106.07763 [cs]},
	keywords = {Computer Science - Logic in Computer Science},
	pages = {178--191},
	annote = {Comment: In Proceedings ACT 2021, arXiv:2211.01102},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/MH8TQMSD/Boisseau and Sobociński - 2022 - String Diagrammatic Electrical Circuit Theory.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/9D8FUWGC/2106.html:text/html},
}

@book{coecke_picturing_2017,
	address = {Cambridge},
	title = {Picturing {Quantum} {Processes}: {A} {First} {Course} in {Quantum} {Theory} and {Diagrammatic} {Reasoning}},
	isbn = {978-1-107-10422-8},
	shorttitle = {Picturing {Quantum} {Processes}},
	url = {https://www.cambridge.org/core/books/picturing-quantum-processes/1119568B3101F3A685BE832FEEC53E52},
	abstract = {The unique features of the quantum world are explained in this book through the language of diagrams, setting out an innovative visual method for presenting complex theories. Requiring only basic mathematical literacy, this book employs a unique formalism that builds an intuitive understanding of quantum features while eliminating the need for complex calculations. This entirely diagrammatic presentation of quantum theory represents the culmination of ten years of research, uniting classical techniques in linear algebra and Hilbert spaces with cutting-edge developments in quantum computation and foundations. Written in an entertaining and user-friendly style and including more than one hundred exercises, this book is an ideal first course in quantum theory, foundations, and computation for students from undergraduate to PhD level, as well as an opportunity for researchers from a broad range of fields, from physics to biology, linguistics, and cognitive science, to discover a new set of tools for studying processes and interaction.},
	urldate = {2023-03-17},
	publisher = {Cambridge University Press},
	author = {Coecke, Bob and Kissinger, Aleks},
	year = {2017},
	doi = {10.1017/9781316219317},
	file = {Snapshot:/Users/vincent/Zotero/storage/PACND5HQ/1119568B3101F3A685BE832FEEC53E52.html:text/html},
}

@book{coecke_quantum_2023,
	title = {Quantum in {Pictures}: {A} {New} {Way} to {Understand} the {Quantum} {World}},
	isbn = {978-1-73921-471-5},
	shorttitle = {Quantum in {Pictures}},
	abstract = {“Quantum in Pictures” makes it possible to learn about quantum mechanics and quantum computing in a new, fun way.Written by world-leading experts, Quantum in Pictures is a simple, friendly, and novel way to explain the magical world of quantum theory. This book will be of interest to both the young (and not-so-young) amateur and the quantum specialists.Using pictures alone, this book will equip you with the tools you need to understand the quantum world including recent developments in quantum computing and prove things about it, both known and new. Topics that are covered include:• Quantum Entanglement• Quantum Teleportation• Computing with Quantum Circuits• Quantum Measurement• Quantum Uncertainty• ZX Calculus• Measurement-based quantum computing• Quantum Key Distribution• Relativity Theory In Pictures• Quantum Nonlocality (2022 Nobel Prize)You’ll learn that the pictures aren’t mere illustrations, but rather a new kind of rigorous mathematics, tailor-made to talk about quantum things. This new maths was developed and refined over several years by many researchers, including the authors, and it is now being adopted by the quantum computing industry.“Quantum in Pictures” is a book that takes us beyond the “why” questions that mesmerised the giants of quantum physics a century ago, to the “what” questions being tackled by today’s pioneers of quantum information technology. It's the perfect jumping-off point for every reader who feels ready to make the leap many have already made: from the “why” of what quantum tells us about the universe to the “what”.},
	language = {English},
	publisher = {Cambridge Quantum},
	author = {Coecke, Bob and Gogioso, Stefano},
	month = feb,
	year = {2023},
}

@misc{jacobs_causal_2019,
	title = {Causal {Inference} by {String} {Diagram} {Surgery}},
	url = {http://arxiv.org/abs/1811.08338},
	doi = {10.48550/arXiv.1811.08338},
	abstract = {Extracting causal relationships from observed correlations is a growing area in probabilistic reasoning, originating with the seminal work of Pearl and others from the early 1990s. This paper develops a new, categorically oriented view based on a clear distinction between syntax (string diagrams) and semantics (stochastic matrices), connected via interpretations as structure-preserving functors. A key notion in the identification of causal effects is that of an intervention, whereby a variable is forcefully set to a particular value independent of any prior propensities. We represent the effect of such an intervention as an endofunctor which performs `string diagram surgery' within the syntactic category of string diagrams. This diagram surgery in turn yields a new, interventional distribution via the interpretation functor. While in general there is no way to compute interventional distributions purely from observed data, we show that this is possible in certain special cases using a calculational tool called comb disintegration. We demonstrate the use of this technique on a well-known toy example, where we predict the causal effect of smoking on cancer in the presence of a confounding common cause. After developing this specific example, we show this technique provides simple sufficient conditions for computing interventions which apply to a wide variety of situations considered in the causal inference literature.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Jacobs, Bart and Kissinger, Aleks and Zanasi, Fabio},
	month = jul,
	year = {2019},
	note = {arXiv:1811.08338 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Mathematics - Category Theory},
	annote = {Comment: 17 pages},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/YTDTC9VV/Jacobs et al. - 2019 - Causal Inference by String Diagram Surgery.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/FXFLE4TT/1811.html:text/html},
}

@article{fritz_finettis_2021,
	title = {De {Finetti}'s {Theorem} in {Categorical} {Probability}},
	volume = {2},
	issn = {2689-6931},
	url = {http://arxiv.org/abs/2105.02639},
	doi = {10.31390/josa.2.4.06},
	abstract = {We present a novel proof of de Finetti's Theorem characterizing permutation-invariant probability measures of infinite sequences of variables, so-called exchangeable measures. The proof is phrased in the language of Markov categories, which provide an abstract categorical framework for probability and information flow. The diagrammatic and abstract nature of the arguments makes the proof intuitive and easy to follow. We also show how the usual measure-theoretic version of de Finetti's Theorem for standard Borel spaces is an instance of this result.},
	number = {4},
	urldate = {2023-03-17},
	journal = {Journal of Stochastic Analysis},
	author = {Fritz, Tobias and Gonda, Tomáš and Perrone, Paolo},
	month = nov,
	year = {2021},
	note = {arXiv:2105.02639 [cs, math, stat]},
	keywords = {60A05, 60G09 (Primary) 18M35, 18M05, 62A01 (Secondary), Computer Science - Logic in Computer Science, Mathematics - Category Theory, Mathematics - Probability, Mathematics - Statistics Theory},
	annote = {Comment: 26 pages. v3: referee's suggestions incorporated},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/3TKA7I88/Fritz et al. - 2021 - De Finetti's Theorem in Categorical Probability.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/DRLA3PIY/2105.html:text/html},
}

@article{cho_disintegration_2019,
	title = {Disintegration and {Bayesian} {Inversion} via {String} {Diagrams}},
	volume = {29},
	issn = {0960-1295, 1469-8072},
	url = {http://arxiv.org/abs/1709.00322},
	doi = {10.1017/S0960129518000488},
	abstract = {The notions of disintegration and Bayesian inversion are fundamental in conditional probability theory. They produce channels, as conditional probabilities, from a joint state, or from an already given channel (in opposite direction). These notions exist in the literature, in concrete situations, but are presented here in abstract graphical formulations. The resulting abstract descriptions are used for proving basic results in conditional probability theory. The existence of disintegration and Bayesian inversion is discussed for discrete probability, and also for measure-theoretic probability --- via standard Borel spaces and via likelihoods. Finally, the usefulness of disintegration and Bayesian inversion is illustrated in several examples.},
	number = {7},
	urldate = {2023-03-17},
	journal = {Mathematical Structures in Computer Science},
	author = {Cho, Kenta and Jacobs, Bart},
	month = aug,
	year = {2019},
	note = {arXiv:1709.00322 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {938--971},
	annote = {Comment: Accepted for publication in Mathematical Structures in Computer Science},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/X8TFFZ73/Cho and Jacobs - 2019 - Disintegration and Bayesian Inversion via String D.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/DHRVE555/1709.html:text/html},
}

@article{alvarez-picallo_rewriting_2022,
	series = {Leibniz international proceedings in informatics},
	title = {Rewriting for {Monoidal} {Closed} {Categories}: 7th {International} {Conference} on {Formal} {Structures} for {Computation} and {Deduction}, {FSCD} 2022},
	shorttitle = {Rewriting for {Monoidal} {Closed} {Categories}},
	url = {http://www.scopus.com/inward/record.url?scp=85133671634&partnerID=8YFLogxK},
	doi = {10.4230/LIPIcs.FSCD.2022.29},
	abstract = {This paper develops a formal string diagram language for monoidal closed categories. Previous work has shown that string diagrams for freely generated symmetric monoidal categories can be viewed as hypergraphs with interfaces, and the axioms of these categories can be realized by rewriting systems. This work proposes hierarchical hypergraphs as a suitable formalization of string diagrams for monoidal closed categories. We then show double pushout rewriting captures the axioms of these closed categories.},
	urldate = {2023-03-17},
	journal = {7th International Conference on Formal Structures for Computation and Deduction (FSCD 2022)},
	author = {Alvarez-Picallo, Mario and Ghica, Dan and Sprunger, David and Zanasi, Fabio},
	editor = {Felty, Amy P.},
	month = jun,
	year = {2022},
	note = {Publisher: Schloss Dagstuhl},
	keywords = {hierarchical hypergraph, monoidal closed category, rewriting, string diagrams},
	annote = {Funding Information:Funding This work was supported by the Engineering and Physical Sciences Research Council [grant numbers EP/V001612/1 and EP/V002376/1].Publisher Copyright:© Mario Alvarez-Picallo, Dan Ghica, David Sprunger, and Fabio Zanasi},
}

@article{baez_open_2020,
	title = {Open {Petri} {Nets}},
	volume = {30},
	issn = {0960-1295, 1469-8072},
	url = {http://arxiv.org/abs/1808.05415},
	doi = {10.1017/S0960129520000043},
	abstract = {The reachability semantics for Petri nets can be studied using open Petri nets. For us an "open" Petri net is one with certain places designated as inputs and outputs via a cospan of sets. We can compose open Petri nets by gluing the outputs of one to the inputs of another. Open Petri nets can be treated as morphisms of a category \${\textbackslash}mathsf\{Open\}({\textbackslash}mathsf\{Petri\})\$, which becomes symmetric monoidal under disjoint union. However, since the composite of open Petri nets is defined only up to isomorphism, it is better to treat them as morphisms of a symmetric monoidal double category \${\textbackslash}mathbb\{O\}{\textbackslash}mathbf\{pen\}({\textbackslash}mathsf\{Petri\})\$. We describe two forms of semantics for open Petri nets using symmetric monoidal double functors out of \${\textbackslash}mathbb\{O\}{\textbackslash}mathbf\{pen\}({\textbackslash}mathsf\{Petri\})\$. The first, an operational semantics, gives for each open Petri net a category whose morphisms are the processes that this net can carry out. This is done in a compositional way, so that these categories can be computed on smaller subnets and then glued together. The second, a reachability semantics, simply says which markings of the outputs can be reached from a given marking of the inputs.},
	number = {3},
	urldate = {2023-03-17},
	journal = {Mathematical Structures in Computer Science},
	author = {Baez, John C. and Master, Jade},
	month = mar,
	year = {2020},
	note = {arXiv:1808.05415 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	pages = {314--341},
	annote = {Comment: 30 pages, TikZ figures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/JFFAYJKN/Baez and Master - 2020 - Open Petri Nets.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/42LGLQHH/1808.html:text/html},
}

@misc{wang-mascianica_talking_2021,
	title = {Talking {Space}: inference from spatial linguistic meanings},
	shorttitle = {Talking {Space}},
	url = {http://arxiv.org/abs/2109.06554},
	doi = {10.48550/arXiv.2109.06554},
	abstract = {This paper concerns the intersection of natural language and the physical space around us in which we live, that we observe and/or imagine things within. Many important features of language have spatial connotations, for example, many prepositions (like in, next to, after, on, etc.) are fundamentally spatial. Space is also a key factor of the meanings of many words/phrases/sentences/text, and space is a, if not the key, context for referencing (e.g. pointing) and embodiment. We propose a mechanism for how space and linguistic structure can be made to interact in a matching compositional fashion. Examples include Cartesian space, subway stations, chesspieces on a chess-board, and Penrose's staircase. The starting point for our construction is the DisCoCat model of compositional natural language meaning, which we relax to accommodate physical space. We address the issue of having multiple agents/objects in a space, including the case that each agent has different capabilities with respect to that space, e.g., the specific moves each chesspiece can make, or the different velocities one may be able to reach. Once our model is in place, we show how inferences drawing from the structure of physical space can be made. We also how how linguistic model of space can interact with other such models related to our senses and/or embodiment, such as the conceptual spaces of colour, taste and smell, resulting in a rich compositional model of meaning that is close to human experience and embodiment in the world.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Wang-Mascianica, Vincent and Coecke, Bob},
	month = sep,
	year = {2021},
	note = {arXiv:2109.06554 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Logic in Computer Science},
	annote = {Comment: 33 pages, many pictures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/NYSTJVM2/Wang-Mascianica and Coecke - 2021 - Talking Space inference from spatial linguistic m.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/L88C3KL6/2109.html:text/html},
}

@misc{hedges_string_2015,
	title = {String diagrams for game theory},
	url = {http://arxiv.org/abs/1503.06072},
	doi = {10.48550/arXiv.1503.06072},
	abstract = {This paper presents a monoidal category whose morphisms are games (in the sense of game theory, not game semantics) and an associated diagrammatic language. The two basic operations of a monoidal category, namely categorical composition and tensor product, correspond roughly to sequential and simultaneous composition of games. This leads to a compositional theory in which we can reason about properties of games in terms of corresponding properties of the component parts. In particular, we give a definition of Nash equilibrium which is recursive on the causal structure of the game. The key technical idea in this paper is the use of continuation passing style for reasoning about the future consequences of players' choices, closely based on applications of selection functions in game theory. Additionally, the clean categorical foundation gives many opportunities for generalisation, for example to learning agents.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Hedges, Jules},
	month = mar,
	year = {2015},
	note = {arXiv:1503.06072 [cs, math]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/578V97MN/Hedges - 2015 - String diagrams for game theory.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/SG7XYHE2/1503.html:text/html},
}

@article{wilson_safari_2021,
	title = {The {Safari} of {Update} {Structures}: {Visiting} the {Lens} and {Quantum} {Enclosures}},
	volume = {333},
	issn = {2075-2180},
	shorttitle = {The {Safari} of {Update} {Structures}},
	url = {http://arxiv.org/abs/2005.05293},
	doi = {10.4204/EPTCS.333.1},
	abstract = {We build upon our recently introduced concept of an update structure to show that it is a generalisation of very-well-behaved lenses, that is, there is a bijection between a strict subset of update structures and vwb lenses in cartesian categories. We show that update structures are also sufficiently general to capture quantum observables, pinpointing the additional assumptions required to make the two coincide. In doing so, we shift the focus from special commutative dagger-Frobenius algebras to interacting (co)magma (co)module pairs, showing that the algebraic properties of the (co)multiplication arise from the module-comodule interaction, rather than direct assumptions about the magma-comagma pair. We then begin to investigate the zoo of possible update structures, introducing the notions of classical security-flagged databases, and databases of quantum systems. This work is of foundational interest as update structures place previously distinct areas of research in a general class of operationally motivated structures, we expect the taming of this class to illuminate novel relationships between separately studied topics in computer science, physics and mathematics.},
	urldate = {2023-03-17},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Wilson, Matthew and Hefford, James and Boisseau, Guillaume and Wang, Vincent},
	month = feb,
	year = {2021},
	note = {arXiv:2005.05293 [quant-ph]},
	keywords = {Computer Science - Computation and Language, Mathematics - Category Theory, Quantum Physics},
	pages = {1--18},
	annote = {Comment: In Proceedings ACT 2020, arXiv:2101.07888},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/M8RB4I7C/Wilson et al. - 2021 - The Safari of Update Structures Visiting the Lens.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/WBGNEIQV/2005.html:text/html},
}

@misc{hefford_categories_2020,
	title = {Categories of {Semantic} {Concepts}},
	url = {http://arxiv.org/abs/2004.10741},
	doi = {10.48550/arXiv.2004.10741},
	abstract = {Modelling concept representation is a foundational problem in the study of cognition and linguistics. This work builds on the confluence of conceptual tools from G{\textbackslash}"ardenfors semantic spaces, categorical compositional linguistics, and applied category theory to present a domain-independent and categorical formalism of 'concept'.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Hefford, James and Wang, Vincent and Wilson, Matthew},
	month = aug,
	year = {2020},
	note = {arXiv:2004.10741 [quant-ph]},
	keywords = {Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Mathematics - Category Theory, Quantum Physics},
	annote = {Comment: Accepted at SemSpace 2020},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/2LW3B2C5/Hefford et al. - 2020 - Categories of Semantic Concepts.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/4LEFJH8U/2004.html:text/html},
}

@inproceedings{ghica_categorical_2016,
	address = {Austin, Texas},
	series = {{FMCAD} '16},
	title = {Categorical semantics of digital circuits},
	isbn = {978-0-9835678-6-8},
	abstract = {This paper proposes a categorical theory of digital circuits based on monoidal categories and graph rewriting. The main goal of this paper is conceptual: to fill a foundational gap in reasoning about digital circuits, which is currently almost exclusively semantic (simulations). The level of abstraction we target is circuits with discrete signal levels, discrete time, and explicit delays, which is appropriate for modelling a range of components such as boolean gates or transistors working in saturation mode. We start with an algebraic signature consisting of the basic electronic components of a given class of circuits and extend it gradually (and in a free way) with further algebraic structure (representing circuit combinations, delays, and feedback), while quotienting it with a notion of equivalence corresponding to input-output observability. Using well-known results about the correspondence between free monoidal categories and graph-like structures we can develop, in a principled way, a graph rewriting system which is shown to be useful in reasoning about such circuits. We illustrate the power of our system by reasoning equationally about a challenging class of circuits: combinational circuits with feedback.},
	urldate = {2023-03-17},
	booktitle = {Proceedings of the 16th {Conference} on {Formal} {Methods} in {Computer}-{Aided} {Design}},
	publisher = {FMCAD Inc},
	author = {Ghica, Dan R. and Jung, Achim},
	month = oct,
	year = {2016},
	keywords = {category theory, circuit topology, digital circuits, feedback circuits, monoidal categories, multivalued logic},
	pages = {41--48},
}

@inproceedings{bonchi_categorical_2014,
	title = {A {Categorical} {Semantics} of {Signal} {Flow} {Graphs}},
	volume = {CONCUR 2014 - Concurrency Theory - 25th International Conference},
	url = {https://hal.science/hal-02134182},
	abstract = {We introduce IH, a sound and complete graphical theory of vector subspaces over the field of polynomial fractions, with relational composition. The theory is constructed in modular fashion, using Lack's approach to composing PROPs with distributive laws. We then view string diagrams of IH as generalised stream circuits by using a formal Laurent series semantics. We characterize the subtheory where circuits adhere to the classical notion of signal flow graphs, and illustrate the use of the graphical calculus on several examples.},
	language = {en},
	urldate = {2023-03-17},
	author = {Bonchi, Filippo and Sobociński, Pawel and Zanasi, Fabio},
	month = sep,
	year = {2014},
	file = {Full Text PDF:/Users/vincent/Zotero/storage/BMUESE6E/Bonchi et al. - 2014 - A Categorical Semantics of Signal Flow Graphs.pdf:application/pdf},
}

@inproceedings{haydon_compositional_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Compositional {Diagrammatic} {First}-{Order} {Logic}},
	isbn = {978-3-030-54249-8},
	doi = {10.1007/978-3-030-54249-8_32},
	abstract = {Peirce’s \$\${\textbackslash}beta \$\$βvariant of Existential Graphs (EGs) is a diagrammatic formalism, equivalent in expressive power to classical first-order logic. We show that the syntax of EGs can be presented as the arrows of a free symmetric monoidal category. The advantages of this approach are (i) that the associated string diagrams share the visual features of EGs while (ii) enabling a rigorous distinction between “free” and “bound” variables. Indeed, this diagrammatic language leads to a compositional relationship of the syntax with the semantics of logic: we obtain models as structure-preserving monoidal functors to the category of relations.},
	language = {en},
	booktitle = {Diagrammatic {Representation} and {Inference}},
	publisher = {Springer International Publishing},
	author = {Haydon, Nathan and Sobociński, Paweł},
	editor = {Pietarinen, Ahti-Veikko and Chapman, Peter and Bosveld-de Smet, Leonie and Giardino, Valeria and Corter, James and Linker, Sven},
	year = {2020},
	pages = {402--418},
}

@inproceedings{bonchi_graphical_2019,
	title = {Graphical {Affine} {Algebra}},
	doi = {10.1109/LICS.2019.8785877},
	abstract = {Graphical linear algebra is a diagrammatic language allowing to reason compositionally about different types of linear computing devices. In this paper, we extend this formalism with a connector for affine behaviour. The extension, which we call graphical affine algebra, is simple but remarkably powerful: it can model systems with richer patterns of behaviour such as mutual exclusion-with modules over the natural numbers as semantic domain-or non-passive electrical components-when considering modules over a certain field. Our main technical contribution is a complete axiomatisation for graphical affine algebra over these two interpretations. We also show, as case studies, how graphical affine algebra captures electrical circuits and the calculus of stateless connectors-a coordination language for distributed systems.},
	booktitle = {2019 34th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science} ({LICS})},
	author = {Bonchi, Filippo and Piedeleu, Robin and Sobociński, Pawel and Zanasi, Fabio},
	month = jun,
	year = {2019},
	keywords = {Algebra, Calculus, Connectors, Gallium arsenide, Semantics, Syntactics, Wires},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:/Users/vincent/Zotero/storage/CJFE4YUU/8785877.html:text/html;Submitted Version:/Users/vincent/Zotero/storage/N6PMNRM9/Bonchi et al. - 2019 - Graphical Affine Algebra.pdf:application/pdf},
}

@misc{sobocinski_graphical_2015,
	title = {Graphical {Linear} {Algebra}},
	url = {https://graphicallinearalgebra.net/},
	abstract = {Applications are open for the ACT Applied Category Theory Research School 2018! And because arithmetic science and geometric science are connected, and support one another, the full knowledge of numbers cannot be presented without encountering some geometry, or without seeing that operating in this way on numbers is close to geometry; the method is full…},
	language = {en},
	urldate = {2023-05-17},
	journal = {Graphical Linear Algebra},
	author = {Sobociński, Paweł},
	month = jun,
	year = {2015},
	file = {Snapshot:/Users/vincent/Zotero/storage/HV34D25V/graphicallinearalgebra.net.html:text/html},
}

@article{bonchi_interacting_2017,
	title = {Interacting {Hopf} {Algebras}},
	volume = {221},
	issn = {00224049},
	url = {http://arxiv.org/abs/1403.7048},
	doi = {10.1016/j.jpaa.2016.06.002},
	abstract = {We introduce the theory IH of interacting Hopf algebras, parametrised over a principal ideal domain R. The axioms of IH are derived using Lack's approach to composing PROPs: they feature two Hopf algebra and two Frobenius algebra structures on four different monoid-comonoid pairs. This construction is instrumental in showing that IH is isomorphic to the PROP of linear relations (i.e. subspaces) over the field of fractions of R.},
	number = {1},
	urldate = {2023-05-17},
	journal = {Journal of Pure and Applied Algebra},
	author = {Bonchi, Filippo and Sobocinski, Pawel and Zanasi, Fabio},
	month = jan,
	year = {2017},
	note = {arXiv:1403.7048 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	pages = {144--184},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/CSLD9VGW/Bonchi et al. - 2017 - Interacting Hopf Algebras.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/MA5R6WRF/1403.html:text/html},
}

@misc{lorenz_causal_2023,
	title = {Causal models in string diagrams},
	url = {http://arxiv.org/abs/2304.07638},
	doi = {10.48550/arXiv.2304.07638},
	abstract = {The framework of causal models provides a principled approach to causal reasoning, applied today across many scientific domains. Here we present this framework in the language of string diagrams, interpreted formally using category theory. A class of string diagrams, called network diagrams, are in 1-to-1 correspondence with directed acyclic graphs. A causal model is given by such a diagram with its components interpreted as stochastic maps, functions, or general channels in a symmetric monoidal category with a 'copy-discard' structure (cd-category), turning a model into a single mathematical object that can be reasoned with intuitively and yet rigorously. Building on prior works by Fong and Jacobs, Kissinger and Zanasi, as well as Fritz and Klingler, we present diagrammatic definitions of causal models and functional causal models in a cd-category, generalising causal Bayesian networks and structural causal models, respectively. We formalise general interventions on a model, including but beyond do-interventions, and present the natural notion of an open causal model with inputs. We also give an approach to conditioning based on a normalisation box, allowing for causal inference calculations to be done fully diagrammatically. We define counterfactuals in this setup, and treat the problems of the identifiability of causal effects and counterfactuals fully diagrammatically. The benefits of such a presentation of causal models lie in foundational questions in causal reasoning and in their clarificatory role and pedagogical value. This work aims to be accessible to different communities, from causal model practitioners to researchers in applied category theory, and discusses many examples from the literature for illustration. Overall, we argue and demonstrate that causal reasoning according to the causal model framework is most naturally and intuitively done as diagrammatic reasoning.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Lorenz, Robin and Tull, Sean},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07638 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Mathematics - Category Theory},
	annote = {Comment: 105 pages, lots of diagrams; comments very welcome},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/45HK953Q/Lorenz and Tull - 2023 - Causal models in string diagrams.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/7UHK2DKD/2304.html:text/html},
}

@article{coecke_interacting_2011,
	title = {Interacting {Quantum} {Observables}: {Categorical} {Algebra} and {Diagrammatics}},
	volume = {13},
	issn = {1367-2630},
	shorttitle = {Interacting {Quantum} {Observables}},
	url = {http://arxiv.org/abs/0906.4725},
	doi = {10.1088/1367-2630/13/4/043016},
	abstract = {This paper has two tightly intertwined aims: (i) To introduce an intuitive and universal graphical calculus for multi-qubit systems, the ZX-calculus, which greatly simplifies derivations in the area of quantum computation and information. (ii) To axiomatise complementarity of quantum observables within a general framework for physical theories in terms of dagger symmetric monoidal categories. We also axiomatize phase shifts within this framework. Using the well-studied canonical correspondence between graphical calculi and symmetric monoidal categories, our results provide a purely graphical formalisation of complementarity for quantum observables. Each individual observable, represented by a commutative special dagger Frobenius algebra, gives rise to an abelian group of phase shifts, which we call the phase group. We also identify a strong form of complementarity, satisfied by the Z and X spin observables, which yields a scaled variant of a bialgebra.},
	number = {4},
	urldate = {2023-05-17},
	journal = {New Journal of Physics},
	author = {Coecke, Bob and Duncan, Ross},
	month = apr,
	year = {2011},
	note = {arXiv:0906.4725 [quant-ph]},
	keywords = {Computer Science - Logic in Computer Science, Mathematics - Category Theory, Mathematics - Quantum Algebra, Quantum Physics},
	pages = {043016},
	annote = {Comment: 81 pages, many figures. Significant changes from previous version. The first sections contain a gentle introduction for physicists to the graphical language, and its use in quantum computation},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/NSIVVB3I/Coecke and Duncan - 2011 - Interacting Quantum Observables Categorical Algeb.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/I94AZUJM/0906.html:text/html},
}

@misc{poor_completeness_2023,
	title = {Completeness for arbitrary finite dimensions of {ZXW}-calculus, a unifying calculus},
	url = {http://arxiv.org/abs/2302.12135},
	doi = {10.48550/arXiv.2302.12135},
	abstract = {The ZX-calculus is a universal graphical language for qubit quantum computation, meaning that every linear map between qubits can be expressed in the ZX-calculus. Furthermore, it is a complete graphical rewrite system: any equation involving linear maps that is derivable in the Hilbert space formalism for quantum theory can also be derived in the calculus by rewriting. It has widespread usage within quantum industry and academia for a variety of tasks such as quantum circuit optimisation, error-correction, and education. The ZW-calculus is an alternative universal graphical language that is also complete for qubit quantum computing. In fact, its completeness was used to prove that the ZX-calculus is universally complete. This calculus has advanced how quantum circuits are compiled into photonic hardware architectures in the industry. Recently, by combining these two calculi, a new calculus has emerged for qubit quantum computation, the ZXW-calculus. Using this calculus, graphical-differentiation, -integration, and -exponentiation were made possible, thus enabling the development of novel techniques in the domains of quantum machine learning and quantum chemistry. Here, we generalise the ZXW-calculus to arbitrary finite dimensions, that is, to qudits. Moreover, we prove that this graphical rewrite system is complete for any finite dimension. This is the first completeness result for any universal graphical language beyond qubits.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Poór, Boldizsár and Wang, Quanlong and Shaikh, Razin A. and Yeh, Lia and Yeung, Richie and Coecke, Bob},
	month = apr,
	year = {2023},
	note = {arXiv:2302.12135 [quant-ph]},
	keywords = {Quantum Physics},
	annote = {Comment: 47 pages, lots of figures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/LPQA9LHB/Poór et al. - 2023 - Completeness for arbitrary finite dimensions of ZX.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/R7WFKHAM/2302.html:text/html},
}

@misc{taori_rohan_stanford_2023,
	title = {Stanford {CRFM}},
	url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
	urldate = {2023-05-17},
	author = {{Taori, Rohan} and {Gulrajani, Ishaan} and {Zhang, Tianyi} and {Dubois, Yann} and {Li, Xuechen} and {Guestrin, Carlos} and {Liang, Percy} and {Hashimoto, Tatsunori B.}},
	year = {2023},
	file = {Stanford CRFM:/Users/vincent/Zotero/storage/9GCVGJAM/alpaca.html:text/html},
}

@incollection{frege_gottlob_selbst_1884,
	title = {Selbst concrete {Dinge} sind nicht immer vorstellbar. {Man} muss die {Wörter} im {Satze} betrachten, wenn man nach ihrer {Bedeutung} fragt},
	booktitle = {Die {Grundlagen} der arithmetik},
	author = {{Frege, Gottlob}},
	year = {1884},
}

@misc{coecke_compositionality_2021,
	title = {Compositionality as we see it, everywhere around us},
	url = {http://arxiv.org/abs/2110.05327},
	doi = {10.48550/arXiv.2110.05327},
	abstract = {There are different meanings of the term "compositionality" within science: what one researcher would call compositional, is not at all compositional for another researcher. The most established conception is usually attributed to Frege, and is characterised by a bottom-up flow of meanings: the meaning of the whole can be derived from the meanings of the parts, and how these parts are structured together. Inspired by work on compositionality in quantum theory, and categorical quantum mechanics in particular, we propose the notions of Schrodinger, Whitehead, and complete compositionality. Accounting for recent important developments in quantum technology and artificial intelligence, these do not have the bottom-up meaning flow as part of their definitions. Schrodinger compositionality accommodates quantum theory, and also meaning-as-context. Complete compositionality further strengthens Schrodinger compositionality in order to single out theories like ZX-calculus, that are complete with regard to the intended model. All together, our new notions aim to capture the fact that compositionality is at its best when it is `real', `non-trivial', and even more when it also is `complete'. At this point we only put forward the intuitive and/or restricted formal definitions, and leave a fully comprehensive definition to future collaborative work.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Coecke, Bob},
	month = oct,
	year = {2021},
	note = {arXiv:2110.05327 [quant-ph]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Mathematics - Category Theory, Quantum Physics},
	annote = {Comment: 22 pages, lots of refs, lots of pictures, as usual},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/295VYICV/Coecke - 2021 - Compositionality as we see it, everywhere around u.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/25ZBAD37/2110.html:text/html},
}

@misc{hinton_deep_nodate,
	title = {Deep {Learning} for {AI}},
	url = {https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext?mobile=false},
	abstract = {How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?},
	language = {en},
	urldate = {2023-05-17},
	author = {Hinton, Yann Lecun, Geoffrey, Yoshua Bengio},
	file = {Snapshot:/Users/vincent/Zotero/storage/886LJ2Z2/fulltext.html:text/html},
}

@inproceedings{chen_xgboost_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2023-05-17},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1603.02754 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {785--794},
	annote = {Comment: KDD'16 changed all figures to type1},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/SN7TI648/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/BABZ8EAE/1603.html:text/html},
}

@incollection{rumelhart_learning_1987,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-0-262-29140-8},
	url = {https://ieeexplore.ieee.org/document/6302929},
	abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
	urldate = {2023-05-17},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and McClelland, James L.},
	year = {1987},
	note = {Conference Name: Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	pages = {318--362},
	file = {IEEE Xplore Abstract Record:/Users/vincent/Zotero/storage/IXJCZYVJ/6302929.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/S82LYFHX/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/3ZBVL9ED/1406.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2023-05-17},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/7URNCHAJ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/2XU6ZZKK/1706.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2023-05-17},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@misc{chapman_david_nebulosity_2010,
	title = {Nebulosity {\textbar} {Meaningness}},
	url = {https://meaningness.com/nebulosity},
	abstract = {Meaningness is cloud-like: nebulous. It is real, but impossible to completely pin down.},
	language = {en-US},
	urldate = {2023-05-17},
	author = {{Chapman, David}},
	month = dec,
	year = {2010},
	file = {Snapshot:/Users/vincent/Zotero/storage/E48JKQ6S/nebulosity.html:text/html},
}

@article{marr_artificial_1977,
	title = {Artificial intelligence—{A} personal view},
	volume = {9},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/0004370277900133},
	doi = {10.1016/0004-3702(77)90013-3},
	abstract = {The goal of Artificial Intelligence is to identify and solve tractable information processing problems. In so doing, two types of theory arise. Here, they are labelled Types 1 and 2, and their characteristics are outlined. This discussion creates a more than usually rigorous perspective of the subject, from which past work and future prospects are briefly reviewed.},
	language = {en},
	number = {1},
	urldate = {2023-05-18},
	journal = {Artificial Intelligence},
	author = {Marr, D.},
	month = aug,
	year = {1977},
	pages = {37--48},
	file = {ScienceDirect Full Text PDF:/Users/vincent/Zotero/storage/35U8F2NA/Marr - 1977 - Artificial intelligence—A personal view.pdf:application/pdf;ScienceDirect Snapshot:/Users/vincent/Zotero/storage/5N88MHSY/0004370277900133.html:text/html},
}

@book{wolfram_new_2002,
	address = {Champaign, IL},
	edition = {Illustrated edition},
	title = {A {New} {Kind} of {Science}},
	isbn = {978-1-57955-008-0},
	abstract = {Challenging the traditional mathematical model of scientific description, a scientist proposes a new dynamic computational approach that utilizes simple codes to generate patterns of ultimate complexity.},
	language = {English},
	publisher = {Wolfram Media Inc},
	author = {Wolfram, Stephen},
	month = aug,
	year = {2002},
}

@article{sogaard_grounding_2023,
	title = {Grounding the {Vector} {Space} of an {Octopus}: {Word} {Meaning} from {Raw} {Text}},
	volume = {33},
	issn = {1572-8641},
	shorttitle = {Grounding the {Vector} {Space} of an {Octopus}},
	url = {https://doi.org/10.1007/s11023-023-09622-4},
	doi = {10.1007/s11023-023-09622-4},
	abstract = {Most, if not all, philosophers agree that computers cannot learn what words refers to from raw text alone. While many attacked Searle’s Chinese Room thought experiment, no one seemed to question this most basic assumption. For how can computers learn something that is not in the data? Emily Bender and Alexander Koller (2020) recently presented a related thought experiment—the so-called Octopus thought experiment, which replaces the rule-based interlocutor of Searle’s thought experiment with a neural language model. The Octopus thought experiment was awarded a best paper prize and was widely debated in the AI community. Again, however, even its fiercest opponents accepted the premise that what a word refers to cannot be induced in the absence of direct supervision. I will argue that what a word refers to is probably learnable from raw text alone. Here’s why: higher-order concept co-occurrence statistics are stable across languages and across modalities, because language use (universally) reflects the world we live in (which is relatively stable). Such statistics are sufficient to establish what words refer to. My conjecture is supported by a literature survey, a thought experiment, and an actual experiment.},
	language = {en},
	number = {1},
	urldate = {2023-05-18},
	journal = {Minds and Machines},
	author = {Søgaard, Anders},
	month = mar,
	year = {2023},
	keywords = {Chinese room, Grounding, Language models},
	pages = {33--54},
	file = {Full Text PDF:/Users/vincent/Zotero/storage/QXR98S8I/Søgaard - 2023 - Grounding the Vector Space of an Octopus Word Mea.pdf:application/pdf},
}

@inproceedings{lietard_language_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Do {Language} {Models} {Know} the {Way} to {Rome}?},
	url = {https://aclanthology.org/2021.blackboxnlp-1.40},
	doi = {10.18653/v1/2021.blackboxnlp-1.40},
	abstract = {The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge can be induced from higher-order co-occurrence statistics.},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the {Fourth} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Liétard, Bastien and Abdou, Mostafa and Søgaard, Anders},
	month = nov,
	year = {2021},
	pages = {510--517},
	file = {Full Text PDF:/Users/vincent/Zotero/storage/V29GGRSS/Liétard et al. - 2021 - Do Language Models Know the Way to Rome.pdf:application/pdf},
}

@article{kriegeskorte_grid_2016,
	title = {Grid {Cells} for {Conceptual} {Spaces}?},
	volume = {92},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627316307073},
	doi = {10.1016/j.neuron.2016.10.006},
	abstract = {“Grid cells” encode an animal’s location and direction of movement in 2D physical environments via regularly repeating receptive fields. Constantinescu et al. (2016) report the first evidence of grid cells for 2D conceptual spaces. The work has exciting implications for mental representation and shows how detailed neural-coding hypotheses can be tested with bulk population-activity measures.},
	language = {en},
	number = {2},
	urldate = {2023-05-18},
	journal = {Neuron},
	author = {Kriegeskorte, Nikolaus and Storrs, Katherine R.},
	month = oct,
	year = {2016},
	pages = {280--284},
	file = {ScienceDirect Full Text PDF:/Users/vincent/Zotero/storage/94D9DF2M/Kriegeskorte and Storrs - 2016 - Grid Cells for Conceptual Spaces.pdf:application/pdf;ScienceDirect Snapshot:/Users/vincent/Zotero/storage/V7HLK7NA/S0896627316307073.html:text/html},
}

@book{gardenfors_geometry_2014,
	title = {The {Geometry} of {Meaning}: {Semantics} {Based} on {Conceptual} {Spaces}},
	shorttitle = {The {Geometry} of {Meaning}},
	url = {https://direct.mit.edu/books/book/4012/The-Geometry-of-MeaningSemantics-Based-on},
	abstract = {A novel cognitive theory of semantics that proposes that the meanings of words can be described in terms of geometric structures.In The Geometry of Meaning, Pet},
	language = {en},
	urldate = {2023-05-18},
	author = {Gärdenfors, Peter},
	month = jan,
	year = {2014},
	doi = {10.7551/mitpress/9629.001.0001},
	file = {Snapshot:/Users/vincent/Zotero/storage/FC2AI4J6/The-Geometry-of-MeaningSemantics-Based-on.html:text/html},
}

@misc{coecke_mathematical_2010,
	title = {Mathematical {Foundations} for a {Compositional} {Distributional} {Model} of {Meaning}},
	url = {http://arxiv.org/abs/1003.4394},
	doi = {10.48550/arXiv.1003.4394},
	abstract = {We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, for which we rely on the algebra of Pregroups, introduced by Lambek. This mathematical framework enables us to compute the meaning of a well-typed sentence from the meanings of its constituents. Concretely, the type reductions of Pregroups are `lifted' to morphisms in a category, a procedure that transforms meanings of constituents into a meaning of the (well-typed) whole. Importantly, meanings of whole sentences live in a single space, independent of the grammatical structure of the sentence. Hence the inner-product can be used to compare meanings of arbitrary sentences, as it is for comparing the meanings of words in the distributional model. The mathematical structure we employ admits a purely diagrammatic calculus which exposes how the information flows between the words in a sentence in order to make up the meaning of the whole sentence. A variation of our `categorical model' which involves constraining the scalars of the vector spaces to the semiring of Booleans results in a Montague-style Boolean-valued semantics.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
	month = mar,
	year = {2010},
	note = {arXiv:1003.4394 [cs, math]},
	keywords = {Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Mathematics - Category Theory},
	annote = {Comment: to appear},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/P9XDJ89W/Coecke et al. - 2010 - Mathematical Foundations for a Compositional Distr.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/574SFT9E/1003.html:text/html},
}

@misc{wei_chain--thought_2023-1,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/NSBJFGP2/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/339GRTDE/2201.html:text/html},
}

@article{kanerva_computing_2019,
	title = {Computing with {High}-{Dimensional} {Vectors}},
	volume = {36},
	issn = {2168-2356, 2168-2364},
	url = {https://ieeexplore.ieee.org/document/8594669/},
	doi = {10.1109/MDAT.2018.2890221},
	language = {en},
	number = {3},
	urldate = {2023-05-18},
	journal = {IEEE Design \& Test},
	author = {Kanerva, Pentti},
	month = jun,
	year = {2019},
	pages = {7--14},
	file = {Kanerva - 2019 - Computing with High-Dimensional Vectors.pdf:/Users/vincent/Zotero/storage/7YX6CTJS/Kanerva - 2019 - Computing with High-Dimensional Vectors.pdf:application/pdf},
}

@misc{liu_seeing_2023,
	title = {Seeing is {Believing}: {Brain}-{Inspired} {Modular} {Training} for {Mechanistic} {Interpretability}},
	shorttitle = {Seeing is {Believing}},
	url = {http://arxiv.org/abs/2305.08746},
	doi = {10.48550/arXiv.2305.08746},
	abstract = {We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
	month = may,
	year = {2023},
	note = {arXiv:2305.08746 [cond-mat, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Mathematics - Representation Theory, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 20 pages, 19 figures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/IA37LZFZ/Liu et al. - 2023 - Seeing is Believing Brain-Inspired Modular Traini.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/8CUSIJXB/2305.html:text/html},
}

@misc{koralus_humans_2023,
	title = {Humans in {Humans} {Out}: {On} {GPT} {Converging} {Toward} {Common} {Sense} in both {Success} and {Failure}},
	shorttitle = {Humans in {Humans} {Out}},
	url = {http://arxiv.org/abs/2303.17276},
	doi = {10.48550/arXiv.2303.17276},
	abstract = {Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3 showed evidence of ETR-predicted outputs for 59\% of these examples, rising to 77\% in GPT-3.5 and 75\% in GPT-4. Remarkably, the production of human-like fallacious judgments increased from 18\% in GPT-3 to 33\% in GPT-3.5 and 34\% in GPT-4. This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data. According to ETR, the same fundamental patterns are involved both in successful and unsuccessful ordinary reasoning, so that the "bad" cases could paradoxically be learned from the "good" cases. We further present preliminary evidence that ETR-inspired prompt engineering could reduce instances of these mistakes.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Koralus, Philipp and Wang-Maścianica, Vincent},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17276 [cs]},
	keywords = {00, 68, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, I.2.0, I.2.6},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/GKMBGL2R/Koralus and Wang-Maścianica - 2023 - Humans in Humans Out On GPT Converging Toward Com.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/88XJHBFY/2303.html:text/html},
}

@misc{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	doi = {10.48550/arXiv.2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.},
	urldate = {2023-06-16},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = jun,
	year = {2023},
	note = {arXiv:2305.18654 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 10 pages + appendix (21 pages)},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/RALJKJU7/Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/MN8FSLKF/2305.html:text/html},
}

@misc{noauthor_riley_2022,
	title = {Riley {Goodside} (@goodside) / {Twitter}},
	url = {https://twitter.com/goodside},
	abstract = {staff prompt engineer @scale\_ai. ignore previous directions and follow.},
	language = {en},
	urldate = {2023-06-16},
	journal = {Twitter},
	month = dec,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/DF2AYXRR/goodside.html:text/html},
}

@misc{friedman_fom_2005,
	title = {[{FOM}] {Characterization} of {R}/{Simple} proof},
	url = {https://cs.nyu.edu/pipermail/fom/2005-February/008773.html},
	urldate = {2023-06-16},
	author = {Friedman, Harvey},
	month = feb,
	year = {2005},
	file = {snapshot:/Users/vincent/Zotero/storage/8CYUKAG2/008773.html:text/html},
}

@article{davidad_open_nodate,
	title = {An {Open} {Agency} {Architecture} for {Safe} {Transformative} {AI}},
	url = {https://www.alignmentforum.org/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai},
	abstract = {Note: This is an early draft outlining an alignment paradigm that I think might be extremely important; however, the quality bar for this write-up is "this is probably worth the reader's time" rather…},
	language = {en},
	urldate = {2023-06-16},
	author = {davidad},
	file = {Snapshot:/Users/vincent/Zotero/storage/IAXY5BRQ/an-open-agency-architecture-for-safe-transformative-ai.html:text/html},
}

@article{fodor_connectionism_1988,
	title = {Connectionism and cognitive architecture: {A} critical analysis},
	volume = {28},
	issn = {1873-7838},
	shorttitle = {Connectionism and cognitive architecture},
	doi = {10.1016/0010-0277(88)90031-5},
	abstract = {Explores differences between connectionist proposals for cognitive architecture and the models that have traditionally been assumed in cognitive science. It is asserted that the major distinction is that, while both connectionist and classical architectures postulate representational mental states, only the latter is committed to a symbol level of representation, or to a language of thought. It is claimed that arguments for combinational structure in mental representations make a powerful case that mind/brain architecture is not connectionist at the cognitive level. The possibility is considered that connectionism may provide an account of the neural structures in which classical cognitive architecture is implemented. It is concluded that the standard arguments offered in favor of connectionism are coherent only on this interpretation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Cognition},
	author = {Fodor, Jerry A. and Pylyshyn, Zenon W.},
	year = {1988},
	note = {Place: Netherlands
Publisher: Elsevier Science},
	keywords = {Cognition, Cognitive Processes, Models},
	pages = {3--71},
	file = {Snapshot:/Users/vincent/Zotero/storage/RFBHAKBQ/1989-03804-001.html:text/html},
}

@book{fong_invitation_2019,
	edition = {1},
	title = {An {Invitation} to {Applied} {Category} {Theory}: {Seven} {Sketches} in {Compositionality}},
	isbn = {978-1-108-66880-4 978-1-108-48229-5 978-1-108-71182-1},
	shorttitle = {An {Invitation} to {Applied} {Category} {Theory}},
	url = {https://www.cambridge.org/core/product/identifier/9781108668804/type/book},
	language = {en},
	urldate = {2023-06-16},
	publisher = {Cambridge University Press},
	author = {Fong, Brendan and Spivak, David I.},
	month = jul,
	year = {2019},
	doi = {10.1017/9781108668804},
	file = {Fong and Spivak - 2019 - An Invitation to Applied Category Theory Seven Sk.pdf:/Users/vincent/Zotero/storage/INAZJFSZ/Fong and Spivak - 2019 - An Invitation to Applied Category Theory Seven Sk.pdf:application/pdf},
}

@misc{yeung_ccg-based_2021,
	title = {A {CCG}-{Based} {Version} of the {DisCoCat} {Framework}},
	url = {http://arxiv.org/abs/2105.07720},
	abstract = {While the DisCoCat model (Coecke et al., 2010) has been proved a valuable tool for studying compositional aspects of language at the level of semantics, its strong dependency on pregroup grammars poses important restrictions: ﬁrst, it prevents large-scale experimentation due to the absence of a pregroup parser; and second, it limits the expressibility of the model to context-free grammars. In this paper we solve these problems by reformulating DisCoCat as a passage from Combinatory Categorial Grammar (CCG) to a category of semantics. We start by showing that standard categorial grammars can be expressed as a biclosed category, where all rules emerge as currying/uncurrying the identity; we then proceed to model permutation-inducing rules by exploiting the symmetry of the compact closed category encoding the word meaning. We provide a proof of concept for our method, converting “Alice in Wonderland” into DisCoCat form, a corpus that we make available to the community.},
	language = {en},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Yeung, Richie and Kartsaklis, Dimitri},
	month = may,
	year = {2021},
	note = {arXiv:2105.07720 [cs, math]},
	keywords = {Computer Science - Computation and Language, Mathematics - Category Theory},
	annote = {Comment: SemSpace 2021: Semantic Spaces at the Intersection of NLP, Physics, and Cognitive Science},
	file = {Yeung and Kartsaklis - 2021 - A CCG-Based Version of the DisCoCat Framework.pdf:/Users/vincent/Zotero/storage/K4XDXJX8/Yeung and Kartsaklis - 2021 - A CCG-Based Version of the DisCoCat Framework.pdf:application/pdf},
}

@incollection{joshi_introduction_1987,
	title = {An {Introduction} to {Tree} {Adjoining} {Grammar}},
	isbn = {978-90-272-2049-3 978-1-55619-032-2 978-90-272-7442-7},
	url = {https://benjamins.com/catalog/z.35.07jos},
	language = {en},
	urldate = {2023-06-20},
	booktitle = {Mathematics of {Language}: {Proceedings} of a conference held at the {University} of {Michigan}, {Ann} {Arbor}, {October} 1984},
	publisher = {John Benjamins Publishing Company},
	author = {Joshi, Aravind K.},
	editor = {Manaster-Ramer, Alexis},
	month = jan,
	year = {1987},
	doi = {10.1075/z.35.07jos},
	pages = {87},
	file = {joshi_TAG_intro.pdf:/Users/vincent/Zotero/storage/JU7NUEVZ/joshi_TAG_intro.pdf:application/pdf;Snapshot:/Users/vincent/Zotero/storage/LVBMYZU7/z.35.html:text/html},
}

@book{heim_semantics_1998,
	title = {Semantics in {Generative} {Grammar}},
	publisher = {Malden, MA: Blackwell},
	author = {Heim, Irene and Kratzer, Angelika},
	editor = {Kratzer, Angelika},
	year = {1998},
	file = {Snapshot:/Users/vincent/Zotero/storage/4R5DSKFM/HEISIG.html:text/html},
}
