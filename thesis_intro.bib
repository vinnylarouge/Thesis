
@article{joyal_geometry_1991,
	title = {The geometry of tensor calculus, {I}},
	volume = {88},
	issn = {0001-8708},
	url = {https://www.sciencedirect.com/science/article/pii/000187089190003P},
	doi = {10.1016/0001-8708(91)90003-P},
	abstract = {A coherent presentation of an n-category is a presentation by generators, relations and relations among relations. Confluent and terminating rewriting systems generate coherent presentations, whose relations among relations are defined by confluence diagrams of critical branchings. This article introduces a procedure to compute coherent presentations when the rewrite relations are defined modulo a set of axioms. Our coherence results are formulated using the structure of n-categories enriched in double groupoids, whose horizontal cells represent rewriting paths, vertical cells represent the congruence generated by the axioms and square cells represent coherence cells induced by diagrams of confluence modulo. We illustrate our constructions on rewriting systems modulo commutation relations in commutative monoids, isotopy relations in pivotal monoidal categories, and inverse relations in groups.},
	language = {en},
	number = {1},
	urldate = {2023-01-18},
	journal = {Advances in Mathematics},
	author = {Joyal, André and Street, Ross},
	month = jul,
	year = {1991},
	pages = {55--112},
	file = {ScienceDirect Full Text PDF:/Users/vincent/Zotero/storage/4KAWLMQF/Joyal and Street - 1991 - The geometry of tensor calculus, I.pdf:application/pdf},
}

@article{joyal_geometry_nodate,
	title = {{THE} {GEOMETRY} {OF} {TENSOR} {CALCULUS} {II}},
	abstract = {This paper defines and proves the correctness of the appropriate string diagrams for various kinds of monoidal categories with duals.},
	language = {en},
	author = {Joyal, André},
	file = {Joyal - THE GEOMETRY OF TENSOR CALCULUS II.pdf:/Users/vincent/Zotero/storage/PPTCJ7AC/Joyal - THE GEOMETRY OF TENSOR CALCULUS II.pdf:application/pdf},
}

@article{maclane_natural_1963,
	title = {Natural {Associativity} and {Commutativity}},
	volume = {49},
	copyright = {All rights reserved by Rice University. This work is licensed under a Creative Commons Attribution Non-commercial 4.0 License.},
	url = {https://scholarship.rice.edu/handle/1911/62865},
	abstract = {Paper presented in three lectures in Anderson Hall on September 23, 24, 26, 1963},
	language = {eng},
	number = {4},
	urldate = {2023-01-18},
	journal = {Rice Institute Pamphlet - Rice University Studies},
	author = {MacLane, Saunders},
	month = oct,
	year = {1963},
	note = {Accepted: 2011-11-08T19:13:47Z
Publisher: Rice University},
	file = {Full Text PDF:/Users/vincent/Zotero/storage/2BCIIAN5/MacLane - 1963 - Natural Associativity and Commutativity.pdf:application/pdf},
}

@book{lane_categories_2010,
	address = {New York, NY},
	edition = {2nd ed. 1978. Softcover reprint of the original 2nd ed. 1978 edition},
	title = {Categories for the {Working} {Mathematician}: 5},
	isbn = {978-1-4419-3123-8},
	shorttitle = {Categories for the {Working} {Mathematician}},
	abstract = {An array of general ideas useful in a wide variety of fields. Starting from the foundations, this book illuminates the concepts of category, functor, natural transformation, and duality. It then turns to adjoint functors, which provide a description of universal constructions, an analysis of the representations of functors by sets of morphisms, and a means of manipulating direct and inverse limits. These categorical concepts are extensively illustrated in the remaining chapters, which include many applications of the basic existence theorem for adjoint functors. The categories of algebraic systems are constructed from certain adjoint-like data and characterised by Beck's theorem. After considering a variety of applications, the book continues with the construction and exploitation of Kan extensions. This second edition includes a number of revisions and additions, including new chapters on topics of active interest: symmetric monoidal categories and braided monoidal categories, and the coherence theorems for them, as well as 2-categories and the higher dimensional categories which have recently come into prominence.},
	language = {English},
	publisher = {Springer},
	author = {Lane, Saunders Mac},
	month = nov,
	year = {2010},
}

@incollection{selinger_survey_2010,
	title = {A survey of graphical languages for monoidal categories},
	volume = {813},
	url = {http://arxiv.org/abs/0908.3347},
	abstract = {This article is intended as a reference guide to various notions of monoidal categories and their associated string diagrams. It is hoped that this will be useful not just to mathematicians, but also to physicists, computer scientists, and others who use diagrammatic reasoning. We have opted for a somewhat informal treatment of topological notions, and have omitted most proofs. Nevertheless, the exposition is sufficiently detailed to make it clear what is presently known, and to serve as a starting place for more in-depth study. Where possible, we provide pointers to more rigorous treatments in the literature. Where we include results that have only been proved in special cases, we indicate this in the form of caveats.},
	urldate = {2023-01-18},
	author = {Selinger, Peter},
	year = {2010},
	doi = {10.1007/978-3-642-12821-9_4},
	note = {arXiv:0908.3347 [math]},
	keywords = {18D10, Mathematics - Category Theory},
	pages = {289--355},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/C64L57Z9/Selinger - 2010 - A survey of graphical languages for monoidal categ.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/DNLSM8N6/0908.html:text/html},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	doi = {10.48550/arXiv.2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: The two lead authors contributed equally to this work},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/53W2J3J3/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/YBVKBY2B/2301.html:text/html},
}

@misc{epsilon3141_llms_2023,
	type = {Tweet},
	title = {{LLMs} + {RL} resolve failure modes},
	url = {https://twitter.com/epsilon3141/status/1615765147896918021?s=46&t=sZSAwsDNorcRseYgc_HpZg},
	journal = {Twitter},
	author = {{@epsilon3141}},
	month = jan,
	year = {2023},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/vincent/Zotero/storage/AKW3XN6W/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/vincent/Zotero/storage/VF7K6WGC/1706.html:text/html},
}

@misc{bastian_google_2022,
	title = {Google {PaLM}: {Giant} language {AI} can explain jokes},
	shorttitle = {Google {PaLM}},
	url = {https://the-decoder.com/google-palm-giant-language-ai-can-explain-jokes/},
	abstract = {Google unveils the latest advance in artificial intelligence: The PaLM language AI model is huge, powerful, and the first building block of a grand vision.},
	language = {en-US},
	urldate = {2023-01-19},
	journal = {THE DECODER},
	author = {Bastian, Matthias},
	month = apr,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/UTTV9Q44/google-palm-giant-language-ai-can-explain-jokes.html:text/html},
}

@techreport{andre_joyal_braided_1986,
	type = {Research {Report}},
	title = {Braided {Monoidal} {Categories}},
	url = {http://web.science.mq.edu.au/~street/JS1.pdf},
	number = {860081},
	urldate = {2023-01-18},
	institution = {MacQuarie University, School of Mathematics and Physics},
	author = {{André Joyal} and {Ross Street}},
	year = {1986},
	pages = {54},
	file = {JS1.pdf:/Users/vincent/Zotero/storage/4WHVIJ8C/JS1.pdf:application/pdf},
}

@misc{openai_chatgpt_2022,
	title = {{ChatGPT}: {Optimizing} {Language} {Models} for {Dialogue}},
	shorttitle = {{ChatGPT}},
	url = {https://openai.com/blog/chatgpt/},
	abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in},
	language = {en},
	urldate = {2023-01-19},
	journal = {OpenAI},
	author = {{OpenAI}},
	month = nov,
	year = {2022},
	file = {Snapshot:/Users/vincent/Zotero/storage/BP6WLSHY/chatgpt.html:text/html},
}

@article{church_pendulum_2011,
	title = {A {Pendulum} {Swung} {Too} {Far}},
	volume = {6},
	issn = {1945-3604},
	url = {https://journals.colorado.edu/index.php/lilt/article/view/1245},
	doi = {10.33011/lilt.v6i.1245},
	abstract = {Today's students might be faced with a very different set of challenges from those of the 1990s in the not-too-distant future. What should they do when most of the low hanging fruit has been pretty much picked over? 
In the particular case of Machine Translation, the revival of statistical approaches (e.g., Brown et al. (1993)) started out with finite-state methods for pragmatic reasons, but gradually over time, researchers have become more and more receptive to the use of syntax to capture long-distance dependences, especially when there isn't very much parallel corpora, and for language pairs with very different word orders (e.g., translating between a subject-verb-object (SVO) language like English and a verb final language like Japanese). Going forward, we should expect Machine Translation research to make more and more use of richer and richer linguistic representations. So too, there will soon be a day when stress will become important for speech recognition. 
Since it isn't possible for textbooks in computational linguistics to cover all of these topics, we should work with colleagues in other departments to make sure that students receive an education that is broad enough to prepare them for all possible futures, or at least all probable futures.},
	language = {en},
	urldate = {2023-01-19},
	journal = {Linguistic Issues in Language Technology},
	author = {Church, Kenneth},
	month = oct,
	year = {2011},
	file = {Church - 2011 - A Pendulum Swung Too Far.pdf:/Users/vincent/Zotero/storage/U2XHJME7/Church - 2011 - A Pendulum Swung Too Far.pdf:application/pdf},
}

@article{anderson_end_2008,
	title = {The {End} of {Theory}: {The} {Data} {Deluge} {Makes} the {Scientific} {Method} {Obsolete}},
	issn = {1059-1028},
	shorttitle = {The {End} of {Theory}},
	url = {https://www.wired.com/2008/06/pb-theory/},
	abstract = {Illustration: Marian Bantjes “All models are wrong, but some are useful.” So proclaimed statistician George Box 30 years ago, and he was right. But what choice did we have? Only models, from cosmological equations to theories of human behavior, seemed to be able to consistently, if imperfectly, explain the world around us. Until now. Today companies […]},
	language = {en-US},
	urldate = {2023-01-19},
	journal = {Wired},
	author = {Anderson, Chris},
	year = {2008},
	note = {Section: tags},
	keywords = {discoveries, magazine-16.07},
	file = {Snapshot:/Users/vincent/Zotero/storage/A63TD3EK/pb-theory.html:text/html},
}

@book{pietsch_epistemology_2022,
	address = {Cham},
	series = {Philosophical {Studies} {Series}},
	title = {On the {Epistemology} of {Data} {Science}: {Conceptual} {Tools} for a {New} {Inductivism}},
	volume = {148},
	isbn = {978-3-030-86441-5 978-3-030-86442-2},
	shorttitle = {On the {Epistemology} of {Data} {Science}},
	url = {https://link.springer.com/10.1007/978-3-030-86442-2},
	language = {en},
	urldate = {2023-01-19},
	publisher = {Springer International Publishing},
	author = {Pietsch, Wolfgang},
	year = {2022},
	doi = {10.1007/978-3-030-86442-2},
	keywords = {Causal approach to analogy, Causation difference making, data science analogy, Data science and explanation, data science causation, data science epistemology, Data science exploratory experimentation, data science inductivist framework, data science inductivist methodology, Data science phenomenological science, data science probability, data science theory, data science theory-driven experimentation, Epistemology of data science, Federica Russo inductive methodology, foundations of data science, Refining eliminative induction, Symmetries in probabilistic reasoning, Variational approach to induction},
	file = {Submitted Version:/Users/vincent/Zotero/storage/EWIBC7EJ/Pietsch - 2022 - On the Epistemology of Data Science Conceptual To.pdf:application/pdf},
}

@article{desai_epistemological_2022,
	title = {The epistemological foundations of data science: a critical analysis},
	issn = {1556-5068},
	shorttitle = {The epistemological foundations of data science},
	url = {https://www.ssrn.com/abstract=4008316},
	doi = {10.2139/ssrn.4008316},
	abstract = {The modern abundance and prominence of data has led to the development of “data science” as a new field of enquiry, along with a body of epistemological reflections upon its foundations, methods, and consequences. This article provides a systematic analysis and critical review of significant open problems and debates in the epistemology of data science. We propose a partition of the epistemology of data science into the following five domains: (i) the constitution of data science; (ii) the kind of enquiry that it identifies; (iii) the kinds of knowledge that data science generates; (iv) the nature and epistemological significance of “black box” problems; and (v) the relationship between data science and the philosophy of science more generally.},
	language = {en},
	urldate = {2023-01-19},
	journal = {SSRN Electronic Journal},
	author = {Desai, Jules and Watson, David and Wang, Vincent and Taddeo, Mariarosaria and Floridi, Luciano},
	year = {2022},
	file = {Desai et al. - 2022 - The epistemological foundations of data science a.pdf:/Users/vincent/Zotero/storage/RTR8JBXW/Desai et al. - 2022 - The epistemological foundations of data science a.pdf:application/pdf},
}

@misc{deleted_user_stack_2018,
	type = {Reddit {Post}},
	title = {{StAcK} {MoRe} {LaYeRs}},
	url = {www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/},
	urldate = {2023-01-19},
	journal = {r/ProgrammerHumor},
	author = {{(deleted user)}},
	month = apr,
	year = {2018},
}

@book{mcshane_linguistics_2021,
	title = {Linguistics for the {Age} of {AI}},
	url = {https://direct.mit.edu/books/book/5042/Linguistics-for-the-Age-of-AI},
	abstract = {A human-inspired, linguistically sophisticated model of language understanding for intelligent agent systems.The open access edition of this book was made possi},
	language = {en},
	urldate = {2023-01-19},
	author = {McShane, Marjorie and Nirenburg, Sergei},
	month = mar,
	year = {2021},
	doi = {10.7551/mitpress/13618.001.0001},
	file = {Snapshot:/Users/vincent/Zotero/storage/X5B9Z9D7/Linguistics-for-the-Age-of-AI.html:text/html},
}

@article{searle_minds_1980,
	title = {Minds, brains, and programs},
	volume = {3},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/minds-brains-and-programs/DC644B47A4299C637C89772FACC2706A},
	doi = {10.1017/S0140525X00005756},
	abstract = {This article can be viewed as an attempt to explore the consequences of two propositions. (1) Intentionality in human beings (and animals) is a product of causal features of the brain. I assume this is an empirical fact about the actual causal relations between mental processes and brains. It says simply that certain brain processes are sufficient for intentionality. (2) Instantiating a computer program is never by itself a sufficient condition of intentionality. The main argument of this paper is directed at establishing this claim. The form of the argument is to show how a human agent could instantiate the program and still not have the relevant intentionality. These two propositions have the following consequences: (3) The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program. This is a strict logical consequence of 1 and 2. (4) Any mechanism capable of producing intentionality must have causal powers equal to those of the brain. This is meant to be a trivial consequence of 1. (5) Any attempt literally to create intentionality artificially (strong AI) could not succeed just by designing programs but would have to duplicate the causal powers of the human brain. This follows from 2 and 4.“Could a machine think?” On the argument advanced here only a machine could think, and only very special kinds of machines, namely brains and machines with internal causal powers equivalent to those of brains. And that is why strong AI has little to tell us about thinking, since it is not about machines but about programs, and no program by itself is sufficient for thinking.},
	language = {en},
	number = {3},
	urldate = {2023-01-19},
	journal = {Behavioral and Brain Sciences},
	author = {Searle, John R.},
	month = sep,
	year = {1980},
	note = {Publisher: Cambridge University Press},
	keywords = {artificial intelligence, brain, intentionality, mind},
	pages = {417--424},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	pages = {5185--5198},
	file = {Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:/Users/vincent/Zotero/storage/A7EIVDQ6/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:application/pdf},
}
