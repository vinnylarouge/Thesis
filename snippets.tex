
\marginnote{
    \begin{rem}[\textbf{Top}]
    The category of topological spaces and continuous functions.
    \end{rem}
}

\begin{defn}[Powerset monad on \textbf{Top}]
Overloading the powerset symbol $\mathcal{P}$, we define the functor $\mathcal{P}: \mathbf{Top} \rightarrow \mathbf{Top}$ to send:
\begin{description}
\item[objects:] $X^\tau \mapsto \mathcal{P}X^{\mathcal{P}\tau}$, where $\mathcal{P}X$ is the powerset of $X$, and we define the new topology \[\mathcal{P}\tau := \{ S \subseteq \tau : \bigcup S \in \tau \}\]
\item[morphisms:] $f: X^\tau \rightarrow Y^\sigma$ to $\mathcal{P}f: \mathcal{P}X^{\mathcal{P}\tau} \rightarrow \mathcal{P}Y^{\mathcal{P}\sigma}$, where $\mathcal{P}f$ is the direct image map.
\[\mathcal{P}f(J \subseteq X) := f(J) \subseteq Y\]
\end{description}
This endofunctor $\mathcal{P}$ on \textbf{Top} is a monad, with unit and multiplication inherited from the powerset monad on \textbf{Set}. We present these explicitly, and verify their continuity:
\begin{description}
\item[unit:] $X^\tau \overset{\eta_{X^{\tau}}}{\longrightarrow} \mathcal{P}X^{\mathcal{P}\tau} := x \in X \mapsto \{x\} \in \mathcal{P}X$.
For continuity, if $S \subseteq \mathcal{P}X$ is open, by definition of $\mathcal{P}\tau$, $\bigcup S \subseteq X$ is open
\item[multiplication:] $\mathcal{P}\mathcal{P}X^{\mathcal{P}\mathcal{P}\tau} \overset{\mu_{X^{\tau}}}{\longrightarrow} \mathcal{P}X^{\mathcal{P}\tau}: S \subseteq \mathcal{P}\tau \mapsto \bigcup S \in \mathcal{P}\tau$
\end{description}
\end{defn}



To the best of my knowledge, the extant approaches with practical promise towards solving this problem tend to have category-theoretic origins. To name two, DisCoCat \citep{} is one, and \citep{} uses categorical methods to ground the computational mechanism of graph neural networks with respect to dynamic programming. The reader is directed to \citep{} for a survey and argument for the necessity and practicality of category-theoretic methods in machine learning.\\




\begin{figure}\label{space:line}
\tikzfig{testspaces/unitinterval}
\caption{The \textbf{unit interval} has the unit interval $[0,1]$ as the underlying set. Open sets are unions of open intervals $(x,y)$. Closed intervals and points are closed. We denote this space $\sim$.}
\end{figure}

\begin{defn}[Partial Functions]
A \textbf{continuous partial function} $X^\tau \rightarrow Y^\sigma$ is the intersection of a continuous function and a bowtie. 
\end{defn}

\marginnote{
\begin{intuition}\label{intuit:screen}
View the space as a screen display, with points as pixels. Then states are "shapes" of lit up pixels, and effects are "tests", which check for lit pixels in an open set. In a discrete topology, all shapes are distinguishable by testing. In an indiscrete topology, the only distinction is between "no shape" and "some shape".
\end{intuition}
}





\subsection{ Compact Hausdorff spaces are arachnid }

\marginnote{

\begin{rem}[Compactness]
An \textbf{open cover} of a space $X^\tau$ is a set of opens $\{U_i: i \in I\} \subset \tau$ such that:
\[ \bigcup\limits_{i \in I} U_i = X \]
A \textbf{subcover} of an open cover is a subset of the open cover that is still an open cover.
$X^\tau$ is \textbf{compact} when every open cover has a finite subcover.
\end{rem}

\begin{rem}[Hausdorff]
$X^\tau$ is \textbf{Hausdorff} when for any two distinct points $a,b \in X$ there are disjoint opens $U,V \in \tau$ such that $a \in U$ and $b \in V$. The mnemonic is that the points are "housed-off".
\end{rem}

}

\begin{defn}[Non-unital Spider]
A \textbf{non-unital spider} on $X^\tau$ is a tuple [[[placeholder]]] that satisfies the following relations:

\end{defn}

\begin{defn}[Arachnidity]
Call $X^\tau$ \textbf{arachnid} if it admits a family of non-unital spiders such that for any finite family $\mathbb{K}:=\{K_i \subset X : i \in I, \ |I| \in \mathbb{N}\}$ of subsets of $X$, there exists a non-unital spider on $X^\tau$ such that:

[[placeholder]]

\end{defn}

\begin{theorem}[Compact Hausdorff is Arachnidity]
$X^\tau$ is compact and Hausdorff iff $X^\tau$ is arachnid.
\end{theorem}

We prove this by first relating arachnidity to some conditions that are phrased in the "distinguishing shapes" setting of Intuition \ref{intuit:screen}, and then bridge to compact Hausdorff.

\subsection{Arachnidity, Distinguishing, Encoding, Recognition}

\newthought{When is the topology of a space "good"?} There are several possible criteria for "goodness", such as the ability to distinguish different shapes, encode those shapes in the results of testing, and reconstruct those shapes from an encoding.

\marginnote{
    \begin{remark}
        Where $\tau$ of all opens represents all possible tests, a test battery is a specific set of tests.
    \end{remark}
}
\begin{defn}[Test battery]
A \textbf{test battery} of $X^\tau$ is a subset of $\tau$.
\end{defn}
\marginnote{
    \begin{remark}
    A curriculum is a range of possible shapes.
    \end{remark}
}
\begin{defn}[Curriculum]
A \textbf{curriculum} of $X^\tau$ is a set of subsets of $X$.
\end{defn}

\marginnote{
\begin{remark}
If $X^\tau$ is not distinguishing, then there are two distinct shapes $J \neq K$ that no test can tell apart.
\end{remark}    
}
\begin{defn}[Distinguishing]\label{def:distinguish}
$X^\tau$ is \textbf{distinguishing} when
\[\forall J,K_{(\subseteq X)} (J \neq K \iff \exists U_{(\in \tau)}: placeholder)\]
\end{defn}

\marginnote{
    \begin{remark}
    An encoding is a binary spectrum; for each test in the battery, a bit indicating whether that test has failed or succeeded.
    \end{remark}
}
\begin{defn}[Encoding]
An \textbf{encoding} of $K \subset X$ with respect to a test battery $\mathbb{T}: \{U_i : i \in I, U_i \in \tau\}$ is a direct sum:
\[placeholder\]
\end{defn}

\marginnote{
    \begin{remark}
    A recogniser can take any finite curriculum and provide a finite test battery such that shapes can be distinguished via their finite encodings. Finiteness is a weak but necessary condition computability -- i.e. practical feasibility.
    \end{remark}
}
\begin{defn}[Recognition: computable distinguishing encodings]
$X^\tau$ is a \textbf{recogniser} if for any finite $\mathbb{K} := \{ K_i \subseteq X : i \in I, \ |I|\in \mathbb{N} \}$, there exists a finite test battery $\mathbb{U} := \{ U_j \in \tau: j \in J, \ |J|\in \mathbb{N} \} \subseteq \tau$ such that:
\[K_a \neq K_b \iff placeholder\]
\end{defn}

\marginnote{
    \begin{remark}
    Instead of providing a different test battery for each curriculum, it would be more convenient to extend the test battery whenever an unseen shape enters the curriculum; this is what a learner does.
    \end{remark}
}

\begin{defn}[Learning: extendable recognisers]
A recogniser is a \textbf{learner} when, for arbitrary finite curricula $\mathbb{K}' \supset \mathbb{K}$ and a finite test battery $\mathbb{U}$:
\[placeholder \iff \exists \mathbb{U}'_{(\mathbb{U} \subseteq \mathbb{U}' \subseteq \tau)}:( |\mathbb{U}'| \in \mathbb{N} \wedge placeholder)\]

\end{defn}

\marginnote{
    \begin{remark}
    Recall 
    \end{remark}
}
\begin{defn}[Recall: reconstruction from encodings]

\end{defn}

\marginnote{
    \begin{remark}

    \end{remark}
}

\begin{defn}

\end{defn}



\section{Enrichment structure}



Denote by $(X \times Y)^{(\tau \multimap \sigma)}$ the topological space of topological relations of type $X^\tau \rightarrow Y^\sigma$ as given above. We show that this topology is finer than the product topology.

\begin{proposition}
For any $X^\tau$ and $Y^\sigma$, $\tau \times \sigma \subseteq \tau \multimap \sigma$
\begin{proof}
Let $\mathfrak{b}_\tau, \mathfrak{b}_\sigma$ be bases for $\tau$ and $\sigma$ respectively, then $\tau \times \sigma$ has basis $\mathfrak{b}_\tau \times \mathfrak{b}_\sigma$. An arbitrarily element $(t \in \tau, s \in \sigma)$ of this product basis can be viewed as a topological relation $t \times s \subseteq X \times Y$. Every open of $\tau \times \sigma$ is a union of such basis elements, and topological relations are closed under arbitrary union, so we have the (evidently injective) correspondence:
\[ \tau \times \sigma \ni \bigcup\limits_{i \in I}(t_i \times s_i) \mapsto \bigcup\limits_{i \in I}(t_i \times s_i) \in \tau \multimap \sigma \]
\end{proof}
\end{proposition}

\marginnote{
\begin{example}[$\tau \multimap \sigma \nsubseteq \tau \times \sigma$]
Recalling Proposition \ref{prop:states}, let $\tau = \{\varnothing,\{\star\}\}$ on the singleton, and $\sigma$ be an arbitrary nondiscrete topology on base space Y. $(\{\star\} \times Y)^{(\tau \times \sigma)}$ is isomorphic to $Y^\sigma$, but $(\{\star\} \times Y)^{\tau \multimap \sigma)}$ is the isomorphic to the discrete topology $Y^\bullet$. For a more concrete example, consider the Sierpi\'{n}ski space $\mathcal{S}$ again, along with the topological relation $\{(0,0),(1,0),(1,1)\} \subset \mathcal{S} \times \mathcal{S}$; due to the presence of $(0,0)$, this topological relation cannot be formed by a union of basis elements of the product topology, which are:
\begin{description}
\item[$\{1\} \times \{1\}$ =] $\{(1,1)\}$
\item[$\{1\} \times \{0,1\}$ =] $\{ (1,0),(1,1) \}$
\item[$\{0,1\} \times \{1\}$ =] $\{ (1,1),(0,1) \}$
\item[$\{0,1\} \times \{0,1\}$ =] $\{ (0,0), (0,1), (1,0), (0,1) \}$
\end{description}

\end{example}
}

\begin{proposition}
$\tau \multimap \sigma = \tau \times \sigma \iff $
\begin{proof}

\end{proof}
\end{proposition}

\clearpage

\subsection{How \textbf{TopRel} relates to other well-known categories}

\[\begin{tikzcd}
    &&& {\mathbf{Loc}} \\
    \\
    \\
    {\mathbf{Top}} &&& {\mathbf{TopRel}} &&& {\mathbf{Rel}} \\
    \\
    \\
    &&& {\mathbf{Set}}
    \arrow[""{name=0, anchor=center, inner sep=0}, "{(-)_U}", curve={height=-12pt}, shorten <=8pt, shorten >=8pt, from=4-4, to=4-7]
    \arrow[""{name=1, anchor=center, inner sep=0}, "{(-)_D}", curve={height=-12pt}, shorten <=8pt, shorten >=8pt, from=4-7, to=4-4]
    \arrow[""{name=2, anchor=center, inner sep=0}, "{(-)_U}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=4-1, to=7-4]
    \arrow[""{name=3, anchor=center, inner sep=0}, "{(-)_D}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=7-4, to=4-1]
    \arrow[""{name=4, anchor=center, inner sep=0}, "{(-)_L}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=4-1, to=1-4]
    \arrow[""{name=5, anchor=center, inner sep=0}, "{(-)_P}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=1-4, to=4-1]
    \arrow[shorten <=5pt, shorten >=5pt, from=4-4, to=1-4]
    \arrow["i"{description}, shorten <=10pt, shorten >=10pt, hook, from=7-4, to=4-7]
    \arrow["i"{description}, shorten <=8pt, shorten >=8pt, hook, from=4-1, to=4-4]
    \arrow["\dashv"{anchor=center, rotate=45}, draw=none, from=3, to=2]
    \arrow["\dashv"{anchor=center, rotate=90}, draw=none, from=1, to=0]
    \arrow["\dashv"{anchor=center, rotate=-44}, draw=none, from=4, to=5]
\end{tikzcd}\]

Here are two ways to think about \textbf{TopRel} in relation to other categories. First, we can think of the analogy
\[\mathbf{TopRel}:\mathbf{Rel}::\mathbf{Top}:\mathbf{Set}\]
This analogy holds up to the free-forgetful adjunction between \textbf{Top} and \textbf{Set} embedding into a free-forgetful adjunction between \textbf{TopRel} and \textbf{Rel}.\marginnote{
    \begin{remark}[\textbf{TopRel} is not Kleisli in the obvious way]
    However, while \textbf{Rel} is the Kleisli category of \textbf{Set} with respect to the powerset monad, \textbf{TopRel} doesn't seem to be the Kleisli category of \textbf{Top} in an analogous way. The obvious notion of a powerset topology $\mathcal{P}\tau := \{S \subset \tau : \bigcup S \in \tau\}$ agrees with our na\"{i}ve definition of continuous relation, but the underlying unit and multiplication maps from the powerset monad on \textbf{Set} fail to be continuous with respect to this topology.
    \end{remark}
}



\begin{proposition}[\textbf{Rel} and \textbf{TopRel} free-forgetful adjunction]

\begin{proof}

\end{proof}
\end{proposition}


\begin{proposition}[(Outer square)]

\begin{proof}

\end{proof}
\end{proposition}

\begin{rem}[Locales, and the \textbf{Top}-\textbf{Loc} adjunction]

\end{rem}

\begin{proposition}[(Inner square)]

\begin{proof}

\end{proof}
\end{proposition}


%%%%%%%%%%%%%

Although this thesis is about language, it is not really linguistics.
Linguistics, conventionally, is about understanding human language.
Formal linguistic theories rely on empirical observation of human language use.
Now there are not-human entities that use human language.
Now there are un-natural languages that humans use.
So I am interested in sketching a form of language that humans and computers can use\footnote{So this is not intended to be a total account of linguistics; there is no discussion of phonetics, pragmatics or anything else beyond text. Even among text, the intention here}.
This sketch is informed by, but not bound to, human language.\\

Although this thesis is mathematical, it is not really mathematics.
This thesis does not introduce significantly new mathematical constructions or relations.
So there are no new "know that"s.
This thesis does use relatively modern mathematics to approach an old problem.
The math is symmetric monoidal categories, the problem is depicting language.
So there is new "know how".\\

This thesis is computer science, a little.
The mathematics used is, to a degree, implementation agnostic.
This thesis is only concerned with the "in principle", rather than the "in practice".
There will be no code demonstration nor machine learning experiments, because
Computer Science is to Programming as Physics is to Engineering.
I will point out where experiments have been done.
Hypothetical procedures will be spelled out if needed.





\newthought{An idempotent on $Y^\sigma$ that splits through a discrete topology $X^\star$ does these things:}
\begin{description}
\item[The section:]{picks a subset of $s(x) \subseteq Y$ for each point $x \in X$. In order to be a split idempotent, $s(x)$ must be distinct for distinct points $x$.}
\item[The retract:]{(reading backwards) picks an open set $r(x) \in \sigma$ for each point $x \in X$. In order to be a split idempotent, $r(x)$ only overlaps $s(x)$ out of all other selected shapes: \[r(x) \cap s(x') = 1 \iff x = x'\]}
\end{description}
The combined effect is that the shapes $s(x)$ become copiable elements of the sticky spider, just as the points are the copiable elements of a regular spider.


\begin{defn}[\textbf{PLoc}]
A \emph{partial frame homomorphism} is a partial map between frames $\psi: A \rightarrow B$ such that, restricted to the domain of definition in $A$, $\psi$ is a frame homomorphism. The category \textbf{PLoc} is obtained in the same manner as \textbf{PLoc}, but using partial frame homomorphisms instead, which is well defined as a subcategory of \textbf{Pfn}, the category of sets and partial functions. \textbf{Loc} is thus a subcategory of \textbf{PLoc}.
\end{defn}



Both linguists and AI-researchers stand to benefit from interaction. Linguists have much to gain by treating LLMs as a rich source of empirical data for linguistic issues. For example, there is fuzzy and widely-held notion that language must somehow be intimately related to reasoning and cognition, or that all of these things are inseparably clustered under the name of "intelligence". An observation from LLMs that sheds light on this notion is that when a language model becomes sophisticated enough, it can employ chain-of-thought \citep{wei_chain--thought_2023} to bootstrap reasoning ability, which hints at an overlap between the 'narrow' task of predicting linguistic output and the very general task of pattern recognition. 


By linguistic introspection, we realise we must account for \emph{Entification} and \emph{Processising} -- the process of turning non-nouns into noun-entities and back again. If we think about English, we find that just about any word can be turned into a noun and back again (e.g. \texttt{run} by gerund to \texttt{running}, \texttt{quick} by a suffix to \texttt{quickness}, and even entire sentences \texttt{Bob drinks Duvel} can become a noun \texttt{the fact that Bob drinks Duvel}).\\

This consideration carries some linguistic interest as well. In the usual treatment of anaphora resolution, pronouns refer to nouns, for instance: \texttt{Bob drinks a beer. \underline{It} is cold.}, where \texttt{it} refers to the beer. But there are situations where pronouns can point to textual data that are not nouns. For instance: \texttt{Jono was paid minimum wage. He didn't mind \underline{it}.}, where \texttt{it} would like to refer to something like \texttt{the fact that Jono was paid minimum wage}. While there are extensions of discourse reference theory to accommodate event structures [], the issue at hand is that pronouns in the appropriate context seem to be able to refer to \emph{any meaningful part of text}. For example, \texttt{The tiles were grey. \underline{It} was a particularly depressing shade.}, where \texttt{it} seems to refer just to the entified adjective \texttt{the greyness (of the tiles)}. Or, \texttt{Alice's cake was tastier than Bob's, but \underline{it} wasn't enough so for the judges to decide unanimously.}, where \emph{it} seems to refer the entified tastiness differential of \texttt{tastier}: \texttt{the difference in tastiness between Alice and Bob's cakes}.\\

\newthought{}

Now we try to interpret these constraints in mathematical terms, staying within the graphical confines we have established in \textbf{TopRel} as much as possible. Let us denote the noun-wire type by $\Xi$. First we observe that any finite collection of noun wires $\bigotimes^n \Xi$ has to be \emph{encodable} in a single noun wire $\Xi$, because we can always interpose with \texttt{and}. We take this to mean that there will exist morphisms such that:

\[placeholder\]

Second, for any word-gate $w$ of grammatical type $\mathfrak{g}$, we ought to have noun-states and an evaluator process that witness entification and processising:

\[placeholder\]

Second-and-a-half, any morphism (or "meaningful part of text") $T \in \bigotimes^n \Xi \ , \ \bigotimes^m \Xi$ for any $n,m \in N$ -- has to be encodable as a state of $\Xi$. This is expressed as the following graphical condition:

\[placeholder\]

Condition two-and-a-half follows if the former conditions are met, provided that all text circuits are made up of a fixed stock of grammatical-gate-types:

\[placeholder\]

If we have all the above, then we can grab any part of a circuit and turn it into a noun. We can notate this using a \emph{lasso}.

\[\]

Recall that Lassos -- a graphical gadget that can encode arbitrary morphisms into a single wire -- can be interpreted in a monoidal computer. Recall that monoidal computers require a universal object $\Xi$. Here we show how in \textbf{TopRel}, by taking $\Xi := \squarehvfill$ the open unit square, we have a monoidal computer in \textbf{Rel} restricted to countable sets and the relations between them. We will make use of sticky spiders. We have to show that; $\squarehvfill$ has a sticky-spider corresponding to every countable set; how there is a suitable notion of sticky-spider morphism to establish a correspondence with relations; what the continuous relations are on $\squarehvfill$ that mimick various compositions of relations.

\footnote{These programs differ intensionally: the equalities of extensional process theories for programs are too coarse-grained to distinguish between programs with different time- and space- requirements; even after the extensional form of the program is set, computer science is also about the practical aim of finding the right implementation subject to intensional complexity, because we are impatient and real computers have finite memory. I want to remark that the monoidal computer framework [] is comprehensive for undergraduate theoretical computer science, accounting for both the extensional domain of computability theory and the intensional domain of complexity theory.}




\footnote{I am unsatisfied with how both machines and formal semanticists go about doing this. For example, when I am reading a text and I get to \texttt{Alice shuts the door}, something about the model in my head acts along with the text, possibly introducing the door if I didn't know about it before. I don't remember all of the previous words in the story, so that can't be my model. I know my model is transparent and manipulable, because I can imagine the events of the story in as much detail as I have patience for and I can imagine would have happened if this-had-happened-instead-of-that, so my model at least as much systematicity and compositionality as the language the story is written in. However good at their task vector representations in massive latent spaces are, I can't be sure that they carry that same systematicity that I have, and I want to be sure that I'm not conversing with an alien mind. On the other hand, even if it were feasible to define formal truth conditions for all the words, the truth-conditional view of meaning is completely backwards for my concerns: truth-conditions require a model to evaluate against and I am concerned with how \emph{that model} is built from language. If \texttt{Alice shuts the door} is true in any sense, it is only because my internal model has evolved in the act of reading in such a way to make it the case. The meaning of the sentence is the procedural content of how it updates my model in the act of reading, not the mathematical consequence that the sentence then happens to be true in my model. In short, I would prefer to have a conversation with something that works with language and internal representations in a similar way I do, not an alien or a pedant.}





\subsection{Spatial predicates}

The following simple inference is what we will try to capture process-theoretically:

\begin{itemize}
\item \texttt{Oxford is north of London}
\item \texttt{Vincent is in Oxford}
\item \texttt{Philipp is in London}
\end{itemize}

How might it follow that \texttt{Philipp is south of Vincent}?\\

One way we might approach such a problem computationally is to assign a global coordinate system, for instance interpreting `north' and `south' and `is in' using longitude and latitude. Another coordinate system we might use is a locally flat map of England. The fact that either coordinate system would work is a sign that there is a degree of implementation-independence.\\

This coordinate/implementation-independence is true of most spatial language: we specify locations only by relative positions to other landmarks or things in space, rather than by means of a coordinate system. This is necessarily so for the communication of spatial information between agents who may have very different reference frames.\\

So the process-theoretic modelling aim is to specify how relations between entities can be \emph{updated} and \emph{classified} without requiring individual spatial entities to intrinsically possess meaningful or determinate spatial location.\\

So far we have established how to update properties of individual entities. We can build on what we have so far by observing that a relation between two entities can be considered a property of the pair.

\[placeholder\]

Spatial relations obey certain compositional constraints, such as transitivity in the case of `north of':

\[placeholder\]

Or the equivalence between 'north of' and 'south of' up to swapping the order of arguments:

\[\]

There are other general constraints on spatial relations, such as order-independence: the order in which spatial relations are updated does not (at least for perfect reasoners) affect the resultant presentation. This is depicted diagrammatically as commuting gates:

\[placeholder\]


\begin{marginfigure}\label{fig:bigcfg}
\centering
\[\resizebox{0.4\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigcfg}}\]
\caption{Consider the sentence \texttt{Alice sees Bob quickly run to school}, which we take to be generated by the above context-free grammar derivation, read from bottom-to-top.}
\end{marginfigure}




Strictification is arguably necessary even for DisCoCat. Pregroups by themselves are thin rigid autonomous categories, meaning there is at most one morphism between any pair of objects and that there are directed cups and caps in addition to plain monoidal structure. Contra the aims of DisCoCat, this implies there is no nontrivial strong monoidal functor from pregroups into $\textbf{FinVect}^\otimes$, where strength for a monoidal functor means that $Fx \otimes Fy \simeq F(x \otimes y)$ in the codomain. This is because for any object $x$ in the pregroup, any strong monoidal $F$ into $\textbf{FinVect}^\otimes$ that interprets duals in the usual way must identify the following diagrams up to isomorphism of finite vector spaces with their duals:
\[placeholder\]
The only vector space $Fx$ for which this holds is the base field of $\textbf{Vect}$; the monoidal unit. The same argument applies for functors into $\textbf{Rel}$ -- another common semantic domain -- where the only valid target is the singleton as monoidal unit. One "fix" for this problem is to only ask for lax monoidal functors, where instead of isomorphism, only a unidirectional laxator natural transformation $Fx \otimes Fy \rightarrow F(x \otimes y)$ is required. I dislike using lax constructions just on their own, because they are an admission that there is extra stuff in $F(x \otimes y)$ we can't probe that wasn't present in $Fx \otimes Fy$. In prose, this is approximately "emergent behaviour in the whole that wasn't present in the parts", which sounds impressive, but doesn't give us any understanding because it is only assigning a name to what we do not know without going into it. What is suggested in this section is to factorise the lax monoidal semantic functor $\mathcal{P} \rightarrow \mathcal{M}$ from categorified pregroup proofs to a monoidal category as $\mathcal{P} \rightarrow \bar{\mathcal{M}} \rightarrow \mathcal{M}$ where $\bar{\mathcal{M}}$ is the strictification of $\mathcal{M}$. %The first leg of the factorisation is a strict monoidal functor, sending categorified pregroup proofs (which are thin, hence strict, i.e. all monoidal structural isomorphisms are equalities) to strictified diagrams of $\mathcal{M}$, which holds onto pregroup proof structure. The second leg destrictifies, forgetting proof structure.}




\begin{construction}
Where $\mathcal{P}$ is the monoidal category for pregroup diagrams and where $\mathcal{G}$ is the monoidal category for the string-diagrammatic presentations of the productive grammars we can define a discrete monoidal fibration $\overline{\mathcal{P}}' \rightarrow \mathcal{G}$, where $\overline{\mathcal{P}}'$ is the subcategory of the strictification of $\mathcal{P}$ as generated by Construction \ref{cons:bracketing}.
\end{construction}



\begin{marginfigure}
\centering
\caption{The concept of exchange can be glossed approximately as the following, with variable noun-entries capitalised.}

Before exchange:\\
\texttt{AGENT has THEME.}\\
\texttt{AGENT wants OBJECT.}\\

During exchange:\\
\texttt{AGENT gives THEME to PATIENT so-to get OBJECT.}\\

After exchange:\\
\texttt{AGENT has OBJECT}\\
\texttt{PATIENT has THEME}

\[\tikzfig{metaphor/timeISmoney}\]
\end{marginfigure}

\marginnote{A post-LLM theory of language requires coherence between theories of syntax -- one for the speaker and one for the listener -- such that their underlying structures are strongly equivalent and are amenable in principle to distributional or vector semantics.} Anything short of this fails to provide any understanding of language beyond what can be gleaned from an LLM. Explaining communication demands two coherent theories of syntax for speaker and listener. Practicality demands a semantics that can be learned in principle from data and represented as vectors-and-operations-upon-them. The stakes are, at worst, the entirety of linguistics as an enterprise of inquiry. At best, it's all made up and nothing really matters so I will subscribe to this standard anyway just because. To the best of my knowledge there isn't any, even partial account of language that satisfies the bare minumum. Perhaps this is due to siloisation, because I don't doubt that all of the ingredients are already present in the literature: there are popular accounts of how grammar composes (albeit practically worthless) truth-theoretic semantics [Heim and Kratzer], and there are too many equivalence proofs between different grammatical formalisms, but I can't find anything that puts the minimum all at once in the same bowl. If it does exist already, my cake is probably prettier and tastier. So in this thesis I will supply a post-LLM theory of language, and I will endeavour to do it as diagrammatically as possible. I only care to get the absolute basics right by my own standard, and I will accordingly treat everything else about language like pragmatics and gesture and whatever else as an afterthought, \emph{as it should be}: consider air resistance only after you know how motion works in a vacuum.}



, the first box depicts "start and finish $R$ and $S$ concurrently, $R$ on the left and $S$ on the right": \[\varepsilon = \varepsilon \cdot \varepsilon \mapsto x \cdot a \cdot b \mapsto \varepsilon \cdot \varepsilon = \varepsilon\]
The second, "start $R$ first. Start $S$ on the right concurrently as $R$ finishes: \[\varepsilon \mapsto x = x \cdot \epsilon \mapsto \epsilon \cdot a \cdot b = a \cdot b \mapsto \epsilon\]
The third, "start and finish $R$ first, and then start and finish $S$": \[\varepsilon \mapsto x \mapsto \varepsilon \mapsto a \cdot b \mapsto \varepsilon\]
The rest follow by symmetry.

\textbf{4.)} Start $S$ first. Start $R$ on the right concurrently as $S$ finishes: $\varepsilon \mapsto a \cdot b = a \cdot b \cdot \varepsilon \mapsto \varepsilon \cdot x = x \mapsto \varepsilon $.\\
\textbf{5.)} Do $S$ and $R$ concurrently, $S$ on the left and $R$ on the right: $\varepsilon = \varepsilon \cdot \varepsilon \mapsto a \cdot b \cdot x \mapsto \varepsilon \cdot \varepsilon = \varepsilon$.\\


\subsection{Sentences}

Before the contents of a sentence are even decided, we may decide to (top row, left to right) get another sentence ready, introduce an adverbial adjunction (such as \texttt{yesterday}), or introduce a conjunction of two sentences (such as \texttt{because}). Introducing new words yields dots that carry information of the grammatical category of the word, and there are separate rewrites that allow dots to be labelled according to the lexicon. In the bottom, we have a rewrite that allows a sentence with one unlabelled noun to be the subject of a "sentential complement verb", abbreviated \texttt{SCV}: these are verbs that take sentences as objects rather than other nouns, and they are typically verbs of cognition, perception, and causation, such as \texttt{Alice \underline{suspects} Bob drinks}. The blue-dotted lines are just syntactic guardrails that correspond to the holes in boxes of circuits later.


Within each sentence bubble, each derivation starts life as a "simple sentence", which only involves nouns and a single verb, which is either an intransitive verb that takes a single noun argument, or a transitive verb that takes two. You just can't have a (propositional) sentence without at least a noun and a verb. Within each sentence, we may start introducing nouns. From left-to-right; we may introduce a new noun (which comes with tendrils that extrude outside the sentence bubble for later use to resolve pronominal reference); split a noun so that the same noun-label is used multiply; and label a noun \emph{if it is saturated -- depicted by a solid black circle} which includes a copy of the label for bookkeeping purposes when resolving references.


Now we deal with nouns. The major complication here is the accommodation of coreference. We want to keep track of which (pro)nouns share a reference so that we can ultimately eliminate the distinction between e.g. \texttt{Bob likes himself} and \texttt{Bob likes Bob}. So we will generate \emph{unsaturated} nouns and their coreference structure first. We will further ask for a distinction between \emph{unsaturated} and \emph{saturated} nouns -- only the latter of which are ready to be labelled -- which will resolve a minor complication regarding the context-sensitivity of adpositions. We define three classes of rules for nouns. The first is the introduction of novel unsaturated nouns, which can occur in any sentence-bubble. The second class handles the introduction of a new, coreferentially-linked noun to the right of an extant noun. The third concerns swapping the positions of unsaturated and linked nouns within and between bubbles.

Running the named rules in reverse yields the following string manipulations on coreference structures, where integers are notated $j,(j+1),k$, and $X_j$ denotes the leading part of the string has highest integer $j$.
\begin{align}
&\texttt{S-}intro &\texttt{]} &\mapsto \texttt{] [ ]}\\
&\texttt{CNJ-}intro &\texttt{[ ] [ ]} &\mapsto \texttt{[ ]}\\
&\texttt{SCV-}intro &\texttt{[ j [ ] ]} &\mapsto \texttt{[ j ]}\\
&linked\texttt{-N-}intro &\texttt{j ] [ j} &\mapsto \texttt{j ] [}\\
&\texttt{N-}shift &\texttt{] [ j} &\mapsto \texttt{j ] [}\\
&\texttt{N-}swap &\texttt{j k} &\mapsto \texttt{k j}
\end{align}

The \texttt{N}-intro rule in reverse affects the coreference structure string globally. It removes an integer if it is the only occurrence of that integer, and decrements all integers higher than it by one.

\begin{align}
&\texttt{N-}intro &\texttt{X}_\texttt{j} \ \texttt{(j+1) ]} &\mapsto \texttt{X}_\texttt{j} \ \texttt{]}\\
\end{align}


\begin{defn}[(restricted) coreference structure]\label{defn:corefstruct}
Considering the \texttt{S}-intro and \texttt{CNJ}-intro rules to be concatenations of sentences and the \texttt{SCV}-intro rule to introduce a nested sentence, \emph{sentential structure} corresponds to a well-bracketing, where $\texttt{[}$ indicates the start of a sentence and $\texttt{]}$ the end. In a text with $K \in \mathbb{N}$ coreference classes of nouns, coreference structure is a string in $\{\texttt{[}, \texttt{]}, \texttt{1}, \texttt{2}, \ldots, \texttt{K}\}$ that:
\begin{itemize}
\item starts with $\texttt{[}$ and ends with $\texttt{]}$
\item is a well-bracketing with respect to $\texttt{[},\texttt{]}$
\item for every occurrence of \texttt{[} after the first, has it immediately preceded by \texttt{[ j}, where $j$ is some integer.
\item contains at least one occurrence of every $k \in [\texttt{1} \cdots \texttt{K}]$
\item For all $k \in [1 \cdots K]$, the sublist up to the first occurrence of $k$ contains all $j < k$.
\item no $k \in [1 \cdots K]$ occurs more than once within a matched pair of brackets.
\end{itemize}
Conditions 1 and 2 together indicate that the simplest coreference structure corresponds to the empty sentence and model sentence structure as well-bracketings. Condition 3 respects that only the \texttt{SCV}-intro rule can introduce nesting sentence structure, which requires a single noun to be present. Conditions 4 and 5 together label every coreference class with an integer "without gaps", such that every new noun that occurs from left to right with a distinct coreference class gets assigned the next available integer. Condition 6 rules out reflexive coreferences that occur within the same sentence, following the discussion surrounding Figure \ref{fig:reflcomp}.
\end{defn}

\begin{example}\label{ex:corefex}
\texttt{sober} $\alpha$ \texttt{sees drunk} $\beta$ \texttt{clumsily dance.} has coreference structure $\texttt{[1[2]]}$. \texttt{sober} $\alpha$ \texttt{sees drunk} $\beta$ \texttt{clumsily dance.} $\alpha$ \texttt{laughs at} $\beta$ has coreference structure $\texttt{[1[2]][12]}$.
\end{example}

\begin{proposition}[Completeness with respect to coreference structures]
\texttt{S}-intro, \texttt{CNJ}-intro, \texttt{SCV}-intro, \texttt{N}-intro, linked-\texttt{N}-intro, \texttt{N}-shift, and \texttt{N}-swap can generate any coreference structure as in Definition \ref{defn:corefstruct}, and only those.
\begin{proof}
For the "only" claim, we must argue that the named rewrites are compliant with the 5 conditions of Definition \ref{defn:corefstruct}. The \texttt{S}-intro, \texttt{CNJ}-intro, \texttt{SCV}-intro rules satisfy conditions 1 through 3. There is nothing to check for condition 4, which governs the interpretation of coreference structures as in Example \ref{ex:corefex}. Condition 5 is satisfied by Lemma \ref{prop:linkedlist}. For condition 6, it suffices to observe that novel linked-nouns occur in later sentences than their spawning noun, and this fact is kept invariant by all the named rules.\\
We approach the "any" claim by demonstrating that the named rules \emph{operating in reverse} suffice to turn any starting coreference structure into the simplest coreference structure $\texttt{[]}$, cf. Condition 1 and that a single sentence-bubble is the start of our diagrammatic derivations. ). First observe that, aided by Lemma \ref{prop:linkedlist}, we can construct a circuit-growing diagram, unique up to homotopies, for any restricted coreference structure. As a running example, suppose the coreference structure is \texttt{[1[[][]]]}
\end{proof}
\end{proposition}


\begin{lemma}
Let the \emph{population} of unsaturated nouns in a diagram be the multiset of kinds with multiplicity of their occurence. The \texttt{N}-shift and \texttt{N}-swap rules keep the population invariant.
\begin{proof}
By the end of the argument in Proposition \ref{prop:linkedlist}, conservation of tail-arities and their endings by these two local rewrites cannot affect the population.
\end{proof}
\end{lemma}

Nouns require verbs in order to be saturated. From left-to-right; if there is precisely one unlabelled noun, we may introduce an unlabelled intransitive verb and saturate the noun so that it is now ready to grow a label; or if there are two unlabelled nouns, we may introduce an unlabelled transitive verb on the surface and saturate the two nouns that will be subject and object; and verbs may be labelled.
\[
\tikzfig{mushroom/simpbestiary}
\]

\subsection{Modifiers}

Modifiers are optional parts of sentences that modify (and hence depend on there being) nouns and verbs. We consider adjectives, adverbs, and adpositions. From left to right; we allow adjectives to sprout immediately before a saturated noun, and we allow adverbs to sprout immediately before any verb.

\[
\tikzfig{mushroom/adjadv}
\]

Adpositions modify verbs by tying in an additional noun argument; e.g. while \texttt{runs} is intransitive, \texttt{runs towards} behaves as a transitive verb. Some more advanced technology is required to place adpositions and their thematic nouns in the correct linear order on the surface. In the left column; an adposition tendril can sprout from a verb via an unsaturated adposition, seeking an unsaturated noun to the right; an unsaturated noun can sprout an tendril seeking a verb to connect to on the left. Both of these rewrites are bidirectional, as tendrils might attempt connection but fail, and so be retracted. In the centre, when an unsaturated adposition and its tendril find an unsaturated noun, they may connect, saturating the adposition so that it is ready to label. In the right column; an unsaturated adposition may move past a saturated noun in the same sentence, which allows multiple adpositions for the same verb; finally, a saturated adposition can be labelled.

\[
\tikzfig{mushroom/adpbestiary}
\]

\subsection{Rewriting to circuit-form}

\newthought{Resolving references}

\[
\tikzfig{mushroom/pronbestiary}
\]

\[
\tikzfig{mushroom/pronres}
\]

\newthought{Connecting circuits}

\newpage
\subsection{Putting it all together}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex1}}
\]
\caption{Starting from the initial sentence bubble, we generate a new sentence, introduce some nouns and an SCV, and then we connect our references at the bottom.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex2}}
\]
\caption{Then we introduce intransitive verbs to saturate nouns, and we may also sprout some modifying adjectives and adverbs.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex3}}
\]
\caption{For the remaining unsaturated noun, we use the adposition introduction rules to sprout tendrils off of the other verb in the bubble, and connect.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex4}}
\]
\caption{All of the non-noun words may now be labelled.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex5}}
\]
\caption{To begin assigning nouns, observe that by compact closure of the bubble boundaries, we can deform the diagram to obtain suitable forms for our local rewrite rules for link-generation at the bottom.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex6}}
\]
\caption{Now we can introduce our noun labels and linearise our link structure.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex7}}
\]
\caption{Once the link structure is linearised, we can undo the deformation, and propagate links to the surface.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/bigex8}}
\]
\caption{The bubbles may be rearranged to respect circuit-form, such that the propagation of noun labels to the surface traces out the wires of the end circuit.}
\end{figure}

\clearpage

If you are already happy to treat such doodles as formal, then I think you're alright, you can skip the rest of this chapter. If you are a category theorist or just curious, hello, how are you, please do write me to share your thoughts if you care to, and please skip the rest of this paragraph. If you are still reading, I assume you are some kind of smelly epistemic-paranoiac Bourbaki-thrall sets-and-lambdas math-phallus-worshipping truth-condition-blinded symbol-pusher who takes things too seriously. I hope you choke on the math. I will begin the intimidation immediately.\\


\begin{convention}\label{conv:sliding}
We consider two circuits the same if their gate-connectivity is the same:
\[
\tikzfig{textcirc/gatecompex1} \quad\quad = \quad\quad \tikzfig{textcirc/gatecompex2}
\]
Since only gate-connectivity matters, we consider circuits the same if all that differs is the horizontal positioning of gates composed in parallel:
\[
\tikzfig{textcirc/gateeqslide} 
\]
We do care about output-to-input connectivity, so in particular, we {\bR\bf do not\e}
consider circuits to be equal up to sequentially composed gates commuting past each other:  
\[
\tikzfig{textcirc/gateneqcommute}  
\]
\end{convention}

\begin{example} 
The sentence \texttt{\underline{ALICE SEES BOB LIKES FLOWERS THAT CLAIRE PICKS}} can intuitively be given the following text circuit:
\[
\tikzfig{textcirc/multigateholeex}
\]
\end{example}

\begin{marginfigure}
    \centering
    \resizebox{\marginparwidth}{!}{\tikzfig{textcirc/circuitgen}}
    \caption{Sentences correspond to filled gates, boxes with fixed arity correspond to first-order modifiers such as adverbs and adpositions, and boxes with variable arity correspond to sentential-level modifiers such as conjunctions and verbs with sentential complements. Composition by connecting wires corresponds to identifying coreferences in discourse, and composition by nesting corresponds to grammatical structure within sentences.}
    \label{fig:circuitgen}
\end{marginfigure}

\newthought{As one more step towards abstraction, we ask: which relations $X^\tau \rightarrow Y^\sigma$ are always continuous?}

\begin{proposition}
The \textbf{empty relation} $X \rightarrow Y$ relates nothing. It is defined $\varnothing \subset X \times Y$. The empty relation is always continuous.
\label{prop:emptyrel}
\begin{proof}
The preimage of the empty relation is always $\varnothing$, which is open by definition.
\end{proof}
\end{proposition}


\section{\textbf{ContRel} diagrammatically}

\subsection{Relations that are always continuous}

\newthought{Here are five continuous relations for any $X^\tau$:}

\[\scalebox{0.75}{\tikzfig{bestiary/generators}}\]

\newthought{Copy and delete obey the following equalities:}

\[\scalebox{0.5}{\tikzfig{bestiary/basicrelations}}\]

\newthought{The copy map can also be used to distinguish the deterministic maps -- points and functions -- which we notate with an extra dot.}

\[\scalebox{0.75}{\tikzfig{structure/determinism}}\]

\newthought{Everything, delete, nothing-states and nothing-tests combine to give two numbers, one and zero.} There are extra expressions in grey squares above: they anticipate the tape-diagrams we will later use to graphically express another monoidal product of \textbf{ContRel}, the direct sum $\oplus$.

\[\scalebox{0.75}{\tikzfig{bestiary/scalarrelations}}\]

\newthought{Zero scalars turn entire diagrams into zero morphisms.} There is a zero-morphism for every input-output pair of objects in \textbf{ContRel}. 

\[\scalebox{0.75}{\tikzfig{bestiary/zerorelations}}\]

The reason for this (as is also the case in \textbf{Rel}, \textbf{Vect}, and any category with biproducts and zero-objects []) is that the zero scalar is an absorbing element of multiplication, and that multiplying any process by a zero scalar sends it to its corresponding zero-morphism.

\[\scalebox{0.75}{\tikzfig{bestiary/zeroscalar}}\]

So whenever a zero-process appears in a diagram, it spawns zero scalars which infect all other processes, turning them all into zero-processes. The same holds for whenever a zero-scalar appears; it makes copies of itself to infect all other processes.

\[\scalebox{0.75}{\tikzfig{bestiary/contagion}}\]

\clearpage

\begin{proposition}\label{prop:bowtie}
Full relations restricted to open sets in the domain are continuous. Given an open $U \subseteq X^\tau$, and an arbitrary subset $K \subset Y^\sigma$, the relation $U \times K \subseteq X \times Y$ is open.
\begin{proof}
Consider an arbitrary open set $V \in \sigma$. Either $V$ and $K$ are disjoint, or they overlap. If they are disjoint, the preimage of $V$ is $\varnothing$, which is open. If they overlap, the preimage of $V$ is $U$, which is open.
\end{proof}
\end{proposition}



\newthought{Summary:} Logical Type Theory in mathematics had a twin sister Categorial Grammar in linguistics, born in the 30s to Ajdukiewicz, raised into the 50s by Bar-Hillel and Lambek. Montague brought formal semantics to the picture via the Lambda-Calculus in the 70s, around the time Category Theory was getting started. The Curry-Howard correspondence between types and logic became Curry-Howard-Lambek to include categories, thus relating the typed lambda-calculus, intuitionistic logic, and cartesian closed categories. Lambek and Moortgat evolved categorial grammar into typelogical grammar; the use of different proof systems as models of grammar, which in turn suggested semantic categories beyond the cartesian closed setting. Alternative semantics in the form of quantum computers were realised by Coecke and Lambek, who together added string diagrams to the correspondence via a \emph{literally} observed correspondence between quantum bell states and reductions in Pregroup Grammar. Sazradeh and Clark enter the collaboration, bringing in Firth's distributional semantics -- which had by the time become computationally practical as the basis of neural methods for word encoding -- to create DisCoCat; a \underline{Dis}tributional, \underline{Co}mpositional and \underline{Cat}egorial framework for language. Mirroring the developmental circumstances of Discourse Representation Theory (itself independently conceived by Kamp and Heim), Coecke suggested promoting DisCoCat as framework for sentences towards a circuit-shaped framework for text -- DisCoCirc. But there remained unanswered questions and ugly spots, and some poor sap had to work out the formal details and clean things up. That poor sap is me.


\subsection{Curry-Howard-Lambek}

\begin{table}[]
\begin{tabular}{ccc}
(Typed) Lambda-Calculus & Intuitionistic Logic & Cartesian Closed Categories  \\
 Types & Propositions & Categories  \\
 Curry & Howard & Lambek 
\end{tabular}
\end{table}

This correspondence means that you can use the lambda-calculus on any family of data organised as a cartesian closed category; this could be strings, or sets and functions, topological shapes with holes, neural nets and finite vectors. So one small benefit of the category-theoretic viewpoint is a formal underpinning for these mild extensions.\\

Describing the bigger benefit requires a bigger picture. 


and Lambek and Coecke fully integrated the picture with syntax and categories. So the linguist's trinity may look something like this:

\begin{table}[]
\begin{tabular}{ccc}
Computation & Syntax & Semantics \\
(Typed) Lambda-calculus & Combinatory Categorial Grammar & Cartesian Closed Categories \\
Montague & Ajdukiewicz & Lambek
\end{tabular}
\end{table}

Or this:

\begin{table}[]
\begin{tabular}{ccc}
Computation & Syntax & Semantics \\
(Typed) Lambda-calculus & Pregroup Grammar & Rigid Autonomous Monoidal Categories \\
Montague & Ajdukiewicz & Lambek
\end{tabular}
\end{table}

So here is the story today as far as a linguist may be concerned. We know that Curry-Howard-Lambek- correspondence generalises: if you poke Howard for a grammar that is expressively distinct from a CCG, the type-theory changes, so Lambek gives a different family of semantic categories with different internal logics, and Curry gives you a syntactic composition gadget that differs from the lambda-calculus.


 Thus, Montague semantics has largely been in the care of linguists rather than mathematicians. This meant sparse opportunity for the ideas to `update' according to mainstream developments in mathematics.\\

{
 \begin{itemize}
\item
Section \ref{} demonstrates how string-diagrammatic reasoning allows for graphical proofs of strong equivalences between typelogical, string-production, and further strong equivalence to a fragment of tree-adjoining grammars. 
\item
Text diagrams and text circuits lie at the heart of the above correspondences and of this thesis, which we introduce and investigate in Section \ref{} in an abridged re-presentation of \bR CITE \e, culminating in a proof relating the expressive capacity of text circuits to a controlled fragment of English that serves as evidence that text circuits are a natural metalanguage for grammatical relationships that make no extraneous distinctions.
\item
In Section \ref{}, moving towards applications, I introduce the category of continuous relations, to set a mathematical stage upon which we can build toy models, expanding upon my previous work on linguistically compositional spatial relations \bR CITE \e towards modelling mechanical systems and containers.
\item
I mathematically investigate the possibilities and limitations of textual modelling with text circuits on classical and quantum computers in Section \ref{} by examining the limitations of cartesian monoidal categories for modelling text circuits, taking the universal approximation theorem into account.
\item
In Section \ref{}, I extend the string-diagrammatic techniques used to prove correspondences between different syntactic theories to text circuits provides a framework for the formal, conceptually-compliant modelling of textual metaphor.
\item
I demonstrate a formal connection between tame topologies and tensed language in Section \ref{}, which extends to a formal framework to model narratives as database rewrites in Section \ref{}.
\end{itemize}
}



Chapter 2 provides the relevant background and foundations for category theory, machine learning, and formal syntax for this thesis, which lives at the intersection. The ideas required from the parent fields will be basic, so the exposition is meant to get readers across disciplines on the same page, not impress experts. For string diagrams I will first provide a primer for how process-theoretic reasoning with string diagrams work, by example. On the category theoretic end, I will recount symmetric monoidal categories as the mathematical objects that string diagrams are syntax for, as well as provide a working understanding of PROPs \bR CITE \e and n-categories as formalised by \bR CITE \e, which provide a metalanguage for specifying families of string diagrams. Once the reader is happy with string diagrams, for machine learning I will just introduce how deep neural nets and backpropagation work in string-diagrammatic terms to provide a foundation of formal understanding, and I will explain the mathematical and real-world reasons why deep learning is so powerful. For formal linguistics, I will sketch out a partial history of categorial linguistics in general, along the way briefly recasting \bR CITE \e in more modern mathematical terminology to justify string diagrams as a generalisation of Montague's original conception of syntax and semantics.\\

Chapter 3 is about string diagrams for formal syntax. Here I recount context-free, pregroup, and tree-adjoining grammars to the reader, recast them string-diagrammatically, and relate them by means of discrete monoidal fibrations, a piece of mathematics I will develop. Then we (re)introduce text circuits as the common structure between those different theories of grammar that abstracts away differences in linear syntactic presentation while conserving a core set of grammatical relations. During my DPhil, I wrote a paper \bR CITE \e in collaboration with Jonathon Liu and Bob Coecke which introduced text circuits in a pedestrian way, and characterised their expressive capacity with respect to a controlled fragment of English. I will just recover the main beats of that paper, this time using the metalanguage of n-categories.\\

Chapter 4 sets a mathematical stage for us to model and calculate using text circuits, for which purpose I introduce the category of continuous relations \textbf{ContRel}, a na\"{i}ve generalisation of the category of continuous maps between topological spaces. \textbf{ContRel} is new, in the sense that the category-theoretically obvious approaches to obtaining such a category either do not work or yield something different. This section culminates in formal semantics for topological concepts such as \texttt{inside} which underpin the kinds of schematic doodle cartoons we might draw on paper to illustrate events occurring in space.\\

Chapter 5 is a formal invitation to playtime for the reader who gets that far. I don't expect that I've explored any novel linguistic phenomena, and I don't think I've invented any substantially new mathematics. All I've done is a form of intellectual arbitrage, putting tools from one field to work in another. To properly give weight to my claim that string diagrams and category theory are a good metalanguage for linguistics as a whole, it is necessary to demonstrate breadth. So, I model linguistic topological concepts; I give a mathematical setting for the study of generalised anaphora that reference any meaningful part of text; I provide formal semantics for the container metaphor in particular and textual metaphors in general; I sketch a formal correspondence between tensed language and tame topologies that extends to formally reckoning with narrative structure. All of this is to show that the methods I use are flexible and not doctrinal. I am not interested in whether these topics have been mathematicised more thoroughly and deeply before; what I care to demonstrate is that a little category theory and some imagination can go a long way.\\

Finally, I close with a discussion and prospectus. For the convenience of the reader, bibliographies are placed at the end of each chapter. 





A philosophical draw for the formal semanticist is that insofar as semantics is synonymy -- the study of when expressions are equivalent -- weak $n$-categories are an exquisite setting to control and study sameness in terms of meta-equivalences. 


\subsection{Avenues III: grammar as geometry, geometry as computation}

The example of noun phrases illustrates the higher principle that the particulars of interpretational choices into string diagrams are inessential: any consistent scheme will do, and the string diagrams will give voice to the computational structure of dynamic semantics.

\begin{example}[\textbf{Noun phrases I}]
\[\texttt{The king of France is bald} \text{ (new discourse)}\]
\end{example}

\begin{example}[\textbf{Noun phrases II}]
\[\texttt{The king of France is bald} \text{ (discourse modifier)}\]
\end{example}





\begin{construction}
For any composite gluing of the unit interval, there is a continuous endo-relation on the real line that achieves arbitrary permutation of the constituent intervals, since continuous relations are unions of partial continuous functions \bR REF \e.
\end{construction}




\begin{construction}[Pregroups with bracketing]\label{cons:bracketing}
Where $\mathbf{PGD}$ is a rigid monoidal category generated by pregroup states and (directed) cups, we define pregroups-with-bracketing as subcategory of the \emph{free strictification without relations of \textbf{PGD}}, which we denote $\overline{\mathbf{PGD}}'$, which consists of all the generators of the strictification $\overline{\mathbf{PGD}}$ as given by Definition \ref{defn:strict}, but none of the additional equations. The subcategory is constructed from the following generators:
\begin{enumerate}
\item For each pregroup state $\texttt{w}: I \rightarrow \bigotimes\limits_{i} X_i$, a strictified state generator $\texttt{w}: I \rightarrow (\cdots((X_1 \otimes X_2) \otimes \cdots X_i) \cdots )$ with left-nested syntactic tensors. To illustrate, a state with 5 wires would correspond to a generator as follows:
\[\tikzfig{strictify/strictstate}\]
\item Let $[A \cdot B \cdots Z]$ denote the left-nested tensoring $((A \otimes B) \cdots \otimes Z)$, and let $\mathbf{X}$ denote $(\bigotimes\limits_i X_i)$. For each directed cap $\mathbf{X} \otimes \mathbf{X}^{-1} \rightarrow I$ (and symmetrically for caps of the other direction and cups), and for each pair of bracketed types $[\mathbf{A} \cdot \mathbf{X}]$ and $[\mathbf{X}^{-1} \cdot \mathbf{B}]$, we ask for a generator that fully detensors, applies the directed cup, and then retensors. Diagrammatically, this amounts to asking for generators that look like the following, that mimick a single proof step.
\[\tikzfig{strictify/stricteval}\]
\end{enumerate}
\end{construction}


$
\AxiomC{$\texttt{Alice} : \textcolor{blue}{n}$}
\AxiomC{$\texttt{likes} : {}^{-}\textcolor{blue}{n} \cdot \textcolor{magenta}{s} \cdot \textcolor{blue}{n}^{-}$}
\BinaryInfC{$\texttt{Alice\textvisiblespace likes} : \textcolor{magenta}{s} \cdot \textcolor{blue}{n}^{-}$}
\AxiomC{$\texttt{Bob} : \textcolor{blue}{n}$}
\BinaryInfC{$\texttt{Alice\textvisiblespace likes\textvisiblespace Bob} : \textcolor{magenta}{s}$}
\DisplayProof$




\subsection{Communicative constraints as a cofunctor from productive to parsing grammar}
\begin{figure}[h!]\label{fig:plan1}
\[\resizebox{0.75\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex012}}\]
\caption{}
\end{figure}

\begin{figure}[h!]\label{fig:plan2}
\[\resizebox{0.5\textwidth}{!}{\tikzfig{tree2gate/workedexample/gather_quickly}}\]
\caption{}
\end{figure}

\begin{figure}[h!]\label{fig:plan3}
\[\resizebox{0.5\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_2bad}}\]
\caption{}
\end{figure}

Since the functor is a monoidal discrete fibration, it introduces the appropriate choice of \texttt{quickly} when we pull the functor-box down, while leaving everything else in parallel alone.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex2and3}}\]
Adpositions can apply for verbs of any noun-arity. We again use the fiber of the functor for bookkeeping by asking it to send all of the following partial pregroup diagrams to the adposition generator. We consider the pregroup typing of a verb of noun-arity $k \geq 1$ to be $^{-1} n \cdot s \cdot \underbrace{n^{-1} \cdots n^{-1}}_{(k-1)}$.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/gather_adp}}\]
When we pull down the functor-box, the discrete fibration introduces the appropriate choice of diagram from above, corresponding to the intransitive verb case.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_3}}
\quad = \quad
\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_4}}\]
Similarly to \texttt{quickly}, we suppose we have a family of processes for the word \texttt{to}, one for each noun-arity of verb.
\[\tikzfig{tree2gate/workedexample/gather_to}\]
Again the discrete fibration introduces the appropriate choice of \texttt{to} when we pull the functor box down.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_4}}
\quad = \quad
\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_5}}\]
Now we visually simplify the inside of the functor-box by applying yanking equations.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_5}}
\quad = \quad
\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_6}}\]
Similarly as before, we can pull the functor-box past the intransitive verb node. There is only one pregroup type $^{-1}n \cdot s$ that corresponds to the grammatical category $\textcolor{green}{\texttt{(I)VP}}$.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_6to7}}\]
Proceeding similarly, we can pull the functor-box past the sentential-complement-verb node. There are multiple possible pregroup types for $\textcolor{ForestGreen}{\texttt{SCV}}$, depending on how many noun-phrases are taken as arguments in addition to the sentence. For example, in $\texttt{Alice \textcolor{ForestGreen}{sees} \textcolor{cyan}{[sentence]}}$, $\textcolor{ForestGreen}{\texttt{sees}}$ returns a sentence after taking a noun to the left and a sentence to the right, so it has pregroup typing $^{-1}n \cdot s \cdot s^{-1}$. On the other hand, for something like $\texttt{Alice \textcolor{ForestGreen}{\texttt{tells}} Bob \textcolor{cyan}{[sentence]}}$, $\textcolor{ForestGreen}{\texttt{tells}}$ returns a sentence after taking a noun (the teller) to the left, a noun (the tellee) to the left, and a sentence (the story) to the left, so it has a pregroup typing $^{-1}n \cdot s \cdot n^{-1} \cdot s^{-1}$. These two instances of sentential-complement-verbs are introduced by different nodes. We can record both of these pregroup typings in the functor by asking for the following:
\[\tikzfig{tree2gate/workedexample/gather_scv}\]
Pulling down the functor box:
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_7}}
\quad = \quad
\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_8}}\]
As before, we can ask the functor to send an appropriate partial pregroup diagram to the dependent label $\textcolor{ForestGreen}{\bar{\texttt{see}}}$.
\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigex_8to9}}\]
Now again we can visually simplify using the yanking equation and isotopies, which obtains a pregroup diagram.
\[\tikzfig{tree2gate/workedexample/bigex_a0}\]
The pregroup diagram corresponds to a particular pregroup proof of the syntactic correctness of the sentence \texttt{Alice sees Bob run quickly to school}.
\[
\resizebox{\textwidth}{!}{
\AxiomC{$\texttt{A} : n$}
\AxiomC{$\textcolor{green}{\texttt{s}} : ^{-1}n \cdot s \cdot s^{-1}$}
\BinaryInfC{$\texttt{A\textvisiblespace \textcolor{green}{s}} : s \cdot s^{-1}$}
\AxiomC{$\texttt{B} : n$}
\AxiomC{$\textcolor{orange}{\texttt{q}} :  (^{-1}n \cdot s) \cdot ( ^{-1}n \cdot s)^{-1} $}
\AxiomC{$\textcolor{green}{\texttt{r}} : ^{-1}n \cdot s$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}} : ^{-1}n \cdot s$}
\AxiomC{$\textcolor{blue}{\texttt{t}} : ^{-1}( ^{-1}n \cdot s) \cdot ( ^{-1}n \cdot s) \cdot n^{-1}$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}} : (^{-1}n \cdot s) \cdot n^{-1}$}
\AxiomC{$\texttt{S} : n$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : ^{-1}n \cdot s$}
\BinaryInfC{$\texttt{B\textvisiblespace \textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : s$}
\BinaryInfC{$\texttt{A\textvisiblespace \textcolor{green}{s}\textvisiblespace B\textvisiblespace \textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : s$}
\DisplayProof
}
\]


\clearpage
\subsection{Relating circuits and CFGs fibrationally}

\clearpage
\subsection{Where internal wirings come from}

\subsection{Discrete monoidal fibrations for grammatical functions}

\clearpage