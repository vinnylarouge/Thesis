
\marginnote{
    \begin{rem}[\textbf{Top}]
    The category of topological spaces and continuous functions.
    \end{rem}
}

\begin{defn}[Powerset monad on \textbf{Top}]
Overloading the powerset symbol $\mathcal{P}$, we define the functor $\mathcal{P}: \mathbf{Top} \rightarrow \mathbf{Top}$ to send:
\begin{description}
\item[objects:] $X^\tau \mapsto \mathcal{P}X^{\mathcal{P}\tau}$, where $\mathcal{P}X$ is the powerset of $X$, and we define the new topology \[\mathcal{P}\tau := \{ S \subseteq \tau : \bigcup S \in \tau \}\]
\item[morphisms:] $f: X^\tau \rightarrow Y^\sigma$ to $\mathcal{P}f: \mathcal{P}X^{\mathcal{P}\tau} \rightarrow \mathcal{P}Y^{\mathcal{P}\sigma}$, where $\mathcal{P}f$ is the direct image map.
\[\mathcal{P}f(J \subseteq X) := f(J) \subseteq Y\]
\end{description}
This endofunctor $\mathcal{P}$ on \textbf{Top} is a monad, with unit and multiplication inherited from the powerset monad on \textbf{Set}. We present these explicitly, and verify their continuity:
\begin{description}
\item[unit:] $X^\tau \overset{\eta_{X^{\tau}}}{\longrightarrow} \mathcal{P}X^{\mathcal{P}\tau} := x \in X \mapsto \{x\} \in \mathcal{P}X$.
For continuity, if $S \subseteq \mathcal{P}X$ is open, by definition of $\mathcal{P}\tau$, $\bigcup S \subseteq X$ is open
\item[multiplication:] $\mathcal{P}\mathcal{P}X^{\mathcal{P}\mathcal{P}\tau} \overset{\mu_{X^{\tau}}}{\longrightarrow} \mathcal{P}X^{\mathcal{P}\tau}: S \subseteq \mathcal{P}\tau \mapsto \bigcup S \in \mathcal{P}\tau$
\end{description}
\end{defn}



To the best of my knowledge, the extant approaches with practical promise towards solving this problem tend to have category-theoretic origins. To name two, DisCoCat \citep{} is one, and \citep{} uses categorical methods to ground the computational mechanism of graph neural networks with respect to dynamic programming. The reader is directed to \citep{} for a survey and argument for the necessity and practicality of category-theoretic methods in machine learning.\\




\begin{figure}\label{space:line}
\tikzfig{testspaces/unitinterval}
\caption{The \textbf{unit interval} has the unit interval $[0,1]$ as the underlying set. Open sets are unions of open intervals $(x,y)$. Closed intervals and points are closed. We denote this space $\sim$.}
\end{figure}

\begin{defn}[Partial Functions]
A \textbf{continuous partial function} $X^\tau \rightarrow Y^\sigma$ is the intersection of a continuous function and a bowtie. 
\end{defn}

\marginnote{
\begin{intuition}\label{intuit:screen}
View the space as a screen display, with points as pixels. Then states are "shapes" of lit up pixels, and effects are "tests", which check for lit pixels in an open set. In a discrete topology, all shapes are distinguishable by testing. In an indiscrete topology, the only distinction is between "no shape" and "some shape".
\end{intuition}
}





\subsection{ Compact Hausdorff spaces are arachnid }

\marginnote{

\begin{rem}[Compactness]
An \textbf{open cover} of a space $X^\tau$ is a set of opens $\{U_i: i \in I\} \subset \tau$ such that:
\[ \bigcup\limits_{i \in I} U_i = X \]
A \textbf{subcover} of an open cover is a subset of the open cover that is still an open cover.
$X^\tau$ is \textbf{compact} when every open cover has a finite subcover.
\end{rem}

\begin{rem}[Hausdorff]
$X^\tau$ is \textbf{Hausdorff} when for any two distinct points $a,b \in X$ there are disjoint opens $U,V \in \tau$ such that $a \in U$ and $b \in V$. The mnemonic is that the points are "housed-off".
\end{rem}

}

\begin{defn}[Non-unital Spider]
A \textbf{non-unital spider} on $X^\tau$ is a tuple [[[placeholder]]] that satisfies the following relations:

\end{defn}

\begin{defn}[Arachnidity]
Call $X^\tau$ \textbf{arachnid} if it admits a family of non-unital spiders such that for any finite family $\mathbb{K}:=\{K_i \subset X : i \in I, \ |I| \in \mathbb{N}\}$ of subsets of $X$, there exists a non-unital spider on $X^\tau$ such that:

[[placeholder]]

\end{defn}

\begin{theorem}[Compact Hausdorff is Arachnidity]
$X^\tau$ is compact and Hausdorff iff $X^\tau$ is arachnid.
\end{theorem}

We prove this by first relating arachnidity to some conditions that are phrased in the "distinguishing shapes" setting of Intuition \ref{intuit:screen}, and then bridge to compact Hausdorff.

\subsection{Arachnidity, Distinguishing, Encoding, Recognition}

\newthought{When is the topology of a space "good"?} There are several possible criteria for "goodness", such as the ability to distinguish different shapes, encode those shapes in the results of testing, and reconstruct those shapes from an encoding.

\marginnote{
    \begin{remark}
        Where $\tau$ of all opens represents all possible tests, a test battery is a specific set of tests.
    \end{remark}
}
\begin{defn}[Test battery]
A \textbf{test battery} of $X^\tau$ is a subset of $\tau$.
\end{defn}
\marginnote{
    \begin{remark}
    A curriculum is a range of possible shapes.
    \end{remark}
}
\begin{defn}[Curriculum]
A \textbf{curriculum} of $X^\tau$ is a set of subsets of $X$.
\end{defn}

\marginnote{
\begin{remark}
If $X^\tau$ is not distinguishing, then there are two distinct shapes $J \neq K$ that no test can tell apart.
\end{remark}    
}
\begin{defn}[Distinguishing]\label{def:distinguish}
$X^\tau$ is \textbf{distinguishing} when
\[\forall J,K_{(\subseteq X)} (J \neq K \iff \exists U_{(\in \tau)}: placeholder)\]
\end{defn}

\marginnote{
    \begin{remark}
    An encoding is a binary spectrum; for each test in the battery, a bit indicating whether that test has failed or succeeded.
    \end{remark}
}
\begin{defn}[Encoding]
An \textbf{encoding} of $K \subset X$ with respect to a test battery $\mathbb{T}: \{U_i : i \in I, U_i \in \tau\}$ is a direct sum:
\[placeholder\]
\end{defn}

\marginnote{
    \begin{remark}
    A recogniser can take any finite curriculum and provide a finite test battery such that shapes can be distinguished via their finite encodings. Finiteness is a weak but necessary condition computability -- i.e. practical feasibility.
    \end{remark}
}
\begin{defn}[Recognition: computable distinguishing encodings]
$X^\tau$ is a \textbf{recogniser} if for any finite $\mathbb{K} := \{ K_i \subseteq X : i \in I, \ |I|\in \mathbb{N} \}$, there exists a finite test battery $\mathbb{U} := \{ U_j \in \tau: j \in J, \ |J|\in \mathbb{N} \} \subseteq \tau$ such that:
\[K_a \neq K_b \iff placeholder\]
\end{defn}

\marginnote{
    \begin{remark}
    Instead of providing a different test battery for each curriculum, it would be more convenient to extend the test battery whenever an unseen shape enters the curriculum; this is what a learner does.
    \end{remark}
}

\begin{defn}[Learning: extendable recognisers]
A recogniser is a \textbf{learner} when, for arbitrary finite curricula $\mathbb{K}' \supset \mathbb{K}$ and a finite test battery $\mathbb{U}$:
\[placeholder \iff \exists \mathbb{U}'_{(\mathbb{U} \subseteq \mathbb{U}' \subseteq \tau)}:( |\mathbb{U}'| \in \mathbb{N} \wedge placeholder)\]

\end{defn}

\marginnote{
    \begin{remark}
    Recall 
    \end{remark}
}
\begin{defn}[Recall: reconstruction from encodings]

\end{defn}

\marginnote{
    \begin{remark}

    \end{remark}
}

\begin{defn}

\end{defn}



\section{Enrichment structure}



Denote by $(X \times Y)^{(\tau \multimap \sigma)}$ the topological space of topological relations of type $X^\tau \rightarrow Y^\sigma$ as given above. We show that this topology is finer than the product topology.

\begin{proposition}
For any $X^\tau$ and $Y^\sigma$, $\tau \times \sigma \subseteq \tau \multimap \sigma$
\begin{proof}
Let $\mathfrak{b}_\tau, \mathfrak{b}_\sigma$ be bases for $\tau$ and $\sigma$ respectively, then $\tau \times \sigma$ has basis $\mathfrak{b}_\tau \times \mathfrak{b}_\sigma$. An arbitrarily element $(t \in \tau, s \in \sigma)$ of this product basis can be viewed as a topological relation $t \times s \subseteq X \times Y$. Every open of $\tau \times \sigma$ is a union of such basis elements, and topological relations are closed under arbitrary union, so we have the (evidently injective) correspondence:
\[ \tau \times \sigma \ni \bigcup\limits_{i \in I}(t_i \times s_i) \mapsto \bigcup\limits_{i \in I}(t_i \times s_i) \in \tau \multimap \sigma \]
\end{proof}
\end{proposition}

\marginnote{
\begin{example}[$\tau \multimap \sigma \nsubseteq \tau \times \sigma$]
Recalling Proposition \ref{prop:states}, let $\tau = \{\varnothing,\{\star\}\}$ on the singleton, and $\sigma$ be an arbitrary nondiscrete topology on base space Y. $(\{\star\} \times Y)^{(\tau \times \sigma)}$ is isomorphic to $Y^\sigma$, but $(\{\star\} \times Y)^{\tau \multimap \sigma)}$ is the isomorphic to the discrete topology $Y^\bullet$. For a more concrete example, consider the Sierpi\'{n}ski space $\mathcal{S}$ again, along with the topological relation $\{(0,0),(1,0),(1,1)\} \subset \mathcal{S} \times \mathcal{S}$; due to the presence of $(0,0)$, this topological relation cannot be formed by a union of basis elements of the product topology, which are:
\begin{description}
\item[$\{1\} \times \{1\}$ =] $\{(1,1)\}$
\item[$\{1\} \times \{0,1\}$ =] $\{ (1,0),(1,1) \}$
\item[$\{0,1\} \times \{1\}$ =] $\{ (1,1),(0,1) \}$
\item[$\{0,1\} \times \{0,1\}$ =] $\{ (0,0), (0,1), (1,0), (0,1) \}$
\end{description}

\end{example}
}

\begin{proposition}
$\tau \multimap \sigma = \tau \times \sigma \iff $
\begin{proof}

\end{proof}
\end{proposition}

\clearpage

\subsection{How \textbf{TopRel} relates to other well-known categories}

\[\begin{tikzcd}
    &&& {\mathbf{Loc}} \\
    \\
    \\
    {\mathbf{Top}} &&& {\mathbf{TopRel}} &&& {\mathbf{Rel}} \\
    \\
    \\
    &&& {\mathbf{Set}}
    \arrow[""{name=0, anchor=center, inner sep=0}, "{(-)_U}", curve={height=-12pt}, shorten <=8pt, shorten >=8pt, from=4-4, to=4-7]
    \arrow[""{name=1, anchor=center, inner sep=0}, "{(-)_D}", curve={height=-12pt}, shorten <=8pt, shorten >=8pt, from=4-7, to=4-4]
    \arrow[""{name=2, anchor=center, inner sep=0}, "{(-)_U}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=4-1, to=7-4]
    \arrow[""{name=3, anchor=center, inner sep=0}, "{(-)_D}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=7-4, to=4-1]
    \arrow[""{name=4, anchor=center, inner sep=0}, "{(-)_L}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=4-1, to=1-4]
    \arrow[""{name=5, anchor=center, inner sep=0}, "{(-)_P}", curve={height=-12pt}, shorten <=10pt, shorten >=10pt, from=1-4, to=4-1]
    \arrow[shorten <=5pt, shorten >=5pt, from=4-4, to=1-4]
    \arrow["i"{description}, shorten <=10pt, shorten >=10pt, hook, from=7-4, to=4-7]
    \arrow["i"{description}, shorten <=8pt, shorten >=8pt, hook, from=4-1, to=4-4]
    \arrow["\dashv"{anchor=center, rotate=45}, draw=none, from=3, to=2]
    \arrow["\dashv"{anchor=center, rotate=90}, draw=none, from=1, to=0]
    \arrow["\dashv"{anchor=center, rotate=-44}, draw=none, from=4, to=5]
\end{tikzcd}\]

Here are two ways to think about \textbf{TopRel} in relation to other categories. First, we can think of the analogy
\[\mathbf{TopRel}:\mathbf{Rel}::\mathbf{Top}:\mathbf{Set}\]
This analogy holds up to the free-forgetful adjunction between \textbf{Top} and \textbf{Set} embedding into a free-forgetful adjunction between \textbf{TopRel} and \textbf{Rel}.\marginnote{
    \begin{remark}[\textbf{TopRel} is not Kleisli in the obvious way]
    However, while \textbf{Rel} is the Kleisli category of \textbf{Set} with respect to the powerset monad, \textbf{TopRel} doesn't seem to be the Kleisli category of \textbf{Top} in an analogous way. The obvious notion of a powerset topology $\mathcal{P}\tau := \{S \subset \tau : \bigcup S \in \tau\}$ agrees with our na\"{i}ve definition of continuous relation, but the underlying unit and multiplication maps from the powerset monad on \textbf{Set} fail to be continuous with respect to this topology.
    \end{remark}
}



\begin{proposition}[\textbf{Rel} and \textbf{TopRel} free-forgetful adjunction]

\begin{proof}

\end{proof}
\end{proposition}


\begin{proposition}[(Outer square)]

\begin{proof}

\end{proof}
\end{proposition}

\begin{rem}[Locales, and the \textbf{Top}-\textbf{Loc} adjunction]

\end{rem}

\begin{proposition}[(Inner square)]

\begin{proof}

\end{proof}
\end{proposition}


%%%%%%%%%%%%%

Although this thesis is about language, it is not really linguistics.
Linguistics, conventionally, is about understanding human language.
Formal linguistic theories rely on empirical observation of human language use.
Now there are not-human entities that use human language.
Now there are un-natural languages that humans use.
So I am interested in sketching a form of language that humans and computers can use\footnote{So this is not intended to be a total account of linguistics; there is no discussion of phonetics, pragmatics or anything else beyond text. Even among text, the intention here}.
This sketch is informed by, but not bound to, human language.\\

Although this thesis is mathematical, it is not really mathematics.
This thesis does not introduce significantly new mathematical constructions or relations.
So there are no new "know that"s.
This thesis does use relatively modern mathematics to approach an old problem.
The math is symmetric monoidal categories, the problem is depicting language.
So there is new "know how".\\

This thesis is computer science, a little.
The mathematics used is, to a degree, implementation agnostic.
This thesis is only concerned with the "in principle", rather than the "in practice".
There will be no code demonstration nor machine learning experiments, because
Computer Science is to Programming as Physics is to Engineering.
I will point out where experiments have been done.
Hypothetical procedures will be spelled out if needed.





\newthought{An idempotent on $Y^\sigma$ that splits through a discrete topology $X^\star$ does these things:}
\begin{description}
\item[The section:]{picks a subset of $s(x) \subseteq Y$ for each point $x \in X$. In order to be a split idempotent, $s(x)$ must be distinct for distinct points $x$.}
\item[The retract:]{(reading backwards) picks an open set $r(x) \in \sigma$ for each point $x \in X$. In order to be a split idempotent, $r(x)$ only overlaps $s(x)$ out of all other selected shapes: \[r(x) \cap s(x') = 1 \iff x = x'\]}
\end{description}
The combined effect is that the shapes $s(x)$ become copiable elements of the sticky spider, just as the points are the copiable elements of a regular spider.


\begin{defn}[\textbf{PLoc}]
A \emph{partial frame homomorphism} is a partial map between frames $\psi: A \rightarrow B$ such that, restricted to the domain of definition in $A$, $\psi$ is a frame homomorphism. The category \textbf{PLoc} is obtained in the same manner as \textbf{PLoc}, but using partial frame homomorphisms instead, which is well defined as a subcategory of \textbf{Pfn}, the category of sets and partial functions. \textbf{Loc} is thus a subcategory of \textbf{PLoc}.
\end{defn}



Both linguists and AI-researchers stand to benefit from interaction. Linguists have much to gain by treating LLMs as a rich source of empirical data for linguistic issues. For example, there is fuzzy and widely-held notion that language must somehow be intimately related to reasoning and cognition, or that all of these things are inseparably clustered under the name of "intelligence". An observation from LLMs that sheds light on this notion is that when a language model becomes sophisticated enough, it can employ chain-of-thought \citep{wei_chain--thought_2023} to bootstrap reasoning ability, which hints at an overlap between the 'narrow' task of predicting linguistic output and the very general task of pattern recognition. 


By linguistic introspection, we realise we must account for \emph{Entification} and \emph{Processising} -- the process of turning non-nouns into noun-entities and back again. If we think about English, we find that just about any word can be turned into a noun and back again (e.g. \texttt{run} by gerund to \texttt{running}, \texttt{quick} by a suffix to \texttt{quickness}, and even entire sentences \texttt{Bob drinks Duvel} can become a noun \texttt{the fact that Bob drinks Duvel}).\\

This consideration carries some linguistic interest as well. In the usual treatment of anaphora resolution, pronouns refer to nouns, for instance: \texttt{Bob drinks a beer. \underline{It} is cold.}, where \texttt{it} refers to the beer. But there are situations where pronouns can point to textual data that are not nouns. For instance: \texttt{Jono was paid minimum wage. He didn't mind \underline{it}.}, where \texttt{it} would like to refer to something like \texttt{the fact that Jono was paid minimum wage}. While there are extensions of discourse reference theory to accommodate event structures [], the issue at hand is that pronouns in the appropriate context seem to be able to refer to \emph{any meaningful part of text}. For example, \texttt{The tiles were grey. \underline{It} was a particularly depressing shade.}, where \texttt{it} seems to refer just to the entified adjective \texttt{the greyness (of the tiles)}. Or, \texttt{Alice's cake was tastier than Bob's, but \underline{it} wasn't enough so for the judges to decide unanimously.}, where \emph{it} seems to refer the entified tastiness differential of \texttt{tastier}: \texttt{the difference in tastiness between Alice and Bob's cakes}.\\

\newthought{}

Now we try to interpret these constraints in mathematical terms, staying within the graphical confines we have established in \textbf{TopRel} as much as possible. Let us denote the noun-wire type by $\Xi$. First we observe that any finite collection of noun wires $\bigotimes^n \Xi$ has to be \emph{encodable} in a single noun wire $\Xi$, because we can always interpose with \texttt{and}. We take this to mean that there will exist morphisms such that:

\[placeholder\]

Second, for any word-gate $w$ of grammatical type $\mathfrak{g}$, we ought to have noun-states and an evaluator process that witness entification and processising:

\[placeholder\]

Second-and-a-half, any morphism (or "meaningful part of text") $T \in \bigotimes^n \Xi \ , \ \bigotimes^m \Xi$ for any $n,m \in N$ -- has to be encodable as a state of $\Xi$. This is expressed as the following graphical condition:

\[placeholder\]

Condition two-and-a-half follows if the former conditions are met, provided that all text circuits are made up of a fixed stock of grammatical-gate-types:

\[placeholder\]

If we have all the above, then we can grab any part of a circuit and turn it into a noun. We can notate this using a \emph{lasso}.

\[\]

Recall that Lassos -- a graphical gadget that can encode arbitrary morphisms into a single wire -- can be interpreted in a monoidal computer. Recall that monoidal computers require a universal object $\Xi$. Here we show how in \textbf{TopRel}, by taking $\Xi := \squarehvfill$ the open unit square, we have a monoidal computer in \textbf{Rel} restricted to countable sets and the relations between them. We will make use of sticky spiders. We have to show that; $\squarehvfill$ has a sticky-spider corresponding to every countable set; how there is a suitable notion of sticky-spider morphism to establish a correspondence with relations; what the continuous relations are on $\squarehvfill$ that mimick various compositions of relations.

\footnote{These programs differ intensionally: the equalities of extensional process theories for programs are too coarse-grained to distinguish between programs with different time- and space- requirements; even after the extensional form of the program is set, computer science is also about the practical aim of finding the right implementation subject to intensional complexity, because we are impatient and real computers have finite memory. I want to remark that the monoidal computer framework [] is comprehensive for undergraduate theoretical computer science, accounting for both the extensional domain of computability theory and the intensional domain of complexity theory.}




\footnote{I am unsatisfied with how both machines and formal semanticists go about doing this. For example, when I am reading a text and I get to \texttt{Alice shuts the door}, something about the model in my head acts along with the text, possibly introducing the door if I didn't know about it before. I don't remember all of the previous words in the story, so that can't be my model. I know my model is transparent and manipulable, because I can imagine the events of the story in as much detail as I have patience for and I can imagine would have happened if this-had-happened-instead-of-that, so my model at least as much systematicity and compositionality as the language the story is written in. However good at their task vector representations in massive latent spaces are, I can't be sure that they carry that same systematicity that I have, and I want to be sure that I'm not conversing with an alien mind. On the other hand, even if it were feasible to define formal truth conditions for all the words, the truth-conditional view of meaning is completely backwards for my concerns: truth-conditions require a model to evaluate against and I am concerned with how \emph{that model} is built from language. If \texttt{Alice shuts the door} is true in any sense, it is only because my internal model has evolved in the act of reading in such a way to make it the case. The meaning of the sentence is the procedural content of how it updates my model in the act of reading, not the mathematical consequence that the sentence then happens to be true in my model. In short, I would prefer to have a conversation with something that works with language and internal representations in a similar way I do, not an alien or a pedant.}





\subsection{Spatial predicates}

The following simple inference is what we will try to capture process-theoretically:

\begin{itemize}
\item \texttt{Oxford is north of London}
\item \texttt{Vincent is in Oxford}
\item \texttt{Philipp is in London}
\end{itemize}

How might it follow that \texttt{Philipp is south of Vincent}?\\

One way we might approach such a problem computationally is to assign a global coordinate system, for instance interpreting `north' and `south' and `is in' using longitude and latitude. Another coordinate system we might use is a locally flat map of England. The fact that either coordinate system would work is a sign that there is a degree of implementation-independence.\\

This coordinate/implementation-independence is true of most spatial language: we specify locations only by relative positions to other landmarks or things in space, rather than by means of a coordinate system. This is necessarily so for the communication of spatial information between agents who may have very different reference frames.\\

So the process-theoretic modelling aim is to specify how relations between entities can be \emph{updated} and \emph{classified} without requiring individual spatial entities to intrinsically possess meaningful or determinate spatial location.\\

So far we have established how to update properties of individual entities. We can build on what we have so far by observing that a relation between two entities can be considered a property of the pair.

\[placeholder\]

Spatial relations obey certain compositional constraints, such as transitivity in the case of `north of':

\[placeholder\]

Or the equivalence between 'north of' and 'south of' up to swapping the order of arguments:

\[\]

There are other general constraints on spatial relations, such as order-independence: the order in which spatial relations are updated does not (at least for perfect reasoners) affect the resultant presentation. This is depicted diagrammatically as commuting gates:

\[placeholder\]


\begin{marginfigure}\label{fig:bigcfg}
\centering
\[\resizebox{0.4\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigcfg}}\]
\caption{Consider the sentence \texttt{Alice sees Bob quickly run to school}, which we take to be generated by the above context-free grammar derivation, read from bottom-to-top.}
\end{marginfigure}




Strictification is arguably necessary even for DisCoCat. Pregroups by themselves are thin rigid autonomous categories, meaning there is at most one morphism between any pair of objects and that there are directed cups and caps in addition to plain monoidal structure. Contra the aims of DisCoCat, this implies there is no nontrivial strong monoidal functor from pregroups into $\textbf{FinVect}^\otimes$, where strength for a monoidal functor means that $Fx \otimes Fy \simeq F(x \otimes y)$ in the codomain. This is because for any object $x$ in the pregroup, any strong monoidal $F$ into $\textbf{FinVect}^\otimes$ that interprets duals in the usual way must identify the following diagrams up to isomorphism of finite vector spaces with their duals:
\[placeholder\]
The only vector space $Fx$ for which this holds is the base field of $\textbf{Vect}$; the monoidal unit. The same argument applies for functors into $\textbf{Rel}$ -- another common semantic domain -- where the only valid target is the singleton as monoidal unit. One "fix" for this problem is to only ask for lax monoidal functors, where instead of isomorphism, only a unidirectional laxator natural transformation $Fx \otimes Fy \rightarrow F(x \otimes y)$ is required. I dislike using lax constructions just on their own, because they are an admission that there is extra stuff in $F(x \otimes y)$ we can't probe that wasn't present in $Fx \otimes Fy$. In prose, this is approximately "emergent behaviour in the whole that wasn't present in the parts", which sounds impressive, but doesn't give us any understanding because it is only assigning a name to what we do not know without going into it. What is suggested in this section is to factorise the lax monoidal semantic functor $\mathcal{P} \rightarrow \mathcal{M}$ from categorified pregroup proofs to a monoidal category as $\mathcal{P} \rightarrow \bar{\mathcal{M}} \rightarrow \mathcal{M}$ where $\bar{\mathcal{M}}$ is the strictification of $\mathcal{M}$. %The first leg of the factorisation is a strict monoidal functor, sending categorified pregroup proofs (which are thin, hence strict, i.e. all monoidal structural isomorphisms are equalities) to strictified diagrams of $\mathcal{M}$, which holds onto pregroup proof structure. The second leg destrictifies, forgetting proof structure.}




\begin{construction}
Where $\mathcal{P}$ is the monoidal category for pregroup diagrams and where $\mathcal{G}$ is the monoidal category for the string-diagrammatic presentations of the productive grammars we can define a discrete monoidal fibration $\overline{\mathcal{P}}' \rightarrow \mathcal{G}$, where $\overline{\mathcal{P}}'$ is the subcategory of the strictification of $\mathcal{P}$ as generated by Construction \ref{cons:bracketing}.
\end{construction}



\begin{marginfigure}
\centering
\caption{The concept of exchange can be glossed approximately as the following, with variable noun-entries capitalised.}

Before exchange:\\
\texttt{AGENT has THEME.}\\
\texttt{AGENT wants OBJECT.}\\

During exchange:\\
\texttt{AGENT gives THEME to PATIENT so-to get OBJECT.}\\

After exchange:\\
\texttt{AGENT has OBJECT}\\
\texttt{PATIENT has THEME}

\[\tikzfig{metaphor/timeISmoney}\]
\end{marginfigure}

\marginnote{A post-LLM theory of language requires coherence between theories of syntax -- one for the speaker and one for the listener -- such that their underlying structures are strongly equivalent and are amenable in principle to distributional or vector semantics.} Anything short of this fails to provide any understanding of language beyond what can be gleaned from an LLM. Explaining communication demands two coherent theories of syntax for speaker and listener. Practicality demands a semantics that can be learned in principle from data and represented as vectors-and-operations-upon-them. The stakes are, at worst, the entirety of linguistics as an enterprise of inquiry. At best, it's all made up and nothing really matters so I will subscribe to this standard anyway just because. To the best of my knowledge there isn't any, even partial account of language that satisfies the bare minumum. Perhaps this is due to siloisation, because I don't doubt that all of the ingredients are already present in the literature: there are popular accounts of how grammar composes (albeit practically worthless) truth-theoretic semantics [Heim and Kratzer], and there are too many equivalence proofs between different grammatical formalisms, but I can't find anything that puts the minimum all at once in the same bowl. If it does exist already, my cake is probably prettier and tastier. So in this thesis I will supply a post-LLM theory of language, and I will endeavour to do it as diagrammatically as possible. I only care to get the absolute basics right by my own standard, and I will accordingly treat everything else about language like pragmatics and gesture and whatever else as an afterthought, \emph{as it should be}: consider air resistance only after you know how motion works in a vacuum.}



, the first box depicts "start and finish $R$ and $S$ concurrently, $R$ on the left and $S$ on the right": \[\varepsilon = \varepsilon \cdot \varepsilon \mapsto x \cdot a \cdot b \mapsto \varepsilon \cdot \varepsilon = \varepsilon\]
The second, "start $R$ first. Start $S$ on the right concurrently as $R$ finishes: \[\varepsilon \mapsto x = x \cdot \epsilon \mapsto \epsilon \cdot a \cdot b = a \cdot b \mapsto \epsilon\]
The third, "start and finish $R$ first, and then start and finish $S$": \[\varepsilon \mapsto x \mapsto \varepsilon \mapsto a \cdot b \mapsto \varepsilon\]
The rest follow by symmetry.

\textbf{4.)} Start $S$ first. Start $R$ on the right concurrently as $S$ finishes: $\varepsilon \mapsto a \cdot b = a \cdot b \cdot \varepsilon \mapsto \varepsilon \cdot x = x \mapsto \varepsilon $.\\
\textbf{5.)} Do $S$ and $R$ concurrently, $S$ on the left and $R$ on the right: $\varepsilon = \varepsilon \cdot \varepsilon \mapsto a \cdot b \cdot x \mapsto \varepsilon \cdot \varepsilon = \varepsilon$.\\


\subsection{Sentences}

Before the contents of a sentence are even decided, we may decide to (top row, left to right) get another sentence ready, introduce an adverbial adjunction (such as \texttt{yesterday}), or introduce a conjunction of two sentences (such as \texttt{because}). Introducing new words yields dots that carry information of the grammatical category of the word, and there are separate rewrites that allow dots to be labelled according to the lexicon. In the bottom, we have a rewrite that allows a sentence with one unlabelled noun to be the subject of a "sentential complement verb", abbreviated \texttt{SCV}: these are verbs that take sentences as objects rather than other nouns, and they are typically verbs of cognition, perception, and causation, such as \texttt{Alice \underline{suspects} Bob drinks}. The blue-dotted lines are just syntactic guardrails that correspond to the holes in boxes of circuits later.


Within each sentence bubble, each derivation starts life as a "simple sentence", which only involves nouns and a single verb, which is either an intransitive verb that takes a single noun argument, or a transitive verb that takes two. You just can't have a (propositional) sentence without at least a noun and a verb. Within each sentence, we may start introducing nouns. From left-to-right; we may introduce a new noun (which comes with tendrils that extrude outside the sentence bubble for later use to resolve pronominal reference); split a noun so that the same noun-label is used multiply; and label a noun \emph{if it is saturated -- depicted by a solid black circle} which includes a copy of the label for bookkeeping purposes when resolving references.