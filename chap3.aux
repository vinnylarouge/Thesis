\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newpmemlabel{^_1}{1}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Text circuits for syntax}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:textcircuits}{{1}{5}{Text circuits for syntax}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}An introduction to weak n-categories for formal linguists}{6}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The category in question can be visualised as a commutative diagram.}}{7}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces When there are too many generating morphisms, we can instead present the same data as a table of $n$-cells; there is a single 0-cell $\star $, and three non-identity 1-cells corresponding to $\leavevmode {\color  {green}\alpha }, \leavevmode {\color  {orange}\beta }, \leavevmode {\color  {cyan}\gamma }$, each with source and target 0-cells $\star $. Typically identity morphisms can be omitted from tables as they come for free. Observe that composition of identities enforces the behaviour of the empty string, so that for any string $x$, we have $\epsilon \cdot x = x = \epsilon \cdot x$.}}{7}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces For a concrete example, we can depict the string $\leavevmode {\color  {green}\alpha } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {orange}\beta }$ as a morphism in a commuting diagram.}}{7}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}String-rewrite systems as 1-object-2-categories}{7}{subsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces The string-diagrammatic view, where $\star $ is treated as a wire and morphisms are treated as boxes or dots is an expression of the same data under the Poincar\'{e} dual.}}{8}{subsection.1.1.1}\protected@file@percent }
\newlabel{fig:ruleR}{{1.1.1}{8}{String-rewrite systems as 1-object-2-categories}{subsection.1.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces We can visualise the rule as a commutative diagram where $\leavevmode {\color  {magenta}R}$ is a 2-cell between the source and target 1-cells. Just as 1-cells are arrows between 0-cell points in a commuting diagram, a 2-cell can also be conceptualised as a directed surface from a 1-cell to another. Taking the Poincar\'{e} dual of this view gives us a string diagram for the 2-cell $\leavevmode {\color  {magenta}R}$.}}{8}{subsection.1.1.1}\protected@file@percent }
\newlabel{fig:cfgsig}{{1.1.1}{8}{String-rewrite systems as 1-object-2-categories}{subsection.1.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces We can describe a context-free grammar with the same combinatorial rewriting data that specifies planar string diagrams as we have been illustrating so far. Here is a context-free grammar for \texttt  {Alice sees Bob quickly run to school}. }}{8}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Tree Adjoining Grammars}{9}{subsection.1.1.2}\protected@file@percent }
\newpmemlabel{^_3}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces \begin  {construction}[Leaf-Ansatz of a CFG] Given a signature $\mathfrak  {G}$ for a CFG, we construct a new signature $\mathfrak  {G}'$ which has the same 0- and 1-cells as $\mathfrak  {G}$. Now, referring to the dashed magenta arrows in the schematic below: for each 1-cell wire type $\texttt  {X}$ of $\mathfrak  {G}$, we introduce a \emph  {leaf-ansatz} 2-cell $\texttt  {X}^\downarrow $. For each leaf 2-cell $\texttt  {X}_L$ in $\mathfrak  {G}$, we introduce a renamed copy $\texttt  {X}'_L$ in $\mathfrak  {G}'$. Now refer to the solid magenta: we construct a 3-cell in $\mathfrak  {G}'$ for each 2-cell in $\mathfrak  {G}$, which has the effect of systematically replacing open output wires in $\mathfrak  {G}$ with leaf-ansatzes in $\mathfrak  {G}'$. \end  {construction} \par \begin  {proposition} Leaf-ansatzes of CFGs are precisely TAGs with only initial trees and substitution. \begin  {proof} By construction. Consider a CFG given by 2-categorical signature $\mathfrak  {G}$, with leaf-ansatz signature $\mathfrak  {G}'$. The types $\texttt  {X}$ of $\mathfrak  {G}$ become substitution marked symbols $\texttt  {X}^{\downarrow }$ in $\mathfrak  {G}'$. The trees $\texttt  {X}_i$ in $\mathfrak  {G}$ become initial trees $\texttt  {X}^0$ in $\mathfrak  {G}'$. The 3-cells $\texttt  {X}_s$ of $\mathfrak  {G}'$ are precisely substitution operations corresponding to appending the 2-cells $\texttt  {X}_i$ of $\mathfrak  {G}$. \end  {proof} \end  {proposition} }}{10}{theorem.1.1.1}\protected@file@percent }
\newlabel{prop:cfgastag1}{{1.7}{10}{Tree Adjoining Grammars}{theorem.1.1.3}{}}
\newpmemlabel{^_2}{11}
\newpmemlabel{^_4}{11}
\newpmemlabel{^_5}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Instead of treating non-terminals as wires and terminals as effects (so that the presence of an open wire available for composition visually indicates non-terminality) the leaf-ansatz construction treats all symbols in a rewrite system as leaves, and the signature bookkeeps the distinction between nonterminals and terminals.}}{11}{theorem.1.1.3}\protected@file@percent }
\newpmemlabel{^_6}{11}
\newpmemlabel{^_7}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Adjoining is sprouting subtrees in the middle of branches. One way we might obtain the sentence \texttt  {Bob runs to school} is to start from the simpler sentence \texttt  {Bob runs}, and then refine the verb \texttt  {runs} into \texttt  {runs to school}. This refinement on part of an already completed sentence is not permitted in CFGs, since terminals can no longer be modified. The adjoining operation of TAGs gets around this constraint by permitting rewrites in the middle of trees.}}{11}{theorem.1.1.3}\protected@file@percent }
\newpmemlabel{^_8}{11}
\newpmemlabel{^_9}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces  Leaf-ansatz signature of \texttt  {Alice sees Bob quickly run to school} CFG. One aspect of rewrite systems we adapt for now is the distinction between terminal and nonterminal symbols; terminal symbols are those after which no further rewrites are possible. We capture this string-diagrammatically by modelling terminal rewrites as 2-cells with target equal to the 1-cell identity of the 0-cell $\star $, which amounts to graphically terminating a wire. The generators subscripted $L$ (for \emph  {label} or \emph  {leaf}) correspond to terminals of the CFG, and represent a family of generators indexed by a lexicon for the language. The generators subscripted $i$ (for introducing a type) correspond to rewrites of the CFG. }}{12}{theorem.1.1.3}\protected@file@percent }
\newpmemlabel{^_10}{13}
\newpmemlabel{^_11}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces TAG signature of \texttt  {Alice sees Bob quickly run to school}. The highlighted 2-cells are auxiliary trees that replace CFG 2-cells for verbs with sentential complement, adverbs, and adpositions. The highlighted 3-cells are the tree adjoining operations of the auxiliary trees. The construction yields as a corollary an alternate proof of Theorem [Joshi 6.1.1...]... \begin  {corollary} For every context-free grammar $\mathfrak  {G}$ there exists a tree-adjoining grammar $\mathfrak  {G}'$ such that $\mathfrak  {G}$ and $\mathfrak  {G}'$ are strongly equivalent -- both formalisms generate the same set of strings (weak equivalence) and the same abstract syntactic structures (in this case, trees) behind the strings (strong equivalence). \begin  {proof} Proposition \ref  {prop:cfgastag1} provides one direction of both equivalences. For the other direction, we have to show that each auxiliary tree (a 2-cell) and its adjoining operation (a 3-cell) in $\mathfrak  {G}'$ corresponds to a single 2-cell tree of some CFG signature $\mathfrak  {G}$, which we demonstrate by construction. The highlighted 3-cells of $\mathfrak  {G}'$ are obtained systematically from the auxiliary 2-cells as follows: the root and foot nodes $\texttt  {X},\texttt  {X}^\star $ indicate which wire-type to take as the identity in the left of the 3-cell, and the right of the 3-cell is obtained by replacing all non-$\texttt  {X}$ open wires $\texttt  {Y}$ with their leaf-ansatzes $\texttt  {Y}^\downarrow $. This establishes a correspondence between any 2-cells of $\mathfrak  {G}$ considered as auxiliary trees in $\mathfrak  {G}'$. \end  {proof} \end  {corollary}}}{13}{theorem.1.1.3}\protected@file@percent }
\newpmemlabel{^_12}{14}
\newpmemlabel{^_13}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Reading the central diagram in the main body from left-to-right, we additionally depict the breakdown of the complete derivation in terms of the constitituent 2-cells, and the source and target 1-cells. Evidently, all string rewriting systems with finitely many symbols and rules can be viewed as finitely presented 1-object-2-categories.}}{14}{theorem.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Tree adjoining grammars with local constraints}{15}{subsection.1.1.3}\protected@file@percent }
\newpmemlabel{^_14}{15}
\newpmemlabel{^_15}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces  Selective and null adjoining diagrammatically: a reproduction of Example 2.5 of [Joshi] which demonstrates the usage of selective and null adjoining. The notation from [Joshi] is presented first, followed by their corresponding representations in an $n$-categorical signature. The initial tree is presented as a 2-cell where the (SA) rules are rewritable nodes, that serve as sources of rewrites in the 3-cell presentations of the auxiliary trees. }}{15}{Item.3}\protected@file@percent }
\newpmemlabel{^_16}{15}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Braiding, symmetries, and suspension}{15}{subsection.1.1.4}\protected@file@percent }
\newpmemlabel{^_17}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces  Obligatory adjoining diagrammatically: a reproduction of Example 2.11 of [Joshi] which demonstrates the usage of obligatory adjoining, marked orange. The notation from [Joshi] is presented first, followed by their corresponding representations in an $n$-categorical signature. The initial tree is presented as a 2-cell where the (OA) rule is given its own 2-cell, which is the source of rewrites in 3-cell presentations of auxiliary trees. We may capture the obligatory nature of the rewrite by asking that finished derivations contain no instance of the orange 2-cell. }}{16}{Item.3}\protected@file@percent }
\newpmemlabel{^_18}{16}
\newpmemlabel{^_19}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces  In our analogy with string rewrite systems, we might like that the following rewrites are equivalent, while respecting that they are not equal, representing $x,a,b$ as blue, red, and green wires respectively. Such rewrites from the empty string to itself are more generally called \emph  {scalars} in the monoidal setting, viewed 2-categorically. }}{16}{subsection.1.1.4}\protected@file@percent }
\newpmemlabel{^_20}{16}
\newpmemlabel{^_22}{16}
\newpmemlabel{^_21}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces We may generally represent such scalars as labelled dots. A fact about scalars in a 1-object-2-category called the Eckmann-Hilton argument \begin  {color}{red}CITE \end  {color}\xspace  is that dots may circle around one another, and all of those expressions are equivalent up to homotopy. The mechanism that enables this in our setting is that the empty string is equal to copies of itself, which creates the necessary space for manoeuvering; translating into the $n$-categorical setting, expressions are equivalent up to introducing and contracting identities.}}{17}{subsection.1.1.4}\protected@file@percent }
\newpmemlabel{^_23}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces We may view the homotopies that get us from one rewrite to another as 3-cells, which produces a braid in a pair of wires when viewed as a vignette. Up to processive isotopies \begin  {color}{red}CITE \end  {color}\xspace  , which are continuous bijective transformations that don't let wires double back on themselves, we can identify two different braidings that are not continuously deformable to one another in the 3-dimensional space of the vignette. We distinguish the braidings visually by letting wires either go over or under one another.}}{17}{subsection.1.1.4}\protected@file@percent }
\newpmemlabel{^_24}{17}
\newpmemlabel{^_25}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces We can depict these swaps by movements in a cubic volume where each axis corresponds to a direction of composition. Whereas on the plane the dots have two ways to swap places -- clockwise and counterclockwise rotation -- in the volume they have two new ways to swap places -- clockwise and counterclockwise in the new dimension. Shown below are two ways to swap left-to-right sequentially composed dots by clockwise rotations in the forward-backward and up-down directions of composition:}}{18}{subsection.1.1.4}\protected@file@percent }
\newpmemlabel{^_26}{18}
\newpmemlabel{^_27}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces For example, taking our CFG signature from earlier, suspension promotes 1-cells to 3-cells and 2-cells to 4-cells. The resulting signature gives us the same diagrams, now with the added ability to consider diagrams equivalent up to twisting wires, which models a string-rewrite system with free swapping of symbol order.}}{19}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}TAGs with links}{19}{subsection.1.1.5}\protected@file@percent }
\newpmemlabel{^_28}{19}
\newpmemlabel{^_30}{19}
\newpmemlabel{^_32}{19}
\newpmemlabel{^_29}{20}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces The TAG signature and example derivation are as above. Joshi stresses that adjoining \emph  {preserves} links, and that elementary trees may become \emph  {stretched} in the process of derivation, which are fundamentally topological constraints, akin to the "only (processive) connectivity" matters criterion identifying string diagrams up to isomorphism. Moreover, TAGs evidently have links of two natures: tree edges intended to be planar, and dashed dependency edges intended to freely cross over tree edges. It is easy, but a hack, to ask for planar processive isomorphisms for tree edges and extraplanar behaviour for dependency edges: these are evidently two different kinds of structure glued together, rather than facets of some whole. Weak $n$-categories offer a unified mathematical framework that natively accommodates the desired topological constraints while also granting expressive control over wire-types of differing behaviours. One method to recover TAGs true to the original conception is to stay in a planar 1-object-2-category setting while explicitly including wire-crossing cells for dependency links. The alternative method we opt for in Section \begin  {color}{red}ref \end  {color}\xspace  is to work in a pure "only connectivity matters" setting, recovering the linear ordering of words by generating cells along a chosen wire. I do not know of any conceptual justification for why planarity is so often an implicit constraint in approaches to formal syntax. My best guesses are either that the first port of call for rewrites between 1-dimensional strings of words is a 2-dimensional setting, or it is a limitation of 2-dimensional paper as a medium of thought along with some confusion of map for territory.}}{20}{theorem.1.1.8}\protected@file@percent }
\newpmemlabel{^_31}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces With our interpretation of TAGS as weak $n$-categorical signatures, We can recover each step of the example derivation automagically in \texttt  {homotopy.io}; just clicking on where we want rewrites allows the proof assistant to execute a typematching tree adjunction. In the process of interpretation, we introduce a link wire-type (in purple), and include directed link generation and elimination morphisms for the $T$ wire-type (in blue). A necessary step in the process of interpretation (which for us involves taking a Poincar\'{e} dual to interpret nodes as wires) is a typing assignment of the tree-branches connected to terminal nodes, which we have opted to read as sharing a $T$-type for minimality, though we could just as well have introduced a separate label-type wire.}}{21}{theorem.1.1.8}\protected@file@percent }
\newpmemlabel{^_33}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces The intended takeaway is that even if you don't buy the necessity or formality of weak $n$-categories, there is always the fallback epistemic underpinning of a formal proof assistant for higher dimensional rewriting theories, which is rather simple to use if I have succeeded in communicating higher-dimensional intuitions in this section. \textbf  {N.B.} In practice when using \texttt  {homotopy.io} for the symmetric monoidal setting, it is simpler to suspend symmetric monoidal signatures to begin at 4-cells rather than 3-cells. The reason for this is that under- and over-braids still exist in the symmetric monoidal setting, and while sequentially composed braids are homotopically equivalent to the pair of identities, they are not uniquely so, thus these homotopies must be input manually. By beginning at 4-cells (or higher, due to the stabilisation hypothesis \begin  {color}{red}CITE \end  {color}\xspace  ), braid-eliminations are unique up to homotopy and can be performed more easily in the proof assistant.}}{21}{theorem.1.1.8}\protected@file@percent }
\tcolorbox@label{1}{22}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Full TAGs in weak $n$-categories}{22}{subsection.1.1.6}\protected@file@percent }
\tcolorbox@label{2}{23}
\newlabel{defn:lex}{{1.2.1}{24}{Lexicon}{theorem.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces \textbf  {How to read the diagrams in this section:} we will be making heavy use of pink and purple bubbles as frames to construct circuits. We will depict the bubbles horizontally, as we are permitted to by compact closure, or by reading diagrams with slightly skewed axes.}}{24}{theorem.1.2.1}\protected@file@percent }
\newlabel{sec:ncat}{{1.1.6}{24}{Full TAGs in weak $n$-categories}{Item.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}A generative grammar for text circuits}{24}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}A circuit-growing grammar}{24}{subsection.1.2.1}\protected@file@percent }
\newpmemlabel{^_34}{25}
\newpmemlabel{^_35}{25}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces In this toy example, obtaining the same rewrite that connects the two yellow nodes with a purple wire using only graph-theoretically-local rewrites could potentially require an infinite family of rules for all possible configurations of pink and cyan nodes that separate the yellow, or would otherwise require disturbing other nodes in the rewrite process. In our setting, strong compact closure homotopies handle navigation between different spatial presentations so that a single rewrite rule suffices: the source and target notated by dotted-black circles. Despite the expressive economy and power of finitely presented signatures, we cannot "computationally cheat" graph isomorphism: formally we must supply the compact-closure homotopies as part of the rewrite, absorbed and hidden here by the $\simeq $ notation.}}{25}{theorem.1.2.1}\protected@file@percent }
\newlabel{fig:locality}{{1.24}{25}{A circuit-growing grammar}{theorem.1.2.1}{}}
\newlabel{dfn:simpCSG}{{1.2.2}{26}{CSG for simple sentences}{theorem.1.2.2}{}}
\newlabel{prop:simpsent}{{1.2.3}{26}{}{theorem.1.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Simple sentences}{26}{subsection.1.2.2}\protected@file@percent }
\newlabel{dfn:sentCSG}{{1.2.4}{27}{Sentence structure}{theorem.1.2.4}{}}
\newlabel{prop:compsent}{{1.2.5}{27}{}{theorem.1.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Complex sentences}{27}{subsection.1.2.3}\protected@file@percent }
\newpmemlabel{^_36}{27}
\newpmemlabel{^_37}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces The dotted-blue wires do not contentfully interact with anything else, but this noninteraction disallows overgeneration cases where adpositional phrases might interject between \texttt  {SCV} verbs and their sentential complement, e.g. \leavevmode {\color  {red}\texttt  {Alice sees \underline  {at lunch} Bob drink}}. The dotted-blue wires also indicate a diagrammatic strategy for extensions to accommodate noun phrases, to be explored later.}}{27}{subsection.1.2.3}\protected@file@percent }
\newlabel{fig:sentbestiary}{{1.25}{27}{Complex sentences}{subsection.1.2.3}{}}
\newpmemlabel{^_38}{27}
\newpmemlabel{^_40}{27}
\newpmemlabel{^_39}{28}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces  \begin  {example}[\texttt  {sober} $\alpha $ \texttt  {sees drunk} $\beta $ \texttt  {clumsily dance.}] Now we can see our rewrites in action for sentences. As a matter of convention -- reflected in how the various pass- rules do not interact with labels -- we assume that labelling occurs after all of the words are saturated. We have still not introduced rules for labelling nouns: we delay their consideration until we have settled coreferential structure. For now they are labelled informally with greeks. \end  {example} }}{28}{theorem.1.2.5}\protected@file@percent }
\newlabel{fig:soberA}{{1.26}{28}{Complex sentences}{theorem.1.2.6}{}}
\newpmemlabel{^_41}{29}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces  \begin  {example}[$\alpha $ \texttt  {laughs at} $\beta $] Adpositions form by first sprouting and connecting tendrils under the surface. Because the tendril- and pass- rules are bidirectional, extraneous tendrils can always be retracted, and failed attempts for verbs to find an adpositional unsaturated noun argument can be undone. Though this seems computationally wasteful, it is commonplace in generative grammars to have the grammar overgenerate and later define the set of sentences by restriction, which is reasonable so long as computing the restriction is not computationally hard. In our case, observe that once a verb has been introduced and its argument nouns have been saturated, only the introduction of adpositions can saturate additionally introduced unsaturated nouns. Therefore we may define the finished sentences of the circuit-growing grammar to be those that e.g. contain no unsaturated nodes on the surface, which is a very plausible linear-time check by traversing the surface. \end  {example} }}{29}{theorem.1.2.6}\protected@file@percent }
\newlabel{fig:Alaughs}{{1.27}{29}{Complex sentences}{theorem.1.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Text structure and noun-coreference}{30}{subsection.1.2.4}\protected@file@percent }
\newpmemlabel{^_42}{30}
\newpmemlabel{^_43}{30}
\@writefile{lof}{\contentsline {figure}{\numberline {1.28}{\ignorespaces Only considering words, text is just a list of sentences. However, for our purposes, text additionally has \emph  {coreferential structure}. Ideally, we would like to connect "the same noun" from distinct sentences as we would circuits.}}{30}{subsection.1.2.4}\protected@file@percent }
\newpmemlabel{^_44}{30}
\newpmemlabel{^_45}{30}
\@writefile{lof}{\contentsline {figure}{\numberline {1.29}{\ignorespaces We choose the convention of connecting from left-to-right and from bottom-to-top, so that we might read circuits as we would text: the components corresponding to words will be arranged left-to-right and top-to-bottom. Connecting nouns across distinct sentences presents no issue, but a complication arises when connecting nouns within the same sentence as with reflexive pronouns e.g. \texttt  {Alice likes herself}.}}{30}{subsection.1.2.4}\protected@file@percent }
\newpmemlabel{^_46}{30}
\newpmemlabel{^_47}{30}
\@writefile{lof}{\contentsline {figure}{\numberline {1.30}{\ignorespaces Reflexive coreference would violate of the processivity condition of string diagrams for symmetric monoidal categories. Not all symmetric monoidal categories possess the appropriate structure to interpret such reflexive pronouns, but there exist interpretative options. From left to right in roughly decreasing stringency, compact closed categories are the most direct solution. More weakly, traced symmetric monoidal categories also suffice. If there are no traces, so long as the noun wire possesses a monoid and comonoid, a convolution works. If all else fails, one can just specify a new gate. We will define coreference structure to exclude such reflexive coreference and revisit the issue as an extension.}}{30}{subsection.1.2.4}\protected@file@percent }
\newlabel{fig:reflcomp}{{1.30}{30}{Text structure and noun-coreference}{subsection.1.2.4}{}}
\newpmemlabel{^_48}{31}
\newpmemlabel{^_50}{31}
\newpmemlabel{^_52}{31}
\newpmemlabel{^_54}{31}
\newpmemlabel{^_56}{31}
\newpmemlabel{^_58}{31}
\newpmemlabel{^_49}{32}
\@writefile{lof}{\contentsline {figure}{\numberline {1.31}{\ignorespaces At this point, it is worth establishing some terminology about the kinds of unsaturated nouns we have in play. The kinds of nouns are distinguished by their tails. \emph  {Lonely} nouns have no coreferences, their tails connect to nothing. \emph  {Head} nouns have a forward coreference in text; they have two tails, one that connects to nothing and the other to a noun later in text. \emph  {Middle} nouns have a forward and backward coreference; they have two tails, one that connects to a noun in some preceding sentence, and one that connects forward to a noun in a succeeding sentence. \emph  {Foot} nouns only have a backward coreference; they have a single tail connecting to a noun in some preceding sentence.}}{32}{subsection.1.2.4}\protected@file@percent }
\newlabel{fig:nounkinds}{{1.31}{32}{Text structure and noun-coreference}{subsection.1.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Text circuit theorem}{32}{subsection.1.2.5}\protected@file@percent }
\newpmemlabel{^_51}{33}
\@writefile{lof}{\contentsline {figure}{\numberline {1.32}{\ignorespaces The $n \in \mathbf  {N}$ notation indicates a family of rewrites (and generators) for each noun in the lexicon. Link-label assigns a noun to a diagrammatically linked collection of coreferent nouns, and link-propagation is a case analysis that copies a link label and distributes is across coreferent nouns. Link-rise is a case analysis to connect labels to the surface, and finally \texttt  {N}-label allows a saturated noun to inherit the label of its coreference class, which may either be a noun \texttt  {n} or a pronoun appropriate for the noun, notated $^\texttt  {*}\texttt  {n}$}}{33}{subsection.1.2.4}\protected@file@percent }
\newpmemlabel{^_53}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {1.33}{\ignorespaces We start the derivation by setting up the sentence structure using \texttt  {S}- and \texttt  {SCV}-intro rules, and two instances of \texttt  {N}-intro, one for Alice, and one for Bob. Observe how the \texttt  {N}-intro for Bob occurs within the subsentence scoped over by the \texttt  {SCV}-rule.}}{34}{theorem.1.2.8}\protected@file@percent }
\newlabel{fig:corefex1}{{1.33}{34}{Text structure and noun-coreference}{theorem.1.2.8}{}}
\newpmemlabel{^_55}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {1.34}{\ignorespaces By homotopy, we can rearrange the previous diagram to obtain the source of the linked-\texttt  {N}-intro rewrite in the dashed-box visual aid. Observe how we drag in the root of what is to be Alice's wire. Then we use the \texttt  {IV}-intro in the second sentence, which sets up the surface structure \texttt  {she laughs}, and the deep structure for bookkeeping that \texttt  {she} refers to \texttt  {Alice}.}}{34}{theorem.1.2.8}\protected@file@percent }
\newlabel{fig:corefex2}{{1.34}{34}{Text structure and noun-coreference}{theorem.1.2.8}{}}
\newpmemlabel{^_57}{34}
\@writefile{lof}{\contentsline {figure}{\numberline {1.35}{\ignorespaces By homotopy again, we can do the same for Bob, this time setting up for the $\gamma $ variant of linked-\texttt  {N}-intro which handles the case when the spawning noun is within the scope of an SCV. Then by applying a series of $\texttt  {N}_\uparrow $-swaps, the unsaturated noun is placed to the right of the intransitive verb phrase.}}{34}{theorem.1.2.8}\protected@file@percent }
\newlabel{fig:corefex3}{{1.35}{34}{Text structure and noun-coreference}{theorem.1.2.8}{}}
\newpmemlabel{^_59}{35}
\@writefile{lof}{\contentsline {figure}{\numberline {1.36}{\ignorespaces We've already done the surface derivation for the two sentences separately in Figures \ref  {fig:soberA} and \ref  {fig:Alaughs}; since neither of those derivations touch the roots of noun-wires, we can emulate those derivations and skip ahead to the first diagram. }}{35}{theorem.1.2.8}\protected@file@percent }
\newlabel{fig:corefex4}{{1.36}{35}{Text structure and noun-coreference}{theorem.1.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.37}{\ignorespaces Nouns are represented by wires, each `distinct' noun having its own wire.}}{36}{theorem.1.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.38}{\ignorespaces We represent adjectives, intransitive verbs, and transitive verbs by gates acting on noun-wires. Since a transitive verb has both a subject and an object noun, that will then be two noun-wires, while adjectives and intransitive verbs only have one.}}{36}{theorem.1.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.39}{\ignorespaces Adverbs, which modify verbs, we represent as boxes with holes in them, with a number of dangling wires in the hole indicating the shape of gate expected, and these should match the input- and output-wires of the box with the whole.}}{36}{theorem.1.2.9}\protected@file@percent }
\newlabel{prop:linkedlist}{{1.2.12}{36}{}{theorem.1.2.12}{}}
\newlabel{prop:norefl}{{1.2.13}{36}{}{theorem.1.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.40}{\ignorespaces Similarly, adpositions also modify verbs, by moreover adding another noun-wire to the right.}}{37}{theorem.1.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.41}{\ignorespaces For verbs that take sentential complements and conjunctions, we have families of boxes to accommodate input circuits of all sizes. They add another noun-wire to the left of a circuit.}}{37}{theorem.1.2.9}\protected@file@percent }
\newlabel{cons:wirejoin}{{1.2.14}{37}{Text to circuit}{theorem.1.2.14}{}}
\newpmemlabel{^_60}{37}
\newpmemlabel{^_61}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.45}{\ignorespaces We turn finished text diagrams into text circuits by operating \emph  {in situ}, with extra rules outside the grammatical system that handle connecting noun wires. }}{37}{theorem.1.2.14}\protected@file@percent }
\newpmemlabel{^_62}{37}
\newpmemlabel{^_63}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.46}{\ignorespaces  In the first step, by Lemmas \ref  {prop:linkedlist} and \ref  {prop:norefl}, we can always rearrange a finished text diagram such that the noun wires are processive.\\ \par In the second step, use the first rewrite of Construction \ref  {cons:wirejoin} to prepare the wires for connection.\\ \par In the third step, we just ignore the existence of the bubble-scaffolding and the loose scalars. We could in principle add more rewrites to melt the scaffolding away if we wanted, but who cares?\\ \par In the fourth step, we apply the second and third rewrites of Construction \ref  {cons:wirejoin} to connect the wires and eliminate nodules underneath labels. We can also straighten up the wires a bit and make them look proper.\\ \par At this point, we're actually done, because the resulting diagram \emph  {is already a text circuit up to a choice of notation}. }}{37}{theorem.1.2.14}\protected@file@percent }
\newlabel{prop:text2circ}{{1.2.15}{37}{Finished text diagrams yield unique-up-to-processive-isotopy text circuits}{theorem.1.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.42}{\ignorespaces Conjunctions are boxes that take two circuits which might share labels on some wires.}}{38}{theorem.1.2.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.43}{\ignorespaces Of course filled up boxes are just gates.}}{38}{theorem.1.2.9}\protected@file@percent }
\newpmemlabel{^_64}{38}
\newpmemlabel{^_65}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.47}{\ignorespaces  \begin  {convention}[Wire twisting] \end  {convention} Wires are labelled by nouns. We consider two circuits the same if their gate-connectivity is the same. In particular, this means that we can eliminate unnecessary twists in wires to obtain diagrammatically simpler representations. }}{38}{theorem.1.2.16}\protected@file@percent }
\newlabel{conv:twist}{{1.47}{38}{Text circuit theorem}{theorem.1.2.17}{}}
\newpmemlabel{^_66}{38}
\newpmemlabel{^_67}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.48}{\ignorespaces  \begin  {convention}[Sliding] \end  {convention} Since only gate-connectivity matters, we consider circuits the same if all that differs is the horizontal positioning of gates composed in parallel. \begin  {convention}[Reading text circuits] \end  {convention} Text circuits ought to be presented so that they can be read from top to bottom and from left to right, like English text. }}{38}{theorem.1.2.17}\protected@file@percent }
\newlabel{conv:reading}{{1.48}{38}{Text circuit theorem}{theorem.1.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.44}{\ignorespaces Gates compose sequentially by matching labels on some of their noun-wires and in parallel when they share no noun-wires, to give \underline  {text circuits}.}}{39}{theorem.1.2.9}\protected@file@percent }
\newlabel{conv:gaps}{{1.2.20}{39}{Arbitary vs. fixed holes}{theorem.1.2.20}{}}
\newlabel{conv:and}{{1.2.21}{39}{Contentless conjunctions}{theorem.1.2.21}{}}
\newlabel{conv:exists}{{1.2.22}{39}{Lonely wires}{theorem.1.2.22}{}}
\newlabel{prop:circ2text}{{1.2.23}{40}{Circuit to text}{theorem.1.2.23}{}}
\newpmemlabel{^_68}{40}
\newpmemlabel{^_69}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.49}{\ignorespaces Starting with a circuit, we may use Convention \ref  {conv:twist} to arrange the circuit into alternating slices of twisting wires and (possibly tensored) circuits, and this arrangement recurses within boxes. Slices with multiple tensored gates will be treated using Convention \ref  {conv:and}. By convention \ref  {conv:exists}, we decorate lonely wires with formal \texttt  {exists} gates, as in the \texttt  {Frank sees} box. Observe how verbs with sentential complement are depicted with grey gaps, whereas the adverb and adposition combination of \texttt  {Mac crazily laughs at Cricket} is gapless, according to Convention \ref  {conv:gaps}.}}{40}{theorem.1.2.23}\protected@file@percent }
\newpmemlabel{^_70}{40}
\newpmemlabel{^_72}{40}
\newpmemlabel{^_74}{40}
\newpmemlabel{^_76}{40}
\newpmemlabel{^_71}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.50}{\ignorespaces We then linearise the slices, representing top-to-bottom composition as left-to-right. Twist layers are eliminated, replaced instead by dotted connections indicating processive connectivity. The dashed vertical line distinguishes slices. This step of the procedure always behaves well, guaranteed by Proposition \ref  {prop:linkedlist}. Noun wires that do not participate in earlier slices can be shifted right until the slice they are introduced.}}{41}{theorem.1.2.23}\protected@file@percent }
\newpmemlabel{^_73}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.51}{\ignorespaces We recurse the linearisation procedure within boxes until there are no more sequentially composed gates. The linearisation procedure evidently terminates for finite text circuits. At this point, we have abstracted away connectivity data, and we are left with individual gates.}}{41}{theorem.1.2.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Extensions I: relative and reflexive pronouns}{41}{subsection.1.2.6}\protected@file@percent }
\newpmemlabel{^_75}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.52}{\ignorespaces By Proposition \ref  {prop:compsent}, gates are equivalent to sentences up to notation, so we swap notations \emph  {in situ}. Conventions \ref  {conv:and} and \ref  {conv:exists} handle the edge cases of parallel gates and lonely wires. Observe that the blue-dotted wiring in text diagrams delineates the contents of boxes that accept sentences.}}{42}{theorem.1.2.23}\protected@file@percent }
\newpmemlabel{^_77}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.53}{\ignorespaces Recursing notation swaps outwards and connecting left-to-right slices as sentence-bubbles connect yields a text circuit, up to the inclusion of rewrites from Conventions \ref  {conv:and} and \ref  {conv:exists}: applying the reverse of those rewrites and the reverse of text-diagram rewrites yields a valid text-diagram derivation, by Propositions \ref  {prop:compsent} and \ref  {prop:linkedlist}. We haven't formally included transitive verbs with sentential complement in our vocabulary, but it should be obvious at this point how they function with our existing machinery.}}{42}{theorem.1.2.23}\protected@file@percent }
\newpmemlabel{^_78}{42}
\newpmemlabel{^_80}{42}
\tcolorbox@label{3}{43}
\newlabel{cons:relpron}{{1.2.24}{43}{}{theorem.1.2.24}{}}
\newpmemlabel{^_79}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.54}{\ignorespaces  \begin  {example}[Introducing relative pronouns] \end  {example} Here we demonstrate derivations of \texttt  {Alice teaches at school that bores Bob} and \texttt  {Alice teaches at school that Bob attends}. The initial steps in both cases are the same, setting up the \texttt  {teaches} phrase structure and introducing a new unsaturated noun in the \texttt  {Bob} phrase to work with the relative pronoun. }}{43}{theorem.1.2.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.7}Extensions II: grammar equations}{43}{subsection.1.2.7}\protected@file@percent }
\newpmemlabel{^_82}{43}
\newpmemlabel{^_84}{43}
\newpmemlabel{^_86}{43}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.8}Extensions III: Types and Nesting}{43}{subsection.1.2.8}\protected@file@percent }
\newpmemlabel{^_81}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.55}{\ignorespaces Now have a branching derivation. We may either directly generate a transitive verb treating the relative pronoun as a subject, or we may first perform an $\texttt  {N}_\uparrow $-swap first and then generate a transitive verb, treating the relative pronoun as an object. Now the ends of either branch can be labelled to recover our initial examples.}}{44}{theorem.1.2.25}\protected@file@percent }
\newpmemlabel{^_83}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.56}{\ignorespaces  \begin  {example}[\textbf  {Passive voice}] \[\texttt  {School bores Bob} = \texttt  {Bob \underline  {is bored by} school}\] \end  {example} Twists in wires can be used to model passive voice constructions, which amount to swapping the argument order of verbs. In the original paper \begin  {color}{red}CITE \end  {color}\xspace  , a more detailed analysis including the flanking words \texttt  {\texttt  {is} bored \texttt  {by}} involves introducing a new diagrammatic region, which is modelled by having more than a single 0-cell in the $n$-categorical signature. }}{44}{subsection.1.2.7}\protected@file@percent }
\newpmemlabel{^_88}{44}
\newpmemlabel{^_90}{44}
\newpmemlabel{^_85}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {1.57}{\ignorespaces  \begin  {example}[\textbf  {Copulas}] \[\texttt  {Red car} = \texttt  {Car is red}\] \end  {example} Modifiers such as adjectives and adverbs when they occur before their respective noun or verb are called \emph  {attributive}. When modifiers occur after their respective target, they are called \emph  {predicative}. In English, without the aid of \texttt  {and}, only a single predicative modifier is permissible, e.g. \texttt  {big red car} and \texttt  {big car is red} are both acceptable, but \leavevmode {\color  {red}texttt{car is big red}} is not. There is no issue in introducing rewrites to handle copular modifier constructions in text diagrams, and in text circuits, there is no distinction between either kind of modifier.}}{45}{theorem.1.2.26}\protected@file@percent }
\newpmemlabel{^_87}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {1.58}{\ignorespaces  \begin  {example}[\textbf  {Possessive pronouns}] \[\texttt  {Bob\underline  {'s} pub} = \texttt  {Pub \underline  {that} Bob \underline  {owns}}\] \end  {example} This example, along with other grammar equations, was first introduced in the pregroups and internal wirings context in \begin  {color}{red}CITE \end  {color}\xspace  . Possessive pronouns are placed contiguously in between noun-phrases, for which the diagrammatic technology we developed for placing adpositions can be repurposed. Possessive pronouns may be dealt with by a single rewrite that relies on the presence of a transitive ownership verb in the lexicon, which corresponds to a box-analysis in text circuits. }}{45}{theorem.1.2.27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Text circuits: details, demos, developments}{45}{section.1.3}\protected@file@percent }
\newlabel{sec:circs}{{1.3}{45}{Text circuits: details, demos, developments}{section.1.3}{}}
\newpmemlabel{^_89}{46}
\@writefile{lof}{\contentsline {figure}{\numberline {1.59}{\ignorespaces  \begin  {example}[\textbf  {Intensifers}] \[\texttt  {Alice very quickly runs}\] \end  {example} The deep nodes of a text diagram may be equivalently viewed as evaluators in a symmetric monoidal closed setting, and the surface nodes as states for the evaluators. By Curry-Howard-Lambek, this view recovers typelogical grammar settings where composition is some variant of modus ponens. So long as the typing rules are operadic or treelike (which is almost always the case for typelogical grammars, as there are rarely gentzen-style sequent rules that generate multiple outputs), we may instead use a notation where parent edges of evaluation branches become nesting boxes. }}{46}{subsection.1.2.8}\protected@file@percent }
\newpmemlabel{^_91}{46}
\@writefile{lof}{\contentsline {figure}{\numberline {1.60}{\ignorespaces  \begin  {example}[\textbf  {Comparatives}] \[\texttt  {Alice drinks \underline  {less than} Bob drinks}\] \end  {example} Just as transitive verbs modify two nouns, comparatives are higher-order transitive modifiers that act on the data of verbs or adjectives. A benefit of the symmetric monoidal closed view is that it easily accommodates mixed-order and multi-argument modifiers. }}{46}{theorem.1.2.29}\protected@file@percent }
\citation{wilson_string_2022}
\citation{merry_reasoning_2014,quick_-logic_2015,zamdzhiev_rewriting_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Avenues I: syncategorematicity as distributivity}{47}{subsection.1.3.1}\protected@file@percent }
\newpmemlabel{^_92}{47}
\newpmemlabel{^_94}{47}
\newpmemlabel{^_96}{47}
\newpmemlabel{^_93}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.61}{\ignorespaces  \begin  {example}[\textbf  {Syncategorematicity I}] \[\texttt  {Alice \underline  {and} Bob drink}\] \end  {example} \emph  {Syncategorematic} are roughly words have contextually-dependent semantics. In our terms, since we consider the semantics of text circuits to be underpinned by monoidal functors that reify the circuits in a target category, syncategorematic words such as \texttt  {and} may be treated as distributive laws. In this example, \underline  {and} occurs as a conjunction of nouns, and is eliminated by distributive-law rewrites within the deep structure of the text diagram \emph  {before translation into circuits}. Note that what is meant by \emph  {distributive} here is, in string-diagrammatic terms, precisely the same as that in algebra, for expressions such as $a \times (b + c) = (a \times b) + (a \times c)$. A new copy-node for verb labels that has rewrites for all verbs facilitates distribution, and the deep \texttt  {and} nodes come in a tensor-dentensor pair analogous to those for nonstrict string diagrams \begin  {color}{red}CITE \end  {color}\xspace  . Sources of rewrites are outlined in dashed boxes. }}{48}{subsection.1.3.1}\protected@file@percent }
\newpmemlabel{^_95}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.62}{\ignorespaces  \begin  {example}[\textbf  {Syncategorematicity II}] \[\texttt  {Bob drinks \underline  {and} smokes}\] \end  {example} In this example, the same word \texttt  {and} is a conjunction of verbs. In this case we choose to interpret the conjunction of verbs as sequential composition, so there is no need for a corresponding detensor for the \texttt  {and} of verbs. }}{48}{theorem.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Avenues II: determiners and quantifiers in context}{48}{subsection.1.3.2}\protected@file@percent }
\newpmemlabel{^_97}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.63}{\ignorespaces  \begin  {example}[\textbf  {Coordination}] \[\texttt  {Alice \underline  {and} Bob drink beer \underline  {and} wine \underline  {respectively}}\] \end  {example} We stand to win in terms of conceptual economy for modelling; more complex phenomena of text structure such as coordination appear to be resolvable in the same framework of distributivity-law rewrites. }}{49}{theorem.1.3.2}\protected@file@percent }
\newpmemlabel{^_98}{49}
\newpmemlabel{^_99}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.64}{\ignorespaces  \begin  {example}[\textbf  {Determiners I}] \[\texttt  {Bob drinks \underline  {the} beer} \text  { (among drinks)}\] \end  {example} Here, \texttt  {drinks} is considered transitive and \texttt  {the beer} a nesting box for \texttt  {drinks} that reaches over to contextual wires representing a selection of beverages. In this case (relying on the implicit uniqueness of \texttt  {the}), a series of \texttt  {beer?} tests may be computed, and the best match chosen as the resulting argument for \texttt  {drinks}. }}{49}{subsection.1.3.2}\protected@file@percent }
\newpmemlabel{^_100}{49}
\newpmemlabel{^_101}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.65}{\ignorespaces  \begin  {example}[\textbf  {Determiners II}] \[\texttt  {Bob drinks \underline  {a} beer} \text  { (among drinks)}\] \end  {example} We take the logical (and pragmatic) reading of \texttt  {a} as $\exists ! x: \texttt  {beer?}(x) \wedge \texttt  {drinks?}(\texttt  {Bob},x)$. Subject to having a method to hold onto alternatives -- in essence an inquisitive semantics approach -- we may create alternative circuits for each successful \texttt  {beer?} test. }}{49}{theorem.1.3.4}\protected@file@percent }
\newpmemlabel{^_102}{49}
\newpmemlabel{^_104}{49}
\newpmemlabel{^_106}{49}
\newpmemlabel{^_103}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.66}{\ignorespaces  \begin  {example}[\textbf  {Determiners III}] \[\texttt  {Bob drinks \underline  {a} beer} \text  { (that we didn't know about)}\] \end  {example} When there are no beers in context, the same statement takes on a dynamic reading: it constitutes the introduction of a beer into discourse. In terms of text circuits, this amounts to introducing a novel beer-state and beer-wire. Determining an appropriate setting to accommodate "arbitrary" vs. "concrete" beers (c.f. Fine's arbitrary objects \begin  {color}{red}CITE \end  {color}\xspace  ) requires further research and experimentation, but preliminarily it is known that density matrices are capable of modelling semantic entailment \begin  {color}{red}CITE \end  {color}\xspace  , at the computational cost of adopting the kronecker product. This diagram doesn't typecheck, but note that it doesn't have to, because our strategy for evaluation of determiners treats circuits as syntactic objects to be manipulated. }}{50}{theorem.1.3.5}\protected@file@percent }
\newpmemlabel{^_105}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.67}{\ignorespaces  \begin  {example}[\textbf  {Quantifiers I}] \[\texttt  {Bob drinks \underline  {all the beers}} \text  { (in context)}\] \end  {example} In a finitary context, drinking all the beers amounts to applying the distributivity of \texttt  {and}. }}{50}{theorem.1.3.6}\protected@file@percent }
\newpmemlabel{^_107}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.68}{\ignorespaces  \begin  {example}[\textbf  {Quantifiers II}] \[\texttt  {Bob drinks \underline  {all} beers} \text  { (generic)}\] \end  {example} Without the determiner \texttt  {the}, this becomes a generic statement, which logically amounts to (analysing the usual conditional as a disjunction) $\forall x: \neg \texttt  {beer?}(x) \vee \texttt  {drinks?}(\texttt  {Bob},x)$. We can treat generic universal quantifiers of this kind in at least two ways. The first essentially truth-conditional approach is to treat the generic as a process-theoretic condition governing measurements: whenever it is the case that something is a beer, it is the case that Bob drinks it. The second "inferential" appraoch is to treat the generic as a rewrite of text circuits conditioned on a beer test: whenever something is a beer we may add on a gate witnessing that Bob drinks that beverage. }}{50}{theorem.1.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}A modern mathematician's companion to Montague's "Universal Grammar"}{50}{section.1.4}\protected@file@percent }
\citation{montague1970universal}
\citation{montague1973proper}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}What did Montague consider grammar to be?}{51}{subsection.1.4.1}\protected@file@percent }
\newlabel{sec:monty}{{1.4.1}{51}{What did Montague consider grammar to be?}{subsection.1.4.1}{}}
\newlabel{algdata}{{1.4.1}{52}{Generating data of an Algebra}{theorem.1.4.1}{}}
\newlabel{ids}{{1.4.2}{52}{Identities}{theorem.1.4.2}{}}
\newlabel{constants}{{1.4.3}{52}{Constants}{theorem.1.4.3}{}}
\newlabel{comp}{{1.4.4}{52}{Composition}{theorem.1.4.4}{}}
\newlabel{polyop}{{1.4.5}{52}{Polynomial Operations}{theorem.1.4.5}{}}
\newlabel{homo}{{1.4.6}{52}{Homomorphism of Algebras}{theorem.1.4.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}On the historical inevitability of text diagrams}{54}{subsection.1.4.2}\protected@file@percent }
\ttl@finishall
\gdef \@abspage@last{54}
