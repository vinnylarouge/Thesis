\chapter{Corrections, as a letter to the reader}

Dear Reader,

How are you? I am well, thank you.

Forgive me for writing in this informal register; it is easier for me than the academic style (which I am no good at), and I would like to get these corrections done as painlessly as possible. You see, in the year or so it has taken for me to get around to these corrections, a lot has happened. I have gotten married, and I am soon to be a father. So I've learnt that there are more important things in life than a thesis, and I have otherwise been busy. While we are on apologies, please forgive me also for the tone of this work, which vacillates between serious and light; in my defense, I am constitutionally a trifler \cite{} (or else I wouldn't have gotten into formal linguistics), and I am still in the process of finding my voice as a writer.

There are several things I would like to get through in this letter, the main purpose of which is to settle most of the substantive corrections (recommended to me by my examiners Stefano Gogioso and Jules Hedges) in one go. I should preface by reminding you that all of this is written with a bit of distance from the rest of the work, and also that any opinions expressed here are strictly my own.

\section{Does it work?}

In the most important sense, no.

I will elaborate what I mean. There is a classical conception of structured approaches to AI/ML that permits capacities that go beyond what connectionist approaches are capable of, grounded in a kind of quasi-magical conception of mathematics as the ultimate form of understanding of --- and hence control over --- a phenomenon. It appears that part of the intellectual shock of generative AI is that one does not need to understand the mechanics of complicated phenomena in order to reliably induce them, given sufficient data and compute. This is obviously disappointing to many, and the highest sort of achievement that this thesis could have achieved is to have been a source of hope. Even a year ago the prospects seemed grim, and part of what took me so long to write the introduction was to compromise the positive vision from some demonstration of the complete superiority of structured approaches (basically no longer tenable after the advent of ChatGPT with RLHF) to something weaker, such as a performant way to synthesise structural/symbolic and connectionist approaches that has some other kind of benefit, such as "interpretability" (whatever that means.)

So taking \emph{"does it work?"} to mean \emph{"does it justify the activity of mathematically formulating the structure of language with respect to non-instrinsic measures of value (such as practical application)?"}, the answer is no, modulo most measures of value that people would care about. I say this because we've done some experiments.

\subsection{Have experiments been done?}

The approaches sketched out in Section \ref{} have been tried with neural networks in various ways by masters' students taking the Distributed Models of Meaning course offered in MFoCS here at Oxford, and separately it has been tried in the quantum setting \ref{} by talented scientists and engineers at the company at which I am employed at the time of writing. In the classical case, it works for some bAbI tasks, but at the cost of hardcoding the structure of the queries, and it doesn't outperform a transformer.

I could be wrong; it could be that all of the experiments done so far were in compute and data regimes that are too small to be indicative of how these approaches may scale. It could be that rather than the level of words and sentences there is some structural benefit to be obtained at the textual level, and so on, but I am not personally holding out hope any more. I do maintain that the symmetric monoidal (and hence string-diagrammatic) approach to formalising the structure of language is the best and most natural way to synthesise the mathematics with modern (practical) machine learning, so the fact that it doesn't work all that well leaves me with a very dim view of everything else. As a result of these experiments and other experiences, my own view of the role of structure has been further demoted. As far as natural language is concerned, at worst such structure appears to be some crutch for us humans: inductive biases that constrain and unnecessarily burden more sophisticated algorithms that can deal with the complexity of "the real world". At best, it appears to be tradeoff: computationally cheaper but worse answers.

\section{Can you situate this work with respect to the literature in formal linguistics?}

Look, it's not in the culture of my people to read \cite{}. It's either uninteresting so we put the book down, or if it is interesting, we put the book down and try to rederive it for ourselves. Accordingly, if armchair introspection was good enough for Harris and Chomsky and so on, I figured it was good enough for me too. Before I go on to do the scholarly thing, I'll reiterate what I told my examiners out of protest: I don't respect \emph{formal} linguists or their methods, in fact they disgust me, and I can hardly stand to look at even my own meagre work in the area. Since I'm probably the first (and, god willing for everyone's sake, the only) formal linguist to have used infinity-categories to model natural language syntax and to have provided a categorical semantics of metaphor starting from syntax, I defer to my own authority in concluding that the mathematics of formal linguistics in the literature (including this work) is not impressive, unfit for purpose, and not worth wasting any further time on. To be fair, I think there is a lot of interesting \emph{linguistics} out there --- in my mind the kind of distinction between formal versus respectable linguistics is exemplified by Daniel Everett uncovering the structure of Pirah\~{a} in the field, causing the formalist Chomsky to move the goalposts on what recursion means \cite{} --- but I have grown increasingly convinced that all of the effort to cast linguistics in mathematical terms up to now is best viewed as a kind of devotional or religious activity, a kind of benign way to kill time. Much like quantum computers at the moment, the only practical problem formal linguistics solves is the gainful employment of overeducated fools such as myself.

One way to summarise things is that the technical contents of this thesis point towards a nearby counterfactual history where all the linguists in the "Garden of Eden period" [Partee?] knew some of the modern structuralist mathematics that their programme obviously would have profited from. The fractiousness of linguists notwithstanding, it is my opinion that the interdisciplinarity of this thesis is accidental: from the perspective of the counterfactual history, the division of formal approaches to syntax and semantics (and some of the subdivisions thereof) would appear contrived. I am aware that this could come across as pretty arrogant and patronising talk, which is not my intention this time, because the point I am ultimately trying to make is that nothing in this thesis is particularly special, and could have been reinvented by anyone. Because I thought things through for myself for the most part, and since the relevant developments were due to collaborations with the similarly ignorant, text circuits and the other contents of this thesis owe no substantial intellectual debt to linguistics outside of perhaps Lambek and Firth, themselves far from the mainstream. Consequently, all of the parallels I am about to point out between the current stream of development and the main body of literature are independent rediscoveries. I take these parallels as indications of the "naturalness" of ideas that could have come about anytime and independently of specific individuals, and accordingly as evidence for the "nearbyness" of the counterfactual history I am trying to gesture at.

\subsection{Pregroups to Text Circuits vs. Transformational Grammar to Formal Semantics, as an addendum for the cartoon literature review}

The cartoon version of the literature review is folkloric, autochtonal, and maybe a little jingoistic: it is a caricature or myth of the field that we in it tell ourselves. I think that is sufficient for most readers, but if you are here, I owe you a critical and comparative retelling. In my view, the development of DisCoCat is only two minor counterfactuals removed from the lineage of mainstream formal semantics from Montague onto Heim \& Kratzer and onwards. Moreover, both of these counterfactuals rest only on the difference of when they began relative to the ambient development of mathematical formalisms and available computing.\\

\subsubsection{Counterfactual 1: Truth-conditional vs. Vectorial Semantics}

Montague semantics may be essentially characterised as the meeting of two ideas [CITE]: structure-preserving maps from syntax, and taking truth-conditions to be the essential data of semantics. On some accounts, only the former aspect of compositionality of semantics according to syntax is essential [CITE]. Accordingly, the first counterfactual is just the swapping of truth-conditional for vectorial semantics. Today there are several good reasons to prefer the latter over the former. First, the view that truth-conditions alone are the \emph{sine qua non} of natural language meanings has been incompatible with correspondence theories of truth at least since Barr fixed Putnam's permutation argument [CITE, CITE]. Second, vectors as lists-of-numbers are more expressive and computationally practical, so much so in its current form that the very need for a formal account of "semantics" is put to question. Third, with a rare few exceptions, the truth-conditional programme and its descendents are bankrupt, and worse, have terrible mathematical taste. A lot of mathematical structure has been marshalled [CITE,CITE,CITE] to salvage the programme by trying to force intensions and pragmatics and everything-in-the-world into the propositional mould, and it is unclear what all of this mathematics buys us except for more of the same. In practical terms, the increasing extent to which statistical language models adequately handle semantics exactly matches the decreasing extent to which a complicated mathematical account of the same is warranted, and in theoretical terms it would be definitionally preposterous to seriously assert that the study of the mathematical models themselves lends insight into the phenomena they are intended to be surrogates for.

But all this criticism can only be said with the benefit of hindsight, and to give credit, it all must have seemed like a very good idea at the time. A model-theoretic, truth-conditional account of semantic data was the natural choice for a concrete target for the structure-preserving map, I speculate, for several reasons: Montague himself was a logician, and truth-conditions were at once flexible enough to capture (to a logician's satisfaction) some semantic phenomena of interest, while being amenable to computation \emph{by hand}, as was necessarily the case at the time owing to the lack of computers. Certainly there was adequate sophistication manipulating vectors by that time as well, but the vectorial view would have been more difficult to calculate with, even restricted to a setting without nonlinearities. It could be argued that, in any case, vectorial semantics in the form of word-embeddings requires a degree of data-storage and computing faculties that would not have been available until fairly recently. Moreover, even the theoretical soil was arguably unready: category theory was insufficiently spread and understood, and hence a broadly accessible mathematical understanding of structure and structure-preservation outside of particular concrete instances was unavailable.

\subsubsection{Counterfactual 2: Generative vs. Typelogical views of syntax}

- the usual mathematical conception of syntax is combinatoric --- see formal language theory.
-- now "generative" in some circles is just synonymous with "formal", but there is the original sense in which mathematical machinery generates correct sentences.
- the typelogical alternative is the proof-theoretic counterpart to a combinatoric conception, where parsing rather than generation takes precedence.
- parsing is the more practical thing for computers; compare to generation, where it becomes very complicated to make generation dependent on a semantic basis.

\subsection{Confession: Criticisms of Quantum Linguistics (and of the Academic-Industrial complex more generally)}

In light of the above counterfactuals, here are two implications of the quantum linguistics myth that I wish to dismiss here. The first that is often touted is that there is some kind of fruitful and practical synthesis to be won from the meeting of grammatical structure and vectorial semantics. This is the same kind of professionally-sensible claim that certain mathematicians will sometimes make in other domains too: that a deep consideration of structure will ultimately pay practical dividends. As far as I can tell in the case of DisCo and offshoots, this hasn't yet been demonstrated to be true. There's just no setting we know of, classicial or quantum, where deliberately introducing grammatical structure helps with any practical task, and shifting the goalposts to interpretability or whatever else has not worked either. So it isn't for lack of technical ability or imagination, it just appears to be that anything you could have done with "structure" or "composition" you could have also done without. This isn't to say that the structural view is totally without merit --- it is certainly more human-friendly and aids in Interpretability writ large as subsuming pedagogy and communication --- it's just impractical. This was surprising and disappointing for me, because of a more deep-seated belief in myself I have had to kill, that \emph{structure is magic}. If you are a computational linguist, I welcome you to try synthesising your structural formalisms with vectorial semantics across similar bridges as are built here, and I would be happy to be proved wrong about the importance and power of structure: the disenchanted view that I currently hold is that adding structure is in general a way to get computationally cheaper but worse answers.

There is a second, unspoken implication that is more seductive; that there exists some deep and fundamental unity between quantum theory and natural language. This is a rather commonplace sin more generally, that in some field XYZ with relatively little mathematical sophistication someone will steal the valour of physics by squatting on "Quantum XYZ", or "Quantum-inspired XYZ", when all they really mean is e.g. the use of noncommuting operators or tensor products or density matrices or some other narrow mathematical facet of quantum theory. Such views by themselves may be harmless, but in conjunction with the vaguely held but common assumption of mathematical realism [CITE], we find ourselves in trouble. The extent to which there are quantum theories of linguistics or cognitive science or anything else deep and multifaceted is that there are mathematical paintbrushes that have been used to illustrate quantum theory with which we can also better sketch and appreciate certain limited phenomena in other domains. However, even this honest appraisal is co-opted by a blameless form of motte-and-bailey, where a serious faction will disavow mysticism, but the field as a whole survives by luring na\"{i}ve researchers with the seductive implication.

These objections would usually be fatal, but as it goes, the alternatives are no better; mostly worse. In fact, any defensive manoeuvre that works to justify formal linguistics as an academic activity or otherwise will provide cover for quantum linguistics too, with minor modifications such as swapping out "quantum" for some exotic dialect of logic. I think that's worth reiteration: if you think what I'm saying about quantum linguistics is bad, consider that the rest of formal linguistics is worse off in terms of what it actually does, or the kind of understanding that is gained. As a personal aside, and not referring to any subfield in particular, I think this sort of dysfunction is a natural consequence of the sociology of academic and industrial research. Insofar as academia generally (1-) does not have the resources to turn theory into practice and that (2-) there is pressure to publish, there is an understandable incentive for theorists to (-2) tell stories valued according to doctrinal measures of niceness, with (-1) no corresponding selection pressure for whether those stories actually translate to anything in the real world. It is actually a na\"{i}ve view --- based on the simpleminded belief that valuation corresponds to effective technology --- that these myths crafted in ivory towers immediately perish in the sunlight of a free market economy. While these myths are endless sources of pain for the engineers who must bring them into fruition, they are crucial memetic pillars for kayfabe or some other epistemic asymmetry that keeps morale high for the technically uninitiated, which includes investors. In this sense academics and capital are natural allies against embattled engineers and builders, an observation which might serve as a sufficient axiom to deduce and explain much of the dynamics of the academic-industrial complex.

\subsection{On Deep Structure, the Universal Base Hypothesis, and the "Lexical Objection"}

???

\subsection{On communication, and the mathematical infeasibility of the Autonomy of Syntax}

\subsection{On frameworks for rewriting systems}

\subsection{On formality in cognitive semantics}