@misc{khatriAnatomyAttention2024,
  title = {On the {{Anatomy}} of {{Attention}}},
  author = {Khatri, Nikhil and Laakkonen, Tuomas and Liu, Jonathon and {Wang-Ma{\'s}cianica}, Vincent},
  year = {2024},
  month = jul,
  number = {arXiv:2407.02423},
  eprint = {2407.02423},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.02423},
  urldate = {2024-08-24},
  abstract = {We introduce a category-theoretic diagrammatic formalism in order to systematically relate and reason about machine learning models. Our diagrams present architectures intuitively but without loss of essential detail, where natural relationships between models are captured by graphical transformations, and important differences and similarities can be identified at a glance. In this paper, we focus on attention mechanisms: translating folklore into mathematical derivations, and constructing a taxonomy of attention variants in the literature. As a first example of an empirical investigation underpinned by our formalism, we identify recurring anatomical components of attention, which we exhaustively recombine to explore a space of variations on the attention mechanism.},
  archiveprefix = {arXiv},
  keywords = {68T01 18M30,Computer Science - Machine Learning,I.2.6,Mathematics - Category Theory},
  note = {Comment: Replaced to fix typos},
  file = {/Users/vincent/Zotero/storage/RSWETZ89/Khatri et al. - 2024 - On the Anatomy of Attention.pdf;/Users/vincent/Zotero/storage/CSC6CMUY/2407.html}
}
