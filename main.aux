\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newpmemlabel{^_1}{1}
\@writefile{toc}{\contentsline {chapter}{\numberline {0}Synopsis}{5}{chapter.0}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Let's say that \textbf  \emph  {{the meaning of text is how it updates a model.}} So we start with some model of the way things are, modelled as data on a wire.}}{6}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Text updates that model; like a gate updates the data on a wire.}}{6}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Text is made of sentences; like a circuit is made of gates and wires.}}{6}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Let's say that \textbf  {\emph  {The meaning of a sentence is how it updates the meanings of its parts.}} As a first approximation, let's say that the \emph  {parts} of a sentence are the nouns it contains or refers to. Noun data is carried by wires. Collections of nouns are related by gates, which play the roles of verbs and adjectives.}}{6}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gates can be related by higher order gates, which play the roles of adverbs, adpositions, and conjunctions; anything that modifies the data of first order gates like verbs.}}{6}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}What this thesis is about}{6}{section.0.1}\protected@file@percent }
\citation{joyal_geometry_1991,joyal_geometry_nodate,maclane_natural_1963,lane_categories_2010,selinger_survey_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In practice, higher order gates may be implemented as gates that modify parameters of other gates. Grammar, and \emph  {function words} -- words that operate on meanings -- are in principle absorbed by the geometry of the diagram. These diagrams are natural vehicles for \emph  {dynamic semantics}, broadly construed, where states are prior contexts and sentences-as-processes update prior contexts.}}{7}{section.0.1}\protected@file@percent }
\citation{vaswani_attention_2017}
\citation{openai_chatgpt_2022}
\citation{bastian_google_2022}
\citation{teddy_teddynpc_i_2022}
\citation{thompson_gpt-35_2022}
\citation{mcshane_linguistics_2021}
\citation{church_pendulum_2011}
\citation{hendrycks_measuring_2021}
\citation{}
\citation{}
\citation{floridi_fourth_2014}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}\textbf  {Question:} What is the practical value of studying language when Large Language Models exist?}{8}{section.0.2}\protected@file@percent }
\citation{sutton_bitter_2019}
\citation{chomsky_new_2000}
\citation{mollica_humans_2019}
\citation{herculano-houzel_remarkable_2012}
\citation{chowdhery_palm_2022,narang_pathways_2022}
\citation{khan_what_2023}
\citation{tom_goldstein_tomgoldsteincs_training_2022}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}\textbf  {First Reply:} I don't know. Maybe explainability, maybe something else.}{9}{section.0.3}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}\textbf  {Objection:} You're forgetting the bitter lesson.}{11}{subsection.0.3.1}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{bender_climbing_2020}
\citation{searle_minds_1980}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2}\textbf  {Objection:} GOFAI? GO-F-yourself.}{12}{subsection.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3}\textbf  {Point of Information:} What is computational irreducibility?}{12}{subsection.0.3.3}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.4}\textbf  {Objection:} How does any of this improve capabilities?}{13}{subsection.0.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.5}\textbf  {Objection:} Aren't string diagrams just graphs?}{13}{subsection.0.3.5}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.6}\textbf  {Objection:} Pictures are heuristics, not math}{14}{subsection.0.3.6}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}\textbf  {Second Reply:} LLMs don't help us understand language; how might string diagrams help?}{15}{section.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}\textbf  {Objection:} Isn't the better theory the one with better predictions?}{15}{subsection.0.4.1}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}\textbf  {Objection:} What's wrong with $\lambda $-calculus and sequent calculi and graphs?}{17}{subsection.0.4.2}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Synopsis of the thesis}{18}{section.0.5}\protected@file@percent }
\bibstyle{plain}
\bibdata{thesis_intro}
\bibcite{bastian_google_2022}{{1}{}{{}}{{}}}
\bibcite{bender_climbing_2020}{{2}{}{{}}{{}}}
\bibcite{chomsky_new_2000}{{3}{}{{}}{{}}}
\bibcite{chowdhery_palm_2022}{{4}{}{{}}{{}}}
\bibcite{church_pendulum_2011}{{5}{}{{}}{{}}}
\bibcite{floridi_fourth_2014}{{6}{}{{}}{{}}}
\bibcite{hendrycks_measuring_2021}{{7}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Bibliography}{21}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{herculano-houzel_remarkable_2012}{{8}{}{{}}{{}}}
\bibcite{joyal_geometry_1991}{{9}{}{{}}{{}}}
\bibcite{khan_what_2023}{{10}{}{{}}{{}}}
\bibcite{lane_categories_2010}{{11}{}{{}}{{}}}
\bibcite{maclane_natural_1963}{{12}{}{{}}{{}}}
\bibcite{mcshane_linguistics_2021}{{13}{}{{}}{{}}}
\bibcite{mollica_humans_2019}{{14}{}{{}}{{}}}
\bibcite{narang_pathways_2022}{{15}{}{{}}{{}}}
\bibcite{openai_chatgpt_2022}{{16}{}{{}}{{}}}
\bibcite{searle_minds_1980}{{17}{}{{}}{{}}}
\bibcite{selinger_survey_2010}{{18}{}{{}}{{}}}
\bibcite{sutton_bitter_2019}{{19}{}{{}}{{}}}
\bibcite{teddy_teddynpc_i_2022}{{20}{}{{}}{{}}}
\bibcite{thompson_gpt-35_2022}{{21}{}{{}}{{}}}
\bibcite{tom_goldstein_tomgoldsteincs_training_2022}{{22}{}{{}}{{}}}
\bibcite{vaswani_attention_2017}{{23}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{25}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:stringdiagrams}{{2}{25}{Background}{chapter.2}{}}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Process Theories}{26}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}What does it mean to copy and delete?}{29}{subsection.2.1.1}\protected@file@percent }
\newlabel{relcopy}{{2.1.18}{29}{Sets and relations}{theorem.2.1.18}{}}
\newlabel{cocom}{{2.1}{30}{What does it mean to copy and delete?}{equation.2.1.1}{}}
\newlabel{ex:copyablestate}{{2.1.19}{30}{Not all states are copyable}{theorem.2.1.19}{}}
\newlabel{ft:determinism}{{2.1.20}{30}{}{theorem.2.1.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}What is an update?}{31}{subsection.2.1.2}\protected@file@percent }
\newlabel{ss:update}{{2.1.2}{31}{What is an update?}{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Pregroup diagrams and correlations}{33}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Equational Constraints and Frobenius Algebras}{33}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Processes, Sets, and Computers}{33}{subsection.2.1.5}\protected@file@percent }
\citation{}
\citation{}
\newlabel{sec:proctheory}{{2.1.5}{35}{Processes, Sets, and Computers}{subsection.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}An introduction to weak n-categories for formal linguists}{36}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The category in question can be visualised as a commutative diagram.}}{37}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces When there are too many generating morphisms, we can instead present the same data as a table of $n$-cells; there is a single 0-cell $\star $, and three non-identity 1-cells corresponding to $\leavevmode {\color  {green}\alpha }, \leavevmode {\color  {orange}\beta }, \leavevmode {\color  {cyan}\gamma }$, each with source and target 0-cells $\star $. Typically identity morphisms can be omitted from tables as they come for free. Observe that composition of identities enforces the behaviour of the empty string, so that for any string $x$, we have $\epsilon \cdot x = x = \epsilon \cdot x$.}}{37}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces For a concrete example, we can depict the string $\leavevmode {\color  {green}\alpha } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {orange}\beta }$ as a morphism in a commuting diagram.}}{37}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}String-rewrite systems as 1-object-2-categories}{37}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The string-diagrammatic view, where $\star $ is treated as a wire and morphisms are treated as boxes or dots is an expression of the same data under the Poincar\'{e} dual.}}{38}{subsection.2.2.1}\protected@file@percent }
\newlabel{fig:ruleR}{{2.2.1}{38}{String-rewrite systems as 1-object-2-categories}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces We can visualise the rule as a commutative diagram where $\leavevmode {\color  {magenta}R}$ is a 2-cell between the source and target 1-cells. Just as 1-cells are arrows between 0-cell points in a commuting diagram, a 2-cell can also be conceptualised as a directed surface from a 1-cell to another. Taking the Poincar\'{e} dual of this view gives us a string diagram for the 2-cell $\leavevmode {\color  {magenta}R}$.}}{38}{subsection.2.2.1}\protected@file@percent }
\newlabel{fig:cfgsig}{{2.2.1}{39}{String-rewrite systems as 1-object-2-categories}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A context-free grammar for \texttt  {Alice sees Bob quickly run to school}. We can describe a context-free grammar with the same combinatorial rewriting d