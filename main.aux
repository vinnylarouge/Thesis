\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newpmemlabel{^_1}{1}
\newpmemlabel{^_3}{2}
\newpmemlabel{^_2}{3}
\tcolorbox@label{1}{4}
\citation{wang-mascianica_distilling_2023}
\tcolorbox@label{2}{5}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Context and synopsis}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nouwen_dynamic_2022}
\citation{nouwen_dynamic_2022}
\citation{sobocinski_graphical_2015,bonchi_interacting_2017,bonchi_graphical_2019}
\citation{haydon_compositional_2020}
\citation{lorenz_causal_2023,jacobs_causal_2019}
\citation{bonchi_categorical_2014}
\citation{boisseau_string_2022}
\citation{hedges_string_2015}
\citation{baez_open_2020}
\citation{fritz_finettis_2021}
\citation{cruttwell_categorical_2022}
\citation{coecke_interacting_2011,coecke_picturing_2017,poor_completeness_2023}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What this thesis is about}{8}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Let's say that \textbf  \emph  {{the meaning of text is how it updates a model.}} So we start with some model of the way things are, modelled as data on a wire.\relax }}{8}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Text updates that model; like a gate updates the data on a wire.\relax }}{8}{figure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Text is made of sentences; like a circuit is made of gates and wires.\relax }}{8}{figure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Let's say that \textbf  {\emph  {The meaning of a sentence is how it updates the meanings of its parts.}} As a first approximation, let's say that the \emph  {parts} of a sentence are the nouns it contains or refers to. Noun data is carried by wires. Collections of nouns are related by gates, which play the roles of verbs and adjectives.\relax }}{8}{figure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Gates can be related by higher order gates, which play the roles of adverbs, adpositions, and conjunctions; anything that modifies the data of first order gates like verbs.\relax }}{8}{figure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces In practice, higher order gates may be implemented as gates that modify parameters of other gates. Grammar, and \emph  {function words} -- words that operate on meanings -- are in principle absorbed by the geometry of the diagram. These diagrams are natural vehicles for \emph  {dynamic semantics} \citep  {nouwen_dynamic_2022}, broadly construed, where states are prior contexts and sentences-as-processes update prior contexts.\relax }}{8}{figure.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Nouns are represented by wires, each `distinct' noun having its own wire.\relax }}{8}{figure.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces We represent adjectives, intransitive verbs, and transitive verbs by gates acting on noun-wires. Since a transitive verb has both a subject and an object noun, that will then be two noun-wires, while adjectives and intransitive verbs only have one.\relax }}{8}{figure.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Adverbs, which modify verbs, we represent as boxes with holes in them, with a number of dangling wires in the hole indicating the shape of gate expected, and these should match the input- and output-wires of the box with the whole.\relax }}{8}{figure.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Similarly, adpositions also modify verbs, by moreover adding another noun-wire to the right.\relax }}{8}{figure.1.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces For verbs that take sentential complements and conjunctions, we have families of boxes to accommodate input circuits of all sizes. They add another noun-wire to the left of a circuit.\relax }}{8}{figure.1.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Conjunctions are boxes that take two circuits which might share labels on some wires.\relax }}{8}{figure.1.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Of course filled up boxes are just gates.\relax }}{8}{figure.1.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Gates compose sequentially by matching labels on some of their noun-wires and in parallel when they share no noun-wires, to give \underline  {text circuits}.\relax }}{8}{figure.1.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces To summarise: composition by nesting corresponds to grammatical structure within sentences. Sentences correspond to filled gates, boxes with fixed arity correspond to first-order modifiers such as adverbs and adpositions, and boxes with variable arity correspond to sentential-level modifiers such as conjunctions and verbs with sentential complements.\relax }}{8}{figure.1.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Composition by connecting wires corresponds to identifying coreferences in discourse. We obtain the same circuit for multiple text presentations of the same content, e.g. \texttt  {Sober Alice who sees drunk Bob clumsily dance laughs at him.} yields the same circuit as the text \texttt  {Alice is sober. She sees Bob clumsily dance. Bob is drunk. She laughs at him.}\relax }}{8}{figure.1.16}\protected@file@percent }
\citation{joyal_geometry_1991,joyal_geometry_nodate,maclane_natural_1963,lane_categories_2010,selinger_survey_2010}
\citation{vaswani_attention_2017}
\citation{openai_chatgpt_2022}
\citation{bastian_google_2022}
\citation{teddy_teddynpc_i_2022}
\citation{thompson_gpt-35_2022}
\citation{mcshane_linguistics_2021}
\citation{church_pendulum_2011}
\citation{hendrycks_measuring_2021}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}\textbf  {Question:} What is the practical value of studying language when Large Language Models exist?}{10}{section.1.2}\protected@file@percent }
\citation{floridi_fourth_2014}
\citation{sutton_bitter_2019}
\citation{chomsky_new_2000}
\citation{mollica_humans_2019}
\citation{herculano-houzel_remarkable_2012}
\citation{chowdhery_palm_2022,narang_pathways_2022}
\citation{khan_what_2023}
\citation{tom_goldstein_tomgoldsteincs_training_2022}
\citation{taori_rohan_stanford_2023}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}\textbf  {First Reply:} Interpretability, maybe.}{11}{section.1.3}\protected@file@percent }
\citation{fodor_connectionism_1988}
\citation{frege_gottlob_selbst_1884}
\citation{coecke_compositionality_2021}
\citation{lecun_deep_2015}
\citation{rumelhart_learning_1987}
\citation{hochreiter_long_1997}
\citation{vaswani_attention_2017}
\citation{bronstein_geometric_2021}
\citation{chapman_david_nebulosity_2010}
\citation{wolfram_new_2002}
\citation{marr_artificial_1977}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}\textbf  {Objec