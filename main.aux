\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newpmemlabel{^_1}{1}
\newpmemlabel{^_3}{2}
\newpmemlabel{^_2}{3}
\tcolorbox@label{1}{6}
\citation{wang-mascianicaDistillingTextCircuits2023a}
\tcolorbox@label{2}{7}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Context and synopsis}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nouwenDynamicSemantics2022}
\citation{nouwenDynamicSemantics2022}
\citation{sobocinskiGraphicalLinearAlgebra2015,bonchiInteractingHopfAlgebras2017,bonchiGraphicalAffineAlgebra2019a}
\citation{haydonCompositionalDiagrammaticFirstOrder2020d,bonchiDiagrammaticAlgebraFirst2024a}
\citation{lorenzCausalModelsString2023,jacobsCausalInferenceString2019b}
\citation{bonchiCategoricalSemanticsSignal2014}
\citation{boisseauStringDiagrammaticElectrical2022}
\citation{hedgesStringDiagramsGame2015}
\citation{baezOpenPetriNets2020}
\citation{fritzFinettiTheoremCategorical2021}
\citation{cruttwellCategoricalFoundationsGradientbased2022,khatriAnatomyAttention2024,rodatzPatternLanguageMachine2024}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Let's say that \textbf  \emph  {{the meaning of text is how it updates a model.}} So we start with some model of the way things are, modelled as data on a wire.\relax }}{10}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Text updates that model; like a gate updates the data on a wire.\relax }}{10}{figure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Text is made of sentences; like a circuit is made of gates and wires.\relax }}{10}{figure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Let's say that \textbf  {\emph  {The meaning of a sentence is how it updates the meanings of its parts.}} As a first approximation, let's say that the \emph  {parts} of a sentence are the nouns it contains or refers to. Noun data is carried by wires. Collections of nouns are related by gates, which play the roles of verbs and adjectives.\relax }}{10}{figure.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What this thesis is about}{10}{section.1.1}\protected@file@percent }
\citation{joyalGeometryTensorCalculus1991c,joyalGEOMETRYTENSORCALCULUSa,maclaneNaturalAssociativityCommutativity1963,laneCategoriesWorkingMathematician2010,selingerSurveyGraphicalLanguages2010d}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Gates can be related by higher order gates, which play the roles of adverbs, adpositions, and conjunctions; anything that modifies the data of first order gates like verbs.\relax }}{11}{figure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces In practice, higher order gates may be implemented as gates that modify parameters of other gates. Grammar, and \emph  {function words} -- words that operate on meanings -- are in principle absorbed by the geometry of the diagram. These diagrams are natural vehicles for \emph  {dynamic semantics} \citep  {nouwenDynamicSemantics2022}, broadly construed, where states are prior contexts and sentences-as-processes update prior contexts.\relax }}{11}{figure.1.6}\protected@file@percent }
\citation{vaswaniAttentionAllYou2017}
\citation{openaiChatGPTOptimizingLanguage2022}
\citation{bastianGooglePaLMGiant2022}
\citation{teddy[@teddynpc]MadeChatGPTTake2022}
\citation{thompsonGPT3IQTesting2022}
\citation{mcshaneLinguisticsAgeAI2021}
\citation{churchPendulumSwungToo2011}
\citation{hendrycksMeasuringMathematicalProblem2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Nouns are represented by wires, each `distinct' noun having its own wire.\relax }}{12}{figure.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces We represent adjectives, intransitive verbs, and transitive verbs by gates acting on noun-wires. Since a transitive verb has both a subject and an object noun, that will then be two noun-wires, while adjectives and intransitive verbs only have one.\relax }}{12}{figure.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Adverbs, which modify verbs, we represent as boxes with holes in them, with a number of dangling wires in the hole indicating the shape of gate expected, and these should match the input- and output-wires of the box with the whole.\relax }}{12}{figure.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}\textbf  {Question:} What is the practical value of studying language when Large Language Models exist?}{12}{section.1.2}\protected@file@percent }
\citation{floridiFourthRevolutionHow2014}
\citation{suttonBitterLesson2019}
\citation{chomskyNewHorizonsStudy2000a}
\citation{mollicaHumansStoreMegabytes2019}
\citation{herculano-houzelRemarkableNotExtraordinary2012}
\citation{chowdheryPaLMScalingLanguage2022,narangPathwaysLanguageModel2022}
\citation{khanWhatAreTokens2023}
\citation{tomgoldstein[@tomgoldsteincs]TrainingPaLMTakes2022}
\citation{taorirohanStanfordCRFM2023}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Similarly, adpositions also modify verbs, by moreover adding another noun-wire to the right.\relax }}{13}{figure.1.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces For verbs that take sentential complements and conjunctions, we have families of boxes to accommodate input circuits of all sizes. They add another noun-wire to the left of a circuit.\relax }}{13}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}\textbf  {First Reply:} Interpretability, maybe.}{13}{section.1.3}\protected@file@percent }
\citation{fodorConnectionismCognitiveArchitecture1988}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Conjunctions are boxes that take two circuits which might share labels on some wires.\relax }}{14}{figure.1.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Of course filled up boxes are just gates.\relax }}{14}{figure.1.13}\protected@file@percent }
\citation{fregegottlobSelbstConcreteDinge1884}
\citation{coeckeCompositionalityWeSee2021}
\citation{lecunDeepLearning2015a}
\citation{rumelhartLearningInternalRepresentations1987}
\citation{hochreiterLongShortTermMemory1997}
\citation{vaswaniAttentionAllYou2017}
\citation{bronsteinGeometricDeepLearning2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Gates compose sequentially by matching labels on some of their noun-wires and in parallel when they share no noun-wires, to give \underline  {text circuits}.\relax }}{15}{figure.1.14}\protected@file@percent }
\citation{chapmandavidNebulosityMeaningness2010}
\citation{wolframNewKindScience2002}
\citation{marrArtificialIntelligencePersonal1977a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces To summarise: composition by nesting corresponds to grammatical structure within sentences. Sentences correspond to filled gates, boxes with fixed arity correspond to first-order modifiers such as adverbs and adpositions, and boxes with variable arity correspond to sentential-level modifiers such as conjunctions and verbs with sentential complements.\relax }}{16}{figure.1.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}\textbf  {Objection:} You're forgetting the bitter lesson.}{16}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}\textbf  {Objection:} GOFAI? GO-F-yourself!}{16}{subsection.1.3.2}\protected@file@percent }
\citation{sogaardGroundingVectorSpace2023}
\citation{benderClimbingNLUMeaning2020}
\citation{searleMindsBrainsPrograms1980a}
\citation{lietardLanguageModelsKnow2021}
\citation{kriegeskorteGridCellsConceptual2016}
\citation{gardenforsGeometryMeaningSemantics2014}
\citation{davidadOpenAgencyArchitecture}
\citation{weiChainofThoughtPromptingElicits2023}
\citation{koralusHumansHumansOut2023}
\citation{kanervaComputingHighDimensionalVectors2019}
\citation{liuSeeingBelievingBrainInspired2023}
\citation{goodfellowGenerativeAdversarialNetworks2014}
\citation{chenXGBoostScalableTree2016a}
\citation{coeckeMathematicalFoundationsCompositional2010a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Composition by connecting wires corresponds to identifying coreferences in discourse. We obtain the same circuit for multiple text presentations of the same content, e.g. \texttt  {Sober Alice who sees drunk Bob clumsily dance laughs at him.} yields the same circuit as the text \texttt  {Alice is sober. She sees Bob clumsily dance. Bob is drunk. She laughs at him.}\relax }}{17}{figure.1.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}\textbf  {Objection:} How does any of this improve capabilities?}{17}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}\textbf  {Second Reply:} LLMs don't help us understand language; how might string diagrams help?}{18}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}\textbf  {Objection:} Isn't the better theory the one with better predictions?}{18}{subsection.1.4.1}\protected@file@percent }
\citation{dziriFaithFateLimits2023}
\citation{RileyGoodsideGoodside2022}
\citation{marrVisionComputationalInvestigation2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Why Category Theory?}{20}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}\textbf  {Objection:} Aren't string diagrams just graphs?}{21}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Synopsis of the thesis}{22}{section.1.5}\protected@file@percent }
\citation{maclaneNaturalAssociativityCommutativity1963,benabouAlgebreElementaireDans1964}
\citation{wang-mascianicaTalkingSpaceInference2021a}
\citation{jeanChildsConceptionSpace1967}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Process Theories}{24}{section.1.6}\protected@file@percent }
\newlabel{sec:proctheory}{{1.6}{24}{Process Theories}{section.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}What does it mean to copy and delete?}{27}{subsection.1.6.1}\protected@file@percent }
\newlabel{relcopy}{{1.6.13}{27}{Sets and relations}{theorem.1.6.13}{}}
\citation{coeckePicturingQuantumProcesses2017a}
\newlabel{ex:copyablestate}{{1.6.14}{28}{Not all states are copyable}{theorem.1.6.14}{}}
\newlabel{ft:determinism}{{1.6.15}{28}{}{theorem.1.6.15}{}}
\citation{czarneckiBidirectionalTransformationsCrossDiscipline2009}
\citation{gibbonsRelatingAlgebraicCoalgebraic2012}
\citation{wilsonSafariUpdateStructures2021a,heffordCategoriesSemanticConcepts2020b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}What is an update?}{29}{subsection.1.6.2}\protected@file@percent }
\newlabel{ss:update}{{1.6.2}{29}{What is an update?}{subsection.1.6.2}{}}
\citation{selingerSurveyGraphicalLanguages2010d,joyalGeometryTensorCalculus1991c,joyalGEOMETRYTENSORCALCULUSa}
\citation{haroldabelsonLecture1AOverview2019}
\citation{paquetteCategoricalQuantumComputation2008}
\citation{nlabauthorsPROPNLab}
\citation{fongHypergraphCategories2018}
\tcolorbox@label{3}{33}
\tcolorbox@label{4}{34}
\citation{moortgatTypelogicalGrammar2014b}
\citation{lambekMathematicsSentenceStructure1958}
\citation{lambekCalculusSyntacticTypes1961}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Previously, on DisCoCat}{35}{section.1.7}\protected@file@percent }
\newlabel{sec:previously}{{1.7}{35}{Previously, on DisCoCat}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Lambek's Linguistics}{35}{subsection.1.7.1}\protected@file@percent }
\newpmemlabel{^_4}{35}
\newpmemlabel{^_5}{35}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces In English, we may consider a noun to have type $n$, and a transitive verb $(n/s)\setminus n$, to yield a well-formedness proof of \texttt  {Bob drinks beer}. The type formation rules for such a grammar are intuitive. Apart from a stock of basic types $\PazoBB  {B}$ that contains special final types to indicate sentences, we have two type formation operators $(- \ / \ =)$ and $(- \ \setminus \ =)$, which along with their elimination rules establish a requirement that grammatical categories require other grammatical categories to their left or right. This is the essence of Lambek's calculi \textbf  {NL} and \textbf  {L}. CCGs keep the same minimal type-formations, but include extra sequent rules such as type-raising and cross-composition.}}{35}{figure.caption.7}\protected@file@percent }
\newpmemlabel{^_6}{35}
\newpmemlabel{^_8}{35}
\newpmemlabel{^_7}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces We can notice an asymmetry in the above formulation when we examine the transitive verb type $(n/s)\setminus n$ again; it asks first for a noun to the right, and then a noun to the left. We could just as well have asked for the nouns in the other order with the typing $(n/s)\setminus n$ and obtained all of the same proofs.}}{36}{figure.caption.8}\protected@file@percent }
\newpmemlabel{^_9}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces To eliminate this asymmetry, Lambek devised pregroup grammars. Whereas a group is a monoid with inverses up to left- and right-multiplication, a pregroup weakens the requirement for inverses so that all elements have distinct left- and right- inverses, denoted $x^{-1}$ and $^{-1}x$ respectively. Eliminating or introducing inverses is a non-identity relation on elements of the pregroup, so we have axioms of the form e.g. $x \cdot ^{-1}x \ \rightarrow \ 1 \ \rightarrow \ ^{-1}x \cdot x$. In this formulation, denoting the multiplication with a dot, both $(n/s)\setminus n$ and $(n/s)\setminus n$ become $^{-1}n \cdot s \cdot n^{-1}$, which just wants a noun to the left and a noun to the right in whatever order to eliminate the flanking inverses to reveal the embedded sentence type. Now we can obtain the same proof of correctness as a series of algebraic reductions.}}{37}{figure.caption.9}\protected@file@percent }
\citation{coeckeLogicEntanglement2004}
\citation{coeckeLogicEntanglement2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Coecke's Composition}{38}{subsection.1.7.2}\protected@file@percent }
\newpmemlabel{^_10}{38}
\newpmemlabel{^_11}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Meanwhile, an underground grunge vagabond moonlighting as a quantum physicist moonlighting as a computer scientist was causing a shortage of cigars and whiskey in a small English town. He noticed a funny thing about the composition of multiple non-destructive measurements of a quantum system, which was that information could be carried, or flow, between them. So he wrote a paper \citep  {coeckeLogicEntanglement2004}, which contained informal diagrams that looked like this.}}{38}{figure.caption.10}\protected@file@percent }
\newpmemlabel{^_12}{38}
\newpmemlabel{^_13}{39}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces There were two impressive things about these diagrams. First, the effects such as transparencies for text boxes and curved serifs for angled arrows give a modern feel, but they were done manually in MacDraw, the diagrammatic equivalent of sticks and stones. Second, though the diagrams were informal, they provided a way to visualise and reason about entanglement that was impossible by staring at the equivalent matrix formulation of the same composite operator. The most important diagram for our story was this one, which captures the information flow of quantum teleportation.}}{39}{figure.caption.11}\protected@file@percent }
\citation{abramskyAbstractScalarsLoops2009a}
\citation{abramskyAbstractScalarsLoops2009a}
\citation{choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}
\citation{choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}
\citation{coeckeClassicalQuantumStructuralism2009}
\citation{coeckeNewDescriptionOrthogonal2013c}
\citation{coeckeClassicalQuantumStructuralism2009}
\citation{coeckeNewDescriptionOrthogonal2013c}
\citation{coeckeInteractingQuantumObservables2011}
\citation{coeckeThreeQubitEntanglement2011,dewittZXcalculusIncompleteQuantum2014}
\citation{hadzihasanovicAlgebraEntanglementGeometry2017a,ngCompletenessZWZX2018,poorCompletenessArbitraryFinite2023}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\citation{coeckeInteractingQuantumObservables2011}
\citation{coeckeThreeQubitEntanglement2011,dewittZXcalculusIncompleteQuantum2014}
\citation{hadzihasanovicAlgebraEntanglementGeometry2017a,ngCompletenessZWZX2018,poorCompletenessArbitraryFinite2023}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Categorical quantum mechanics}{40}{subsection.1.7.3}\protected@file@percent }
\newpmemlabel{^_14}{40}
\newpmemlabel{^_15}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Category theorists and physicists such as Abramsky and Baez were excited about these diagrams, which looked like string diagrams waiting to be made formal. The graphical cups and caps in the important diagram were determined to correspond to a special form of symmetric monoidal closed category called strong compact closed \citep  {abramskyAbstractScalarsLoops2009a}.}}{40}{figure.caption.12}\protected@file@percent }
\newpmemlabel{^_16}{40}
\newpmemlabel{^_17}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces Diagrammatically, reasoning in a strongly compact closed category amounts to ignoring the usual requirement of processiveness and forgetting the distinction between inputs and outputs, so that "future" outputs could curl back and be "past" inputs. This formulation also gave insight into the structure of quantum mechanics. For example, the process-state duality of strong compact closure manifested as the Choi–Jamiołkowski isomorphism \citep  {choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}.}}{40}{figure.caption.13}\protected@file@percent }
\newpmemlabel{^_18}{40}
\newpmemlabel{^_20}{40}
\newpmemlabel{^_19}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces However, dealing with superpositions necessitated using summation operators within diagrams, which is cumbersome to write especially when dealing with even theoretically simple Bell states. An elegant diagrammatic simplification arose with the observation that special-$\dagger $-frobenius algebras \citep  {coeckeClassicalQuantumStructuralism2009}, or spiders, correspond to choices of orthonormal bases \citep  {coeckeNewDescriptionOrthogonal2013c} in \textbf  {FdHilb}, the ambient setting of finite-dimensional hilbert spaces. Not only did this remove the need for summation operators, it also revealed that strong compact closure was a derived, rather than fundamental structure, since spiders induce compact closed structure.}}{41}{figure.caption.14}\protected@file@percent }
\newpmemlabel{^_21}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces And so the stage was set for a purely diagrammatic treatment of ZX quantum mechanics via interacting spiders \citep  {coeckeInteractingQuantumObservables2011}. The story of ZX diverges away from our interest, so I will summarise what happened afterwards. In no particular order, the development of ZX went on to accommodate a third axis of measurement \citep  {coeckeThreeQubitEntanglement2011,dewittZXcalculusIncompleteQuantum2014} to yield a ZXW calculus then proven to be complete for the usual hilbert space formalism for quantum \citep  {hadzihasanovicAlgebraEntanglementGeometry2017a,ngCompletenessZWZX2018,poorCompletenessArbitraryFinite2023}. There are at the time of writing two expository books \citep  {coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}, and ZX-variants are becoming an industry standard for quantum circuit specification and rewriting.}}{41}{figure.caption.15}\protected@file@percent }
\citation{firthStudiesLinguisticAnalysis1957}
\citation{firthStudiesLinguisticAnalysis1957}
\citation{coeckeMathematicalFoundationsCompositional2010a}
\citation{coeckeMathematicalFoundationsCompositional2010a}
\citation{sadrzadehFrobeniusAnatomyWord2013a,sadrzadehFrobeniusAnatomyWord2016a}
\citation{meyerModellingLexicalAmbiguity2020}
\citation{sadrzadehFrobeniusAnatomyWord2013a,sadrzadehFrobeniusAnatomyWord2016a}
\citation{meyerModellingLexicalAmbiguity2020}
\citation{boltInteractingConceptualSpaces2017b}
\citation{gardenforsGeometryMeaningSemantics2014}
\citation{marsdenCustomHypergraphCategories2017}
\citation{boltInteractingConceptualSpaces2017b}
\citation{gardenforsGeometryMeaningSemantics2014}
\citation{marsdenCustomHypergraphCategories2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.4}Enter computational linguistics}{42}{subsection.1.7.4}\protected@file@percent }
\newpmemlabel{^_22}{42}
\newpmemlabel{^_23}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Somewhere in Canada at the turn of the millenium, Bob met Jim, who saw something familiar about the diagram for quantum teleportation. The snake equation for compact closure looked a lot like the categorified version of introducing and eliminating pregroup types. }}{42}{figure.caption.16}\protected@file@percent }
\newpmemlabel{^_24}{42}
\newpmemlabel{^_25}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces Bob and Jim's meeting put the adjectives \emph  {compositional} and \emph  {categorical} on the same table, but the cake wasn't ready. Two more actors Steve and Mehrnoosh were required to introduce \emph  {distributional}, which refers to Firth's maxim "you shall know a word by the company it keeps" \citep  {firthStudiesLinguisticAnalysis1957}. In its modern incarnation, this refers generally to vector-based semantics for words, where it is desirable but not necessarily so (as in the case of generic latent space embeddings by an autoencoder) that proximity of vectors models semantic closeness.}}{42}{figure.caption.17}\protected@file@percent }
\newpmemlabel{^_26}{42}
\newpmemlabel{^_28}{42}
\newpmemlabel{^_30}{42}
\newpmemlabel{^_27}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.28}{\ignorespaces Steve Clark was a professor in the computer science department at Oxford, and he was wondering how to compose vector-based semantic representations. Steve asked Bob, who realised suddenly what Jim was talking about. Mediated by the linguistic expertise of Mehrnoosh who was a postdoctoral researcher in Oxford at the time, pregroup diagrams were born. The basic types $n$ and $s$ are assigned finite-dimensional vector spaces, concatenation of types the Kronecker product $\otimes $, and by the isomorphism of dual spaces in finite dimensions there is no need to keep track of the left- and right- inverse data. Words become vectors, and pregroup reductions become bell-states, or bell-measurements, depending on whether one reads top-down or bottom-up. There was simply no other game in town for an approach to computational linguistics that combined linguistic compositionality with distributional representations \citep  {coeckeMathematicalFoundationsCompositional2010a}.}}{43}{figure.caption.18}\protected@file@percent }
\newpmemlabel{^_29}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.29}{\ignorespaces In \citep  {sadrzadehFrobeniusAnatomyWord2013a,sadrzadehFrobeniusAnatomyWord2016a}, the trio realised that spiders could play the role of relative pronouns, which was genuinely novel linguistics. If one follows the noun-wire of "movies", one sees that by declaring the relative pronoun to be a vector made up of a particular bunch of spiders-as-multiwires, "movies" is copied to be related to the "liked" word, copied again by "which" to be related to the "is-famous" word, and a third time to act as the noun in the whole noun-phrase. This discovery clarified a value proposition: insights from quantum theory could be applied in the linguistic setting: for example, density matrices were used to model semantic ambiguity \citep  {meyerModellingLexicalAmbiguity2020}.}}{44}{figure.caption.19}\protected@file@percent }
\newpmemlabel{^_31}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {1.30}{\ignorespaces Keeping the structure of the diagrams but seeking set-relational rather than vector-based semantics, a bridge was made between linguistics and cognitive science in \emph  {Interacting Conceptual Spaces I} \citep  {boltInteractingConceptualSpaces2017b}. Briefly, G\"{a}rdenfors posits \citep  {gardenforsGeometryMeaningSemantics2014} that spatial representations of concepts mediate raw sense data and symbolic representations -- e.g. red is a region in colourspace -- and moreover that concepts ought to be spatially convex -- e.g. mixing any two shades of red still gives red. A new point in the value proposition arose: that new mathematics would arise from investigating the linguistic-quantum bridge, e.g. generalised relations \citep  {marsdenCustomHypergraphCategories2017}. Although labelled as if it is the first in a series, the paper never saw a sequel by the same title, blocked by an apparently simple but actually tricky theoretical problem: while this convex-relational story worked for conceptual adjectives modifying a single noun (such as for "sweet yellow bananas"), there was difficulty in extending the story to work for multiple objects interacting in the same space, as in "cup on table in room". It couldn't be worked out what structure a sentence-wire in \textbf  {ConvexRel} ought to have in order to accommodate (in principle) arbitrarily many objects and spatial relations between them.}}{45}{figure.caption.20}\protected@file@percent }
\citation{lorenzQNLPPracticeRunning2023}
\citation{kartsaklisLambeq2023}
\citation{aquantumcomputerLudovicoQuanthoven}
\citation{coeckeMathematicsTextStructure2020a}
\citation{coeckeMathematicsTextStructure2020a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.5}I killed DisCoCat, and I would do it again.}{46}{subsection.1.7.5}\protected@file@percent }
\newpmemlabel{^_32}{46}
\newpmemlabel{^_33}{46}
\@writefile{lof}{\contentsline {figure}{\numberline {1.31}{\ignorespaces On the practical side, wide tensors were (and remain) prohibitively expensive to simulate classically and actual quantum computers did not (and still do not) have many qubits, hence in practice pregroup diagrams were reduced to thinner and deeper circuits, often with the help of an additional simplifying assumption that sentence wires were pairs of noun wires in the illustrated form on the left. Theoretically, seeking dynamic epistemic logic, Bob had an epiphanous hangover (really) where he envisioned that these "Cartesian verbs" could be used in service of compositional text meanings, and he called this idea DisCoCirc \citep  {coeckeMathematicsTextStructure2020a}.}}{46}{figure.caption.21}\protected@file@percent }
\citation{carboniCartesianBicategories1987b,carboniCartesianBicategoriesII2007}
\newpmemlabel{^_34}{47}
\newpmemlabel{^_35}{47}
\@writefile{lof}{\contentsline {figure}{\numberline {1.32}{\ignorespaces I met Bob in my master's in 2019, where he taught the picturing quantum processes course. When quantum teleportation was explained in half a minute by a diagram, I decided to pursue a DPhil in diagrammatic mathematics. In the last lecture, I threw Bob a cider, after which he seemed to like me.}}{47}{figure.caption.22}\protected@file@percent }
\citation{wangGraphicalGrammarGraphical2019}
\citation{delpeuchAutonomizationMonoidalCategories2020a}
\citation{wang-mascianicaTalkingSpaceInference2021a}
\citation{wang-mascianicaTalkingSpaceInference2021a}
\citation{liuLanguageCircuits2021a}
\citation{liuLanguageCircuits2021a}
\newpmemlabel{^_36}{48}
\newpmemlabel{^_37}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.33}{\ignorespaces Then COVID happened. During the first lockdown, I visited Bob's garden under technically legal circumstances, and I suggested a solution to the longstanding problem of representing linguistic spatial relationships. My theoretical concern was the culprit: the initial attempts at the problem failed because the approach was to find a single sentence object $s$ in which one could paste the data of arbitrarily many distinct spatial entities. The simple solution was a change in perspective.}}{48}{figure.caption.23}\protected@file@percent }
\newpmemlabel{^_38}{48}
\newpmemlabel{^_40}{48}
\newpmemlabel{^_42}{48}
\newpmemlabel{^_44}{48}
\newpmemlabel{^_46}{48}
\newpmemlabel{^_48}{48}
\newpmemlabel{^_50}{48}
\newpmemlabel{^_52}{48}
\newpmemlabel{^_54}{48}
\newpmemlabel{^_56}{48}
\newpmemlabel{^_58}{48}
\newpmemlabel{^_39}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.34}{\ignorespaces That this move of splitting up the sentence-wire into a sentence-dependent collection of wires was sufficient to solve what had appeared to be a difficult problem prompted some re-examination of foundations. The free autonomisation trick in conjunction with sentence-wire-as-tensored-nouns seemed promising, but it became clear that right way to drown a DisCoCat thoroughly was to explain and eliminate the spiders.}}{49}{figure.caption.24}\protected@file@percent }
\newpmemlabel{^_41}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.35}{\ignorespaces I then discovered that by interpreting spiders as the well-known "pair of pants" algebra in a compact closed monoidal setting allowed for a procedure in which the final form was purely symmetric monoidal -- the absence of cups and caps meant that there was no practical necessity to interpret diagrams on quantum computers: \emph  {any} computer would suffice. The role of spiders for relative pronouns was illuminated in the presence of splitting the sentence wire: the pair-of-pants are the algebra of morphism composition, and splitting the sentence wire into a collection of nouns allowed relative-pronoun-spiders to pick out the participating nouns to compose relationships onto.}}{49}{figure.caption.25}\protected@file@percent }
\newpmemlabel{^_43}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.36}{\ignorespaces A coherent conservative generalisation of DisCoCat with less baggage had emerged, or rather, DisCoCirc was placed to formally subsume DisCoCat. It was now understood that the sentence type was a formal syntactic ansatz for the sake of grammar, which was to be interpreted in the semantic domain not as a single wire, but as a sentence-dependent collection of wires. It was further realised that the complexity of pregroup diagrams was due to grammar -- the topological deformation of semantic connections to fit the one-dimensional line of language -- whereas the essential connective content of language could be expressed in a simple form that distilled away the bureaucracy of syntax.}}{50}{figure.caption.26}\protected@file@percent }
\newpmemlabel{^_45}{51}
\@writefile{lof}{\contentsline {figure}{\numberline {1.37}{\ignorespaces We wrote up the story about spaces in \citep  {wang-mascianicaTalkingSpaceInference2021a}, the spiritual successor to \emph  {interacting conceptual spaces I}. We could formally calculate the meanings of sentences that used linguistic spatial relations, all using a simple and tactile diagrammatic calculus.}}{51}{figure.caption.27}\protected@file@percent }
\newpmemlabel{^_47}{51}
\newpmemlabel{^_49}{52}
\@writefile{lof}{\contentsline {figure}{\numberline {1.38}{\ignorespaces The paper on spatial relations actually came very late, because I was busy with Bob's ludicrous request to go turn "all of language" into circuits. I bitched and moaned about how I wasn't a linguist and how it was an impossible task, but I was in too deep to back out.}}{52}{figure.caption.29}\protected@file@percent }
\newpmemlabel{^_51}{53}
\@writefile{lof}{\contentsline {figure}{\numberline {1.39}{\ignorespaces I suppose the nice thing about aiming for the moon is that even failure might mean you leave orbit. So I settled for what I thought was a sensible fragment of English, for which I devised internal wirings and an algorithm that transformed pregroup diagrams with the internal wirings into circuit form. Many tiring diagrams later, I presented my results in the first draft of "distilling text into circuits". It was exhausting.}}{53}{figure.caption.30}\protected@file@percent }
\newpmemlabel{^_53}{53}
\newpmemlabel{^_55}{54}
\newpmemlabel{^_57}{54}
\@writefile{lof}{\contentsline {figure}{\numberline {1.40}{\ignorespaces Bob had a good point. Everything worked, but we had no understanding as to why, and accordingly, whether or not it would all break. At this point in time, Jonathon Liu, a masters' student I tutored during COVID, had committed the grave error of thinking diagrams were cool, and was now hanging out with me and Bob. After understanding the procedure, Jono independently devised the same arcane internal wirings as I had, but neither of us could explain how we did it. So we had evidence of an underlying governing structure that was coherent but inarticulable.}}{54}{figure.caption.33}\protected@file@percent }
\newpmemlabel{^_59}{55}
\@writefile{lof}{\contentsline {figure}{\numberline {1.41}{\ignorespaces I realised that our intuitions were coming from an implicit productive grammar, rather than a parsing one, and that the path of least resistance for obtaining formal guarantees for the language-to-circuit procedure was to just handcraft a generative grammar for the fragment of language we were interested in. This meant scrapping everything in the first draft and starting again from scratch. Bob always had a word of gentle encouragement, giving me the motivation to persevere.  \par So now we had two ways to obtain text circuits. One from pregroups (which Jono had extended the technique for to CCGs and dependency grammars in his master's thesis \citep  {liuLanguageCircuits2021a}), and one from handcrafted productive grammars. Then came time for me to write my thesis, and there were three salient questions I wanted to address.  What are internal wirings?  Are text circuits a generative grammar?  What are text circuits good for?  These questions are what the rest of this thesis seeks to answer, one chapter at a time. }}{55}{figure.caption.34}\protected@file@percent }
\citation{suitsGrasshopperGamesLife2005}
\@writefile{toc}{\contentsline {chapter}{\numberline {1\chapfrac {1}{2}}Corrections}{57}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:oneandhalf}{{1\chapfrac {1}{2}}{57}{Corrections}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1\chapfrac {1}{2}.1}Does it work?}{57}{section.1.1}\protected@file@percent }
\citation{everettDonSleepThere2009}
\citation{fitchEvolutionLanguageFaculty2005}
\citation{parteeBriefHistorySyntaxSemantics2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.1.1}Have experiments been done?}{58}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1\chapfrac {1}{2}.2}Can you situate this work with respect to the literature in formal linguistics?}{58}{section.1.2}\protected@file@percent }
\citation{parteeChapterMontagueGrammar1997}
\citation{szaboCompositionalitySupervenience2000}
\citation{putnamProblemReference1981,barrNOTETHEOREMPUTNAMa}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.1}On Pregroups to Text Circuits vs. Transformational Grammar to Formal Semantics}{59}{subsection.1.2.1}\protected@file@percent }
\citation{janssenMontagueSemantics2021a}
\citation{montagueUniversalGrammar1970a}
\citation{montagueProperTreatmentQuantification1973}
\citation{churchSetPostulatesFoundation1933}
\citation{carnapMeaningNecessityStudy1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.2}On Montague's conception of grammar}{60}{subsection.1.2.2}\protected@file@percent }
\newlabel{sec:monty}{{1\chapfrac {1}{2}.2.2}{60}{On Montague's conception of grammar}{subsection.1.2.2}{}}
\citation{portnerFormalSemanticsEssential2008}
\citation{InquisitiveSemanticsInquisitive}
\citation{nouwenDynamicSemantics2022}
\newlabel{algdata}{{1\chapfrac {1}{2}.2.1}{62}{Generating data of an Algebra}{theorem.1.2.1}{}}
\newlabel{ids}{{1\chapfrac {1}{2}.2.2}{62}{Identities}{theorem.1.2.2}{}}
\newlabel{constants}{{1\chapfrac {1}{2}.2.3}{62}{Constants}{theorem.1.2.3}{}}
\newlabel{comp}{{1\chapfrac {1}{2}.2.4}{62}{Composition}{theorem.1.2.4}{}}
\newlabel{polyop}{{1\chapfrac {1}{2}.2.5}{62}{Polynomial Operations}{theorem.1.2.5}{}}
\newlabel{homo}{{1\chapfrac {1}{2}.2.6}{62}{Homomorphism of Algebras}{theorem.1.2.6}{}}
\citation{kerkhoffShortIntroductionClones2014a}
\citation{yauColoredOperads2016}
\citation{harrisLinguisticsWars1993}
\citation{petersNoteUniversalBase1969}
\citation{croftAutonomyFunctionalistLinguistics1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.3}On Deep Structure, the Universal Base Hypothesis, and the "Lexical Objection"}{63}{subsection.1.2.3}\protected@file@percent }
\citation{coeckeGrammarEquations2021a}
\citation{lakoffInstrumentalAdverbsConcept1968}
\citation{DisplaCyDependencyVisualizer}
\citation{fillmoreFrameSemanticsText2001}
\citation{rodatzPatternLanguageMachine2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.4}On communication, and the mathematical infeasibility of the Autonomy of Syntax}{69}{subsection.1.2.4}\protected@file@percent }
\newpmemlabel{^_60}{69}
\newpmemlabel{^_61}{69}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1\chapfrac {1}{2}.1}{\ignorespaces Here for example is a context free grammar for simple sentences that involve either a single transitive or intransitive verb. I've gathered together terminals, depicted as feet, into the basic generators.}}{69}{figure.caption.35}\protected@file@percent }
\newlabel{fig:corrcfg}{{1\chapfrac {1}{2}.1}{69}{\@tufte@stored@shortcaption }{figure.caption.35}{}}
\newpmemlabel{^_62}{70}
\newpmemlabel{^_63}{70}
\@writefile{lof}{\contentsline {figure}{\numberline {1\chapfrac {1}{2}.2}{\ignorespaces And here is its corresponding finite state machine. States are depicted as nodes, and transitions are depicted as labelled directed edges. This machine operates over the alphabet $\{ \texttt  {N}, \texttt  {IV}, \texttt  {TV}$, and it only accepts those strings that are producible by its paired CFG.}}{70}{figure.caption.36}\protected@file@percent }
\newlabel{fig:corrfinstate}{{1\chapfrac {1}{2}.2}{70}{\@tufte@stored@shortcaption }{figure.caption.36}{}}
\citation{heimSemanticsGenerativeGrammar1998b}
\newpmemlabel{^_64}{71}
\newpmemlabel{^_65}{71}
\@writefile{lof}{\contentsline {figure}{\numberline {1\chapfrac {1}{2}.3}{\ignorespaces  The blue arrows branching off of the transitions indicate what rewrites to perform on a side-buffer, where $\star $ denotes whatever was previously in the buffer, and $\texttt  {\textvisiblespace }$ indicates the placement of a cursor position where future rewrites are to take place. We're overloading notation here so that terminal symbols \texttt  {N}, \texttt  {IV}, \texttt  {TV} in the buffer correspond to typed elements and functions $\texttt  {N} \in D_e$, $\texttt  {IV} \in D_{\langle e,t \rangle }$ and $\texttt  {TV} \in D_{\langle e, \langle e , t \rangle \rangle }$ respectively, with respect to some bestiary of individuals/entities $D_e$ and truth values $D_t$ and their type closures in \textbf  {Set} under exponentiation. }}{71}{figure.caption.37}\protected@file@percent }
\newlabel{fig:corrpush}{{1\chapfrac {1}{2}.3}{71}{\@tufte@stored@shortcaption }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.5}On frameworks for rewriting systems}{72}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1\chapfrac {1}{2}.2.6}On formality in cognitive semantics}{73}{subsection.1.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}On communication}{75}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:internalwirings}{{2}{75}{On communication}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}How do we communicate using language?}{76}{section.2.1}\protected@file@percent }
\newlabel{sec:miracle}{{2.1}{76}{How do we communicate using language?}{section.2.1}{}}
\citation{haydonCompositionalDiagrammaticFirstOrder2020d}
\citation{haydonCompositionalDiagrammaticFirstOrder2020d}
\newpmemlabel{^_66}{77}
\citation{heimSemanticsGenerativeGrammar1998b}
\citation{yeungCCGBasedVersionDisCoCat2021b}
\citation{heimSemanticsGenerativeGrammar1998b}
\citation{yeungCCGBasedVersionDisCoCat2021b}
\newpmemlabel{^_67}{78}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Charlie and Dennis agree on the conceptual organisation entities and relations up to the words for those entities and relations. Just as a running example that does not affect the point, let's say we can gloss a thought in first order logic as $\exists a \exists b \exists c \exists f : A(a) \wedge B(b) \wedge C(c) \wedge F(f) \wedge L(a,f) \wedge G(b,c,f)$. In diagrammatic first order logic \citep  {haydonCompositionalDiagrammaticFirstOrder2020d}, this is equivalently presented as the following diagrams (and any other diagram that agrees up to connectivity.) For example, Charlie could ask Dennis comprehension questions such as \texttt  {WHO GAVE WHAT? TO WHOM?}, and if Dennis can always correctly answer -- e.g. \texttt  {BOB GAVE FLOWERS. TO CLAIRE.} -- then both Charlie and Dennis agree on the relational structure of the communicated thought to the extent permitted by language.}}{78}{figure.caption.38}\protected@file@percent }
\newlabel{fig:GFOLex}{{2.1}{78}{\@tufte@stored@shortcaption }{figure.caption.38}{}}
\newpmemlabel{^_68}{78}
\newpmemlabel{^_70}{78}
\newpmemlabel{^_72}{78}
\newpmemlabel{^_69}{79}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The rule of the game is that Charlie and Dennis can agree on a string-diagrammatic encoding strategy before having to communicate with each other. Here is one such strategy. Charlie might generate the example sentence as depicted.}}{79}{figure.caption.39}\protected@file@percent }
\newlabel{fig:GFOLex2a}{{2.2}{79}{\@tufte@stored@shortcaption }{figure.caption.39}{}}
\newpmemlabel{^_71}{79}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Mathematically, it makes no difference if we take the Poincar\'{e} dual of the tree, so that zero-dimensional nodes become one-dimensional wires, and branchings become zero-dimensional points linking wires -- but we can just as well depict those points as boxes to label them more clearly.}}{79}{figure.caption.40}\protected@file@percent }
\newlabel{fig:GFOLex2b}{{2.3}{79}{\@tufte@stored@shortcaption }{figure.caption.40}{}}
\newpmemlabel{^_73}{80}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Now that Charlie can express their grammatical structure string-diagrammatically, they can try to deform their first-order-logic diagram -- representing what they mean to communicate -- subject to the constraint that every one of their branchings (the structure of the CFG) is something recoverable by Dennis using just pregroup reductions. To do so, Charlie introduces a formal blue wire to mimic Dennis's sentence-type, and stuffs some complexity inside the labels in the form of internal wirings: a multiwire configuration for \texttt  {that}, and a twist for \texttt  {gives}. Those internal wirings are the content of Charlie and Dennis's shared strategy. In passing, I'll remark that by the outside-in convention for functor boxes \ref  {fig:outsidein}, this diagram constitutes a monoidal functor from this particular CFG to pregroup diagrams, where nonlabel tree-nodes are partial monoidal closure evaluators. Replacing rigid autonomous closure with cartesian closure and $n,s$ with $e,t$ recovers montague semantics for CFGs (c.f. Curry-Howard-Lambek correspondence for the case of typed lambda-calculus and cartesian closed categories, and all of Heim and Kratzer \citep  {heimSemanticsGenerativeGrammar1998b}), and interpreting the closure in a compact closed setting recovers montague semantics for CCGs \citep  {yeungCCGBasedVersionDisCoCat2021b}.}}{80}{figure.caption.41}\protected@file@percent }
\newlabel{fig:GFOLex2c}{{2.4}{80}{\@tufte@stored@shortcaption }{figure.caption.41}{}}
\newpmemlabel{^_74}{81}
\newpmemlabel{^_75}{81}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces So, when Dennis receives the sentence, Dennis's pregroup derivation yields a pregroup diagram that is connectively equivalent to what Charlie stuffed inside the context-free grammar structure. So now the two have strong equivalence between their grammars in the sense that every one of Charlie's branches is resolved by one of Dennis's reductions. As is convention for pregroup diagrams, we only use types $n$ and $s$ -- the latter denoted by a blue wire here -- and we'll leave the directionality (rigid autonomous turning number) of wires implicit, so you can either trust me that everything typechecks or do it yourself.}}{81}{figure.caption.42}\protected@file@percent }
\newlabel{fig:GFOLex2d}{{2.5}{81}{\@tufte@stored@shortcaption }{figure.caption.42}{}}
\newpmemlabel{^_76}{81}
\newpmemlabel{^_77}{81}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Now to fully recover Charlie's intended FOL-diagram, Dennis refers to the internal wirings from their shared strategy, and fills those in.}}{81}{figure.caption.43}\protected@file@percent }
\newlabel{fig:GFOLex2}{{2.6}{81}{\@tufte@stored@shortcaption }{figure.caption.43}{}}
\citation{wang-mascianicaInternalWirings}
\newpmemlabel{^_78}{82}
\newpmemlabel{^_79}{82}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Charlie's diagram morphed to fit a text circuit. The dotted blue line is a formal mark to indicate a sentential boundary. Observe how new discourse elements are introduced as states, and how open wires correspond to ongoing discourse and deletions mark completed discourse. This diagram also indicates that text circuits can be given semantics in FOL.}}{82}{figure.caption.44}\protected@file@percent }
\newlabel{fig:GFOLex3a}{{2.7}{82}{\@tufte@stored@shortcaption }{figure.caption.44}{}}
\newpmemlabel{^_80}{82}
\newpmemlabel{^_81}{82}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Dennis already knows how to parse individual sentences to extract the FOL using internal wirings. Observe there is a mathematical complication that arises in determining how many noun-wires should go into the sentence wire-bundle; we need to account for this later.}}{82}{figure.caption.45}\protected@file@percent }
\newpmemlabel{^_82}{82}
\newpmemlabel{^_83}{83}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces To deal with text, Dennis can pass a growing bundle of sentence wires along horizontally.}}{83}{figure.caption.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}An issue with functorial semantics of internal wirings}{84}{subsection.2.1.1}\protected@file@percent }
\newlabel{ex:nonfunctprod}{{2.1.3}{84}{Nonfunctoriality of internal wirings for productive grammars}{theorem.2.1.3}{}}
\newlabel{ex:nonfunctparse}{{2.1.4}{85}{Nonfunctoriality of internal wirings for parsing grammars}{theorem.2.1.4}{}}
\citation{melliesFunctorialBoxesString2006b}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Discrete Monoidal Opfibrations}{86}{section.2.2}\protected@file@percent }
\newpmemlabel{^_84}{86}
\newpmemlabel{^_85}{86}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces There are two conventions for depicting the action of a monoidal functor on parts of a string diagram. The first follows source-to-target \emph  {outside-in}. This convention is used for other work in internal wirings, since it is well-suited for describing functors that send atomic generators in their domain to more complex diagrams in their codomain.}}{86}{figure.caption.47}\protected@file@percent }
\newlabel{fig:outsidein}{{2.10}{86}{\@tufte@stored@shortcaption }{figure.caption.47}{}}
\newpmemlabel{^_86}{86}
\newpmemlabel{^_87}{86}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The other convention is \emph  {inside-out}. For the following section, we will define the coherence conditions of discrete monoidal opfibrations using this convention.}}{86}{figure.caption.48}\protected@file@percent }
\newlabel{fig:insideout}{{2.11}{86}{\@tufte@stored@shortcaption }{figure.caption.48}{}}
\newpmemlabel{^_88}{86}
\newpmemlabel{^_90}{86}
\newpmemlabel{^_92}{86}
\newpmemlabel{^_94}{86}
\newpmemlabel{^_96}{86}
\newpmemlabel{^_98}{86}
\newpmemlabel{^_89}{87}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Suppose we have a functor between monoidal categories $\mathbf  {F}: \mathcal  {C} \rightarrow \mathcal  {D}$. Then we have this diagrammatic representation of a morphism $\mathbf  {F}A \begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {$\rightarrow \mathsurround \z@ $}\kern -\wd \tw@ ${}\rightarrow {}\mathsurround \z@ $}\edef {\mathop {\kern \z@ \rightarrow }\limits ^{\mathbf  {F}f}} \mathbf  {F}B$ in $\mathcal  {D}$.}}{87}{figure.caption.49}\protected@file@percent }
\newpmemlabel{^_91}{87}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The use of a functor box is like a window from the target category $\mathcal  {D}$ into the source category $\mathcal  {C}$; when we know that a morphism in $\mathcal  {D}$ is the image under $\mathbf  {F}$ of some morphism in $\mathcal  {C}$, the functor box notation is just a way of presenting all of that data at once. Since $\mathbf  {F}$ is a functor, we must have that $\mathbf  {F}f ; \mathbf  {F}g = \mathbf  {F}(f;g)$. Diagrammatically this equation is represented by freely splitting and merging functor boxes vertically. \textbf  {N.B.} sequential merging of two boxes requires that the two wires to-be-connected within the boxes -- in this case labelled $B$ -- need to be the same; a case where merging is disallowed is when $Ff;Fg$ typechecks in the outside/target category, but $f;g$ does not in the inside/source category because the functor identifies nonequal wires.}}{87}{figure.caption.50}\protected@file@percent }
\newpmemlabel{^_93}{88}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Assume that $\mathbf  {F}$ is strict monoidal; without loss of generality by the strictification theorem, this lets us gloss over the associators and unitors and treat them as equalities. For $\mathbf  {F}$ to be strict monoidal, it has to preserve monoidal units and tensor products on the nose: i.e. $\mathbf  {F}I_\mathcal  {C} = I_\mathcal  {D}$ and $\mathbf  {F}A \otimes _\mathcal  {D} \mathbf  {F}B = \mathbf  {F}(A \otimes _\mathcal  {C} B)$. Diagrammatically these structural constraints amount to these equations.}}{88}{figure.caption.51}\protected@file@percent }
\newpmemlabel{^_95}{88}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces What remains is the monoidality of $\mathbf  {F}$, which is the requirement $\mathbf  {F}f \otimes \mathbf  {F}g = \mathbf  {F}(f \otimes g)$. Diagrammatically, this equation is represented by freely splitting and merging functor boxes horizontally; analogously to how splitting vertically is the functor-boxes' way of respecting sequential composition, splitting horizontally is how they respect parallel composition.}}{88}{figure.caption.52}\protected@file@percent }
\newpmemlabel{^_97}{88}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces And for when we want $\mathbf  {F}$ to be a (strict) symmetric monoidal functor, we are just asking that boxes and twists do not get stuck on one another.}}{88}{figure.caption.53}\protected@file@percent }
\newpmemlabel{^_99}{89}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces To motivate opfibrations, first observe that by the diagrammatic equations of monoidal categories and functor boxes we have so far, we can always "slide out" the contents of a functor box out of the bottom. When can we do the reverse? That is, take a morphism in $\mathcal  {D}$ and \emph  {slide it into} a functor box? We know that in general this is not possible, because not all morphisms in $\mathcal  {D}$ may be in the image of $\mathbf  {F}$. So instead we ask "under what circumstances" can we do this for a functor $\mathbf  {F}$? The answer is when $\mathbf  {F}$ is a discrete opfibration.}}{89}{figure.caption.54}\protected@file@percent }
\newlabel{defn:discopf}{{2.2.2}{90}{Discrete opfibration}{theorem.2.2.2}{}}
\newlabel{defn:mondiscopf}{{2.2.3}{91}{Monoidal discrete opfibration}{theorem.2.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}What are they good for?}{92}{subsection.2.2.1}\protected@file@percent }
\newpmemlabel{^_100}{92}
\newpmemlabel{^_101}{92}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces  Now we try to use monoidal discrete opfibrations to help us solve the speaker's nonfunctoriality problem (Example \ref  {ex:nonfunctprod}). First we flip over the labels and introduction rules for adverbs. Call this a \emph  {dependent CFG}, or \emph  {dCFG}. There are several ways to do this formally, by e.g. specifying a new string-diagram signature from the old one or assuming rigid autonomous completion, and it doesn't matter which we use. }}{92}{figure.caption.55}\protected@file@percent }
\newpmemlabel{^_102}{92}
\newpmemlabel{^_103}{92}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces  Treating the label as a test rather than a state will allow the opfibration-box to choose the right version based on the domain wires as it expands top-down. In this case, since CFGs are planar, flipping causes no confusion, since we can always flip the labels back over. Recall that opfibrations can decide which lift to depict given a choice of codomain wires. We would like to encode the dependency of the upside-down adverb labels and introduction rules as lifts that depend on the lift of the verb wire, which may be either an intransitive or transitive verb. }}{92}{figure.caption.56}\protected@file@percent }
\newpmemlabel{^_104}{92}
\newpmemlabel{^_106}{92}
\newpmemlabel{^_105}{93}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces  Instead of \emph  {us} making the choice, we can force the choice using the information of the CFG structure. Starting from a dCFG diagram, the state-labels have unique lifts: noun labels in CFGs correspond uniquely to noun-states in pregroup diagrams, and verb labels to verb-states which may be either intransitive or transitive. This obtains the first equation. The second equation is obtained by monoidality. The third "eating downwards" equation is obtained by the opfibration property; note that because the codomain wires before the lift are already decided to be those of an intransitive verb's pregroup type, the correct adverb introduction rule can be selected for the lift. }}{93}{figure.caption.57}\protected@file@percent }
\newpmemlabel{^_107}{93}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces  But there's a technical problem. We have been assigning wires from the codomain of the lift to the dCFG implicitly, by grouping wires together visually to indicate which wires inside the functor box correspond to wires outside. However, when we consider the algebraic data available, all we know is depicted in the figure: we need some way to assign the wires. Solving the wire assignment probem will be the focus of the next section. }}{93}{figure.caption.58}\protected@file@percent }
\newlabel{fig:wireproblem}{{2.21}{93}{\@tufte@stored@shortcaption }{figure.caption.58}{}}
\citation{wilsonStringDiagramsNonstrict2022a}
\citation{wilsonStringDiagramsNonstrict2022a}
\newlabel{defn:strict}{{2.3.1}{94}{Strictified string diagrams}{theorem.2.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Strictified diagrams for monoidal categories}{94}{section.2.3}\protected@file@percent }
\newlabel{cons:bracketing}{{2.3.3}{94}{Pregroups with bracketing}{theorem.2.3.3}{}}
\newlabel{ex:prooftree}{{2.3.4}{95}{Pregroups with bracketing recover proof trees}{theorem.2.3.4}{}}
\tcolorbox@label{5}{96}
\newlabel{cons:pg2cfg}{{2.3.5}{96}{Discrete monoidal opfibration from pregroups with bracketing into dependent CFGs}{theorem.2.3.5}{}}
\tcolorbox@label{6}{97}
\tcolorbox@label{7}{98}
\citation{clarkeDoubleCategoryLenses2023a}
\newlabel{prop:strictequiv}{{2.3.2}{99}{$\bar {\mathcal {M}}$ and $\mathcal {M}$ are monoidally equivalent}{theorem.2.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Monoidal cofunctor boxes}{99}{section.2.4}\protected@file@percent }
\newlabel{defn:bijonobj}{{2.4.3}{99}{Bijective-on-objects functor}{theorem.2.4.3}{}}
\newlabel{prop:cofunctorspan}{{2.4.4}{99}{Cofunctors as spans of functors}{theorem.2.4.4}{}}
\tcolorbox@label{8}{100}
\tcolorbox@label{9}{101}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Monoidal kinda-cofunctor boxes}{102}{section.2.5}\protected@file@percent }
\newpmemlabel{^_108}{102}
\newpmemlabel{^_109}{102}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces  Starting from the leftmost diagram, in order to let the functor box eat the whole diagram, we need to first choose a lift for the left-sentence wire for the cup. Recalling Figures \ref  {fig:GFOLex2} and \ref  {ex:nonfunctparse}, there are at least two lifts for the sentence-wire in pregroup diagrams, for the case of two or three noun-wires. Everything works smoothly when the lifts on the two sentence wires of a cup match. When if we make the wrong choice and they don't, there is no lift, because there is no such thing as a cup that has two wires on one end and three on the other. Recall from Definition \ref  {defn:discopf} that a unique lift is required for \emph  {every possible} codomain inside the functor box; so we do not have a discrete opfibration, and so we cannot have a cofunctor. }}{102}{figure.caption.64}\protected@file@percent }
\newlabel{fig:notafibex}{{2.22}{102}{\@tufte@stored@shortcaption }{figure.caption.64}{}}
\newlabel{defn:discopf}{{2.5.1}{102}{Kinda-opfibration}{theorem.2.5.1}{}}
\citation{wang-mascianicaInternalWirings}
\citation{huExternalTracedMonoidala,romanOpenDiagramsCoend2021a}
\tcolorbox@label{10}{103}
\newlabel{ex:bigexlift}{{2.5.3}{103}{A monoidal kinda-opfibration into pregroup diagrams from a subcategory of bracketed pregroups with spiders}{theorem.2.5.3}{}}
\tcolorbox@label{11}{104}
\tcolorbox@label{12}{105}
\tcolorbox@label{13}{106}
\tcolorbox@label{14}{107}
\tcolorbox@label{15}{108}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Discussion and Limitations}{109}{section.2.6}\protected@file@percent }
\citation{linuspaulingmemoriallectureseriesNeuroscienceLanguageThought2018}
\citation{buszkowskiPregroupGrammarsContextfreea}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Text circuits for syntax}{115}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:textcircuits}{{3}{115}{Text circuits for syntax}{chapter.3}{}}
\citation{baezIntroductionNCategories1997}
\citation{nlabauthorsHomotopyIoNLab,dornAssociativeCategories2023,reutterHighlevelMethodsHomotopy2019c,heidemannZigzagNormalisationAssociative2022}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}An introduction to weak n-categories for formal linguists}{116}{section.3.1}\protected@file@percent }
\newlabel{sec:weakn}{{3.1}{116}{An introduction to weak n-categories for formal linguists}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The category in question can be visualised as a commutative diagram.\relax }}{117}{figure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces When there are too many generating morphisms, we can instead present the same data as a table of $n$-cells; there is a single 0-cell $\star $, and three non-identity 1-cells corresponding to $\leavevmode {\color  {green}\alpha }, \leavevmode {\color  {orange}\beta }, \leavevmode {\color  {cyan}\gamma }$, each with source and target 0-cells $\star $. Typically identity morphisms can be omitted from tables as they come for free. Observe that composition of identities enforces the behaviour of the empty string, so that for any string $x$, we have $\epsilon \cdot x = x = \epsilon \cdot x$.\relax }}{117}{figure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}String-rewrite systems as 1-object-2-categories}{117}{subsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces For a concrete example, we can depict the string $\leavevmode {\color  {green}\alpha } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {cyan}\gamma } \cdot \leavevmode {\color  {orange}\beta }$ as a morphism in a commuting diagram.\relax }}{118}{figure.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The string-diagrammatic view, where $\star $ is treated as a wire and morphisms are treated as boxes or dots is an expression of the same data under the Poincar\'{e} dual.\relax }}{118}{figure.3.4}\protected@file@percent }
\newlabel{fig:ruleR}{{3.1.1}{118}{String-rewrite systems as 1-object-2-categories}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces We can visualise the rule as a commutative diagram where $\leavevmode {\color  {magenta}R}$ is a 2-cell between the source and target 1-cells. Just as 1-cells are arrows between 0-cell points in a commuting diagram, a 2-cell can also be conceptualised as a directed surface from a 1-cell to another. Taking the Poincar\'{e} dual of this view gives us a string diagram for the 2-cell $\leavevmode {\color  {magenta}R}$.\relax }}{118}{figure.3.5}\protected@file@percent }
\newlabel{fig:cfgsig}{{3.1.1}{119}{String-rewrite systems as 1-object-2-categories}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces We can describe a context-free grammar with the same combinatorial rewriting data that specifies planar string diagrams as we have been illustrating so far. Here is a context-free grammar for \texttt  {Alice sees Bob quickly run to school}. \relax }}{119}{figure.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Tree Adjoining Grammars}{119}{subsection.3.1.2}\protected@file@percent }
\newpmemlabel{^_111}{120}
\newlabel{prop:cfgastag1}{{3.1.3}{121}{}{theorem.3.1.3}{}}
\newpmemlabel{^_110}{121}
\newpmemlabel{^_112}{121}
\newpmemlabel{^_113}{121}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Instead of treating non-terminals as wires and terminals as effects (so that the presence of an open wire available for composition visually indicates non-terminality) the leaf-ansatz construction treats all symbols in a rewrite system as leaves, and the signature bookkeeps the distinction between nonterminals and terminals.}}{121}{figure.caption.72}\protected@file@percent }
\newpmemlabel{^_114}{121}
\newpmemlabel{^_115}{121}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Adjoining is sprouting subtrees in the middle of branches. One way we might obtain the sentence \texttt  {Bob runs to school} is to start from the simpler sentence \texttt  {Bob runs}, and then refine the verb \texttt  {runs} into \texttt  {runs to school}. This refinement on part of an already completed sentence is not permitted in CFGs, since terminals can no longer be modified. The adjoining operation of TAGs gets around this constraint by permitting rewrites in the middle of trees.}}{121}{figure.caption.73}\protected@file@percent }
\newpmemlabel{^_116}{121}
\newpmemlabel{^_117}{122}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces  Leaf-ansatz signature of \texttt  {Alice sees Bob quickly run to school} CFG.  \par 1-cells correspond to types. There are three kinds of 2-cells organised in rows; terminals, substitutable ans\"{a}tze that convert wires into bulbs; and the symbolic rewrites of CFGs respectively. There are two kinds of 3-cells similarly organised in rows; terminal-rewrites that replace a bulb with a terminal, and rewrites that mimic the CFG rewrites on ans\"{a}tze. Note the one-to-one correspondence between the 2-cells and 3-cells for CFG rewrites and terminals.  \par In more detail, one aspect of rewrite systems we adapt for now is the distinction between terminal and nonterminal symbols; terminal symbols are those after which no further rewrites are possible. We capture this string-diagrammatically by modelling terminal rewrites as 2-cells with target equal to the 1-cell identity of the 0-cell $\star $, which amounts to graphically terminating a wire. The generators subscripted $L$ (for \emph  {label} or \emph  {leaf}) correspond to terminals of the CFG, and represent a family of generators indexed by a lexicon for the language. The generators subscripted $i$ (for introducing a type) correspond to rewrites of the CFG. }}{122}{figure.caption.74}\protected@file@percent }
\citation{joshiIntroductionTreeAdjoining1987}
\citation{joshiIntroductionTreeAdjoining1987}
\newpmemlabel{^_118}{123}
\newpmemlabel{^_119}{123}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces TAG signature of \texttt  {Alice sees Bob quickly run to school}. The \texthl  {highlighted 2-cells} are auxiliary trees that replace CFG 2-cells for verbs with sentential complement, adverbs, and adpositions. The \texthl  {highlighted 3-cells} are the tree adjoining operations of the auxiliary trees. The construction yields as a corollary an alternate proof of Theorem 6.1.1 of \citep  {joshiIntroductionTreeAdjoining1987} that recovers CFGs as TAGs.}}{123}{figure.caption.75}\protected@file@percent }
\newpmemlabel{^_120}{124}
\newpmemlabel{^_121}{124}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Reading the central diagram in the main body from left-to-right, we additionally depict the breakdown of the complete derivation in terms of the constitituent 2-cells, and the source and target 1-cells. Evidently, all context-sensitive grammars may be viewed as finitely presented 1-object-2-categories by considering multi-input-multi-output rewrites. More broadly, any string rewriting system is recoverable in the presence of higher dimensional cells. My source for that is that I made a Turing machine in \texttt  {homotopy.io} and executed busy-beaver on it as a homework exercise when Jamie Vicary taught Categorical Quantum Mechanics at Oxford.}}{124}{figure.caption.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Tree adjoining grammars with local constraints}{125}{subsection.3.1.3}\protected@file@percent }
\newpmemlabel{^_122}{125}
\newpmemlabel{^_123}{125}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces  Selective and null adjoining diagrammatically: a reproduction of Example 2.5 of [Joshi] which demonstrates the usage of selective and null adjoining. The notation from [Joshi] is presented first, followed by their corresponding representations in an $n$-categorical signature. The initial tree is presented as a 2-cell where the (SA) rules are rewritable nodes, that serve as sources of rewrites in the 3-cell presentations of the auxiliary trees. }}{125}{figure.caption.77}\protected@file@percent }
\newpmemlabel{^_124}{125}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Braiding, symmetries, and suspension}{125}{subsection.3.1.4}\protected@file@percent }
\newpmemlabel{^_125}{126}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces  Obligatory adjoining diagrammatically: a reproduction of Example 2.11 of [Joshi] which demonstrates the usage of obligatory adjoining, marked orange. The notation from [Joshi] is presented first, followed by their corresponding representations in an $n$-categorical signature. The initial tree is presented as a 2-cell where the (OA) rule is given its own 2-cell, which is the source of rewrites in 3-cell presentations of auxiliary trees. We may capture the obligatory nature of the rewrite by asking that finished derivations contain no instance of the orange 2-cell. Such global acceptance conditions are hacky but common, and in this case it is efficiently verifiable that diagrams do not contain certain generators. }}{126}{figure.caption.78}\protected@file@percent }
\newpmemlabel{^_126}{126}
\newpmemlabel{^_127}{126}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces  In our analogy with string rewrite systems, we might like that the following rewrites are equivalent, while respecting that they are not equal, representing $x,a,b$ as blue, red, and green wires respectively. Such rewrites from the empty string to itself are more generally called \emph  {scalars} in the monoidal setting, viewed 2-categorically. }}{126}{figure.caption.79}\protected@file@percent }
\newpmemlabel{^_128}{126}
\newpmemlabel{^_130}{126}
\newpmemlabel{^_129}{127}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces We may generally represent such scalars as labelled dots. A fact about scalars in a 1-object-2-category called the Eckmann-Hilton argument is that dots may circle around one another, and all of those expressions are equivalent up to homotopy. The mechanism that enables this in our setting is that the empty string is equal to copies of itself, which creates the necessary space for manoeuvering; translating into the $n$-categorical setting, expressions are equivalent up to introducing and contracting identities.}}{127}{figure.caption.80}\protected@file@percent }
\newpmemlabel{^_131}{127}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces We may view the homotopies that get us from one rewrite to another as 3-cells, which produces a braid in a pair of wires when viewed as a vignette. Up to processive isotopies, which are continuous bijective transformations that don't let wires double back on themselves, we can identify two different braidings that are not continuously deformable to one another in the 3-dimensional space of the vignette. We distinguish the braidings visually by letting wires either go over or under one another.}}{127}{figure.caption.81}\protected@file@percent }
\newpmemlabel{^_132}{127}
\newpmemlabel{^_133}{128}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces We can depict these swaps by movements in a cubic volume where each axis corresponds to a direction of composition. Whereas on the plane the dots have two ways to swap places -- clockwise and counterclockwise rotation -- in the volume they have two new ways to swap places -- clockwise and counterclockwise in the new dimension. Shown below are two ways to swap left-to-right sequentially composed dots by clockwise rotations in the forward-backward and up-down directions of composition:}}{128}{figure.caption.82}\protected@file@percent }
\newpmemlabel{^_134}{128}
\citation{nlabauthorsStabilizationHypothesisNLab}
\citation{nlabauthorsStabilizationHypothesisNLab}
\newpmemlabel{^_135}{129}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces For example, taking our CFG signature from earlier, suspension promotes 1-cells to 3-cells and 2-cells to 4-cells. The resulting signature gives us the same diagrams, now with the added ability to consider diagrams equivalent up to twisting wires, which models a string-rewrite system with free swapping of symbol order.}}{129}{figure.caption.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}TAGs with links}{129}{subsection.3.1.5}\protected@file@percent }
\newpmemlabel{^_136}{129}
\newpmemlabel{^_138}{129}
\newpmemlabel{^_140}{129}
\newpmemlabel{^_137}{130}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces TAG signature and example derivation. Joshi stresses that adjoining \emph  {preserves} links, and that elementary trees may become \emph  {stretched} in the process of derivation, which are fundamentally topological constraints, akin to the "only (processive) connectivity matters" criterion identifying string diagrams up to isomorphism. Moreover, TAGs evidently have links of two natures: tree edges intended to be planar, and dashed dependency edges intended to freely cross over tree edges. It is easy, but a hack, to ask for planar processive isomorphisms for tree edges and extraplanar behaviour for dependency edges: these are evidently two different kinds of structure glued together, rather than facets of some whole. Weak $n$-categories offer a unified mathematical framework that natively accommodates the desired topological constraints while also granting expressive control over wire-types of differing behaviours. One method to recover TAGs true to the original conception is to stay in a planar 1-object-2-category setting while explicitly including wire-crossing cells for dependency links. The alternative method we opt for in Section \ref  {sec:gencirc} is to work in a pure "only connectivity matters" setting, recovering the linear ordering of words by generating cells along a chosen wire. I do not know of any conceptual justification for why planarity is so often an implicit constraint in approaches to formal syntax. My best guesses are either that the first port of call for rewrites between 1-dimensional strings of words is a 2-dimensional setting, or it is a limitation of 2-dimensional paper as a medium of thought along with some confusion of map for territory.}}{130}{figure.caption.84}\protected@file@percent }
\newpmemlabel{^_139}{131}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces With our interpretation of TAGS as weak $n$-categorical signatures, We can recover each step of the same example derivation automagically in \texttt  {homotopy.io}; just clicking on where we want rewrites allows the proof assistant to execute a typematching tree adjunction. In the process of interpretation, we introduce a link wire-type (in purple), and include directed link generation and elimination morphisms for the $T$ wire-type (in blue). A necessary step in the process of interpretation (which for us involves taking a Poincar\'{e} dual to interpret nodes as wires) is a typing assignment of the tree-branches connected to terminal nodes, which we have opted to read as sharing a $T$-type for minimality, though we could just as well have introduced a separate label-type wire.}}{131}{figure.caption.85}\protected@file@percent }
\newpmemlabel{^_141}{131}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces The intended takeaway is that even if you don't buy the necessity or formality of weak $n$-categories, there is always the fallback epistemic underpinning of a formal proof assistant for higher dimensional rewriting theories, which is rather simple to use if I have succeeded in communicating higher-dimensional intuitions in this section. \textbf  {N.B.} In practice when using \texttt  {homotopy.io} for the symmetric monoidal setting, it is simpler to suspend symmetric monoidal signatures to begin at 4-cells rather than 3-cells. The reason for this is that under- and over-braids still exist in the symmetric monoidal setting, and while sequentially composed braids are homotopically equivalent to the pair of identities, they are not uniquely so, thus these homotopies must be input manually. By beginning at 4-cells (or higher, due to the stabilisation hypothesis \citep  {nlabauthorsStabilizationHypothesisNLab}), braid-eliminations are unique up to homotopy and can be performed more easily in the proof assistant.}}{131}{figure.caption.86}\protected@file@percent }
\tcolorbox@label{16}{132}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Full TAGs in weak $n$-categories}{132}{subsection.3.1.6}\protected@file@percent }
\tcolorbox@label{17}{133}
\newlabel{defn:lex}{{3.2.1}{134}{Lexicon}{theorem.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces \textbf  {How to read the diagrams in this section:} we will be making heavy use of pink and purple bubbles as frames to construct circuits. We will depict the bubbles horizontally, as we are permitted to by compact closure, or by reading diagrams with slightly skewed axes.\relax }}{134}{figure.3.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Every derivation starts with a single blank sentence bubble, to which we may append more blank sentences.\relax }}{134}{figure.3.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}A generative grammar for text circuits}{134}{section.3.2}\protected@file@percent }
\newlabel{sec:gencirc}{{3.2}{134}{A generative grammar for text circuits}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A circuit-growing grammar}{134}{subsection.3.2.1}\protected@file@percent }
\newpmemlabel{^_142}{135}
\newpmemlabel{^_143}{135}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces In this toy example, obtaining the same rewrite that connects the two yellow nodes with a purple wire using only graph-theoretically-local rewrites could potentially require an infinite family of rules for all possible configurations of pink and cyan nodes that separate the yellow, or would otherwise require disturbing other nodes in the rewrite process. In our setting, strong compact closure homotopies handle navigation between different spatial presentations so that a single rewrite rule suffices: the source and target notated by dotted-black circles. Despite the expressive economy and power of finitely presented signatures, we cannot "computationally cheat" graph isomorphism: formally we must supply the compact-closure homotopies as part of the rewrite, absorbed and hidden here by the $\simeq $ notation.}}{135}{figure.caption.89}\protected@file@percent }
\newlabel{fig:locality}{{3.24}{135}{\@tufte@stored@shortcaption }{figure.caption.89}{}}
\tcolorbox@label{18}{137}
\newlabel{rules:simp}{{3.2.2}{137}{Simple sentences}{theorem.3.2.2}{}}
\tcolorbox@label{19}{138}
\newlabel{rules:comp}{{3.2.3}{138}{Complex sentences}{theorem.3.2.3}{}}
\tcolorbox@label{20}{139}
\newlabel{ex:soberA}{{3.2.4}{139}{\texttt {sober} $\alpha $ \texttt {sees drunk} $\beta $ \texttt {clumsily dance.}}{theorem.3.2.4}{}}
\tcolorbox@label{21}{140}
\newlabel{ex:Alaughs}{{3.2.5}{140}{$\alpha $ \texttt {laughs at} $\beta $}{theorem.3.2.5}{}}
\tcolorbox@label{22}{141}
\newlabel{rules:coref}{{3.2.6}{141}{Coreferential structure and noun labels}{theorem.3.2.6}{}}
\tcolorbox@label{23}{142}
\newlabel{rules:labels}{{3.2.7}{142}{Labelling nouns}{theorem.3.2.7}{}}
\tcolorbox@label{24}{143}
\newlabel{ex:corefex1}{{3.2.8}{143}{\texttt {sober Alice sees Bob clumsily dance. She laughs at him.}}{theorem.3.2.8}{}}
\tcolorbox@label{25}{144}
\tcolorbox@label{26}{145}
\newlabel{cons:wirejoin}{{3.2.9}{145}{Text to circuit}{theorem.3.2.9}{}}
\tcolorbox@label{27}{146}
\tcolorbox@label{28}{147}
\newlabel{ex:directgrowth}{{3.2.11}{147}{Growing circuits directly}{theorem.3.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Text circuit theorem}{148}{subsection.3.2.2}\protected@file@percent }
\newlabel{dfn:simpCSG}{{3.2.12}{149}{CSG for simple sentences}{theorem.3.2.12}{}}
\newpmemlabel{^_144}{149}
\newpmemlabel{^_145}{149}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces  Reading each diagram from top-to-bottom, from left-to-right we have generators for intransitive verbs, transitive verbs, adjectives, and adverbs. Generators for verbs require a number of $\texttt  {N}_{\uparrow }$ matching their arity as input, hence a CSG. }}{149}{figure.caption.101}\protected@file@percent }
\newlabel{fig:simpleCFG}{{3.25}{149}{\@tufte@stored@shortcaption }{figure.caption.101}{}}
\newpmemlabel{^_146}{149}
\newpmemlabel{^_147}{149}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces  Adpositions require several helper-generators, which are the components within dashed boxes in the depicted example demonstrating the process of appending adpositions to an intransitive verb. }}{149}{figure.caption.102}\protected@file@percent }
\newlabel{fig:simpleADP}{{3.26}{149}{\@tufte@stored@shortcaption }{figure.caption.102}{}}
\newlabel{prop:simpsent}{{3.2.13}{149}{}{theorem.3.2.13}{}}
\newpmemlabel{^_148}{149}
\newpmemlabel{^_149}{149}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces  Viewing nodes on the pink surface of circuit-growing grammar as 1-cells, each rewrite rule yields a 2-cell; e.g. the dashed-blue helper lines for adpositions correspond to the \leavevmode {\color  {blue}\texttt  {ADP}}-pass rules in circuit-growing grammar. The correspondence between the \leavevmode {\color  {green}\texttt  {IV}}-intro rules of both grammars is depicted. }}{149}{figure.caption.103}\protected@file@percent }
\newlabel{fig:correspondence}{{3.27}{149}{\@tufte@stored@shortcaption }{figure.caption.103}{}}
\newlabel{prop:compsent}{{3.2.14}{149}{}{theorem.3.2.14}{}}
\newpmemlabel{^_150}{150}
\newpmemlabel{^_151}{150}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces  The first rule instantiates the left and right boundaries of a sentence, corresponding the starting bubble in circuit-growing grammar. The second corresponds to $\texttt  {N}_\uparrow $-intro, the third $\leavevmode {\color  {blue}\texttt  {CNJ}}$-intro, and the fourth $\leavevmode {\color  {green}\texttt  {SCV}}$-intro. }}{150}{figure.caption.104}\protected@file@percent }
\newlabel{fig:compsentCSG}{{3.28}{150}{\@tufte@stored@shortcaption }{figure.caption.104}{}}
\newpmemlabel{^_152}{150}
\newpmemlabel{^_153}{150}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces We choose the convention of connecting from left-to-right and from bottom-to-top, so that we might read circuits as we would English text: the components corresponding to words will be arranged in the reverse order, left-to-right and top-to-bottom.}}{150}{figure.caption.105}\protected@file@percent }
\newlabel{fig:nounconnection}{{3.29}{150}{\@tufte@stored@shortcaption }{figure.caption.105}{}}
\newlabel{term:nounkinds}{{3.2.15}{150}{Kinds of nouns with respect to coreference}{theorem.3.2.15}{}}
\citation{wang-mascianicaDistillingTextCircuits2023a}
\citation{wang-mascianicaDistillingTextCircuits2023a}
\newlabel{prop:linkedlist}{{3.2.16}{151}{}{theorem.3.2.16}{}}
\newlabel{prop:norefl}{{3.2.17}{151}{}{theorem.3.2.17}{}}
\newpmemlabel{^_154}{151}
\newpmemlabel{^_155}{151}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces From left to right in roughly decreasing stringency, compact closed categories are the most direct solution for reflexive pronouns. Traced symmetric monoidal categories also suffice. So long as the noun wire possesses a monoid and comonoid, a convolution works. We also can just specify a new gate. We provide a purely syntactic treatment in \citep  {wang-mascianicaDistillingTextCircuits2023a}; for now we treat them as if they were just verbs of lower arity.}}{151}{figure.caption.106}\protected@file@percent }
\newlabel{fig:reflcomp}{{3.30}{151}{\@tufte@stored@shortcaption }{figure.caption.106}{}}
\newlabel{defn:finished}{{3.2.18}{152}{Finished text diagram}{theorem.3.2.18}{}}
\newlabel{prop:text2circ}{{3.2.20}{152}{Finished text diagrams yield unique text circuits (up to processive isotopies)}{theorem.3.2.20}{}}
\newlabel{conv:twist}{{3.2.22}{152}{Wire twisting}{theorem.3.2.22}{}}
\newpmemlabel{^_156}{152}
\newlabel{conv:gaps}{{3.2.23}{152}{Arbitary vs. fixed holes}{theorem.3.2.23}{}}
\newlabel{conv:sliding}{{3.2.24}{152}{Sliding}{theorem.3.2.24}{}}
\newpmemlabel{^_158}{152}
\newpmemlabel{^_157}{153}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces  Only connectivity matters in text circuits, which we may use to freely rearrange and simplify presentations.}}{153}{figure.caption.107}\protected@file@percent }
\newlabel{fig:twistsimple}{{3.31}{153}{\@tufte@stored@shortcaption }{figure.caption.107}{}}
\newpmemlabel{^_159}{153}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces  \leavevmode {\color  {blue}Postscript: While sequential composition in process theories often has implicit temporality, this is not necessarily the case for text circuits, which may just (for instance) represent relational constraints. Temporality may be achieved in text circuits by interpreting them in premonoidal settings [CITE], at the cost of the interchange rule depicted here.} }}{153}{figure.caption.108}\protected@file@percent }
\newlabel{fig:sliding}{{3.32}{153}{\@tufte@stored@shortcaption }{figure.caption.108}{}}
\newlabel{conv:reading}{{3.2.25}{153}{Reading text circuits}{theorem.3.2.25}{}}
\newlabel{conv:and}{{3.2.26}{153}{Contentless conjunctions}{theorem.3.2.26}{}}
\newpmemlabel{^_160}{153}
\newpmemlabel{^_161}{153}
\@writefile{lof}{\contentsline {figure}{\numberline {3.33}{\ignorespaces  Parallel gates represent compound sentences with contentless conjunctions. In English, some examples might be a punctuation mark such as a comma, or phrases such as \texttt  {and also}. }}{153}{figure.caption.109}\protected@file@percent }
\newlabel{fig:contentlessCNJ}{{3.33}{153}{\@tufte@stored@shortcaption }{figure.caption.109}{}}
\newlabel{conv:exists}{{3.2.27}{153}{Lonely wires}{theorem.3.2.27}{}}
\newpmemlabel{^_162}{154}
\newpmemlabel{^_163}{154}
\@writefile{lof}{\contentsline {figure}{\numberline {3.34}{\ignorespaces  Lonely wires in text circuits are identity processes. We require a text diagram analogue, and an intransitive "null-verb" in English that seems to work is \texttt  {is}, in the sense of \texttt  {exists}. }}{154}{figure.caption.110}\protected@file@percent }
\newlabel{fig:exists}{{3.34}{154}{\@tufte@stored@shortcaption }{figure.caption.110}{}}
\tcolorbox@label{29}{155}
\newlabel{cons:circ2text}{{3.2.28}{155}{Circuit to text}{theorem.3.2.28}{}}
\tcolorbox@label{30}{156}
\tcolorbox@label{31}{157}
\tcolorbox@label{32}{158}
\citation{BAbIMetaResearch}
\citation{anonymousquantinuumresearchersDisCoCircCompositionalDiscource}
\citation{dudzikGraphNeuralNetworks2022}
\citation{wilsonStringDiagramsNonstrict2022a}
\citation{merryReasoningGraphs2014a,quickdavidLogic2015,zamdzhievRewritingContextfreeFamilies2017a}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Text circuits: details and development}{159}{section.3.3}\protected@file@percent }
\newlabel{sec:circs}{{3.3}{159}{Text circuits: details and development}{section.3.3}{}}
\citation{coeckeGrammarEquations2021a}
\citation{heffordCategoriesSemanticConcepts2020b}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}How to play}{160}{section.3.4}\protected@file@percent }
\citation{wang-mascianicaDistillingTextCircuits2023a}
\citation{coeckeGrammarEquations2021a}
\tcolorbox@label{33}{163}
\newlabel{rules:relpron}{{3.4.1}{163}{}{theorem.3.4.1}{}}
\tcolorbox@label{34}{164}
\newlabel{ex:relpron}{{3.4.2}{164}{Introducing relative pronouns}{theorem.3.4.2}{}}
\tcolorbox@label{35}{165}
\newlabel{ex:pass}{{3.4.3}{165}{\textbf {Passive voice}}{theorem.3.4.3}{}}
\tcolorbox@label{36}{165}
\newlabel{ex:copula}{{3.4.4}{165}{\textbf {Copulas}}{theorem.3.4.4}{}}
\tcolorbox@label{37}{166}
\newlabel{ex:posspron}{{3.4.5}{166}{\textbf {Possessive pronouns}}{theorem.3.4.5}{}}
\tcolorbox@label{38}{167}
\newlabel{ex:intensifiers}{{3.4.6}{167}{\textbf {Intensifers}}{theorem.3.4.6}{}}
\tcolorbox@label{39}{168}
\newlabel{ex:comparatives}{{3.4.7}{168}{\textbf {Comparatives}}{theorem.3.4.7}{}}
\tcolorbox@label{40}{169}
\newlabel{ex:syncat1}{{3.4.8}{169}{\textbf {Syncategorematicity I}}{theorem.3.4.8}{}}
\citation{urquhartFineArbitraryObjects2020}
\citation{balkirDistributionalSentenceEntailment2015}
\tcolorbox@label{41}{170}
\newlabel{ex:syncat2}{{3.4.9}{170}{\textbf {Syncategorematicity II}}{theorem.3.4.9}{}}
\tcolorbox@label{42}{170}
\newlabel{ex:coord}{{3.4.10}{170}{\textbf {Coordination}}{theorem.3.4.10}{}}
\tcolorbox@label{43}{171}
\newlabel{ex:det1}{{3.4.11}{171}{\textbf {Determiners I}}{theorem.3.4.11}{}}
\tcolorbox@label{44}{171}
\newlabel{ex:det2}{{3.4.12}{171}{\textbf {Determiners II}}{theorem.3.4.12}{}}
\tcolorbox@label{45}{172}
\newlabel{ex:det3}{{3.4.13}{172}{\textbf {Determiners III}}{theorem.3.4.13}{}}
\tcolorbox@label{46}{173}
\newlabel{ex:quant2}{{3.4.14}{173}{\textbf {Quantifiers I}}{theorem.3.4.14}{}}
\tcolorbox@label{47}{174}
\newlabel{ex:quant2}{{3.4.15}{174}{\textbf {Quantifiers II}}{theorem.3.4.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Continuous relations for semantics}{175}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:contrel}{{4}{175}{Continuous relations for semantics}{chapter.4}{}}
\citation{reddyConduitMetaphorCase}
\citation{reddyConduitMetaphorCase}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Continuous Relations for iconic semantics}{176}{section.4.1}\protected@file@percent }
\newlabel{sec:contrelintro}{{4.1}{176}{Continuous Relations for iconic semantics}{section.4.1}{}}
\newpmemlabel{^_164}{176}
\newpmemlabel{^_165}{176}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Sometimes it is very helpful to illustrate concepts using iconic representations in cartoons. For instance in the \emph  {conduit metaphor} \citep  {reddyConduitMetaphorCase}, \texttt  {words} are considered \emph  {containers} for \texttt  {ideas}, and \texttt  {communication} is considered a \emph  {conduit} along which those containers are sent.}}{176}{figure.caption.130}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Continuous Relations by examples}{178}{section.4.2}\protected@file@percent }
\newlabel{sec:contrelex}{{4.2}{178}{Continuous Relations by examples}{section.4.2}{}}
\newlabel{defn:Contrelation}{{4.2.12}{178}{Continuous Relation}{theorem.4.2.12}{}}
\newlabel{ex:nontop}{{4.2.7}{179}{A noncontinuous relation}{theorem.4.2.7}{}}
\newlabel{prop:states}{{4.2.9}{179}{}{theorem.4.2.9}{}}
\newlabel{prop:tests}{{4.2.10}{179}{}{theorem.4.2.10}{}}
\newlabel{prop:framehom}{{4.2.13}{179}{}{theorem.4.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Regions of $\blacksquare $ in the image of the yellow point alone will be coloured yellow, and regions in the image of both yellow and cyan will be coloured green:\relax }}{180}{figure.4.2}\protected@file@percent }
\newlabel{fig:yellowgreen}{{4.2}{180}{Regions of $\blacksquare $ in the image of the yellow point alone will be coloured yellow, and regions in the image of both yellow and cyan will be coloured green:\relax }{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Regions in the image of the cyan point alone cannot be open sets by continuity, so they are either points or lines. Points and lines in cyan must be surrounded by an open region in either yellow or green, or else we violate continuity (open sets in red).\relax }}{180}{figure.4.3}\protected@file@percent }
\newlabel{fig:cyan}{{4.3}{180}{Regions in the image of the cyan point alone cannot be open sets by continuity, so they are either points or lines. Points and lines in cyan must be surrounded by an open region in either yellow or green, or else we violate continuity (open sets in red).\relax }{figure.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces A continuous relation $\mathcal  {S} \rightarrow \blacksquare $: "Flower and critter in a sunny field".\relax }}{180}{figure.4.4}\protected@file@percent }
\newlabel{fig:flower}{{4.4}{180}{A continuous relation $\mathcal {S} \rightarrow \blacksquare $: "Flower and critter in a sunny field".\relax }{figure.4.4}{}}
\newlabel{cor:homspace}{{4.2.14}{180}{}{theorem.4.2.14}{}}
\newlabel{lem:capideal}{{4.2.17}{180}{Partial functions are a $\cap $-ideal}{theorem.4.2.17}{}}
\newlabel{lem:edgecomplete}{{4.2.18}{180}{Any single edge can be extended to a continuous partial function}{theorem.4.2.18}{}}
\newlabel{prop:hombasis}{{4.2.19}{180}{}{theorem.4.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A continuous relation $\blacksquare \rightarrow \mathcal  {S}$: "still math?". Black lines and dots indicate gaps.\relax }}{181}{figure.4.5}\protected@file@percent }
\newlabel{fig:shitpost}{{4.5}{181}{A continuous relation $\blacksquare \rightarrow \mathcal {S}$: "still math?". Black lines and dots indicate gaps.\relax }{figure.4.5}{}}
\newpmemlabel{^_166}{181}
\newpmemlabel{^_167}{182}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Hasse diagram of all continuous relations from the Sierpi\'{n}ski space to itself. Each relation is depicted left to right, and inclusion order is bottom-to-top. Relations that form the topological basis are boxed.}}{182}{figure.caption.131}\protected@file@percent }
\newlabel{fig:hassesierpinski}{{4.6}{182}{\@tufte@stored@shortcaption }{figure.caption.131}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces  continuous functions $[0,1] \rightarrow \blacksquare $ follow the na\"{i}ve notion of continuity: a line one can draw on paper without lifting the pen off the page. \relax }}{183}{figure.4.7}\protected@file@percent }
\newlabel{fig:contline}{{4.7}{183}{continuous functions $[0,1] \rightarrow \blacksquare $ follow the na\"{i}ve notion of continuity: a line one can draw on paper without lifting the pen off the page. \relax }{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces  An example of a continuous relation is \texttt  {"(countably) many (open-ended) lines, each of which one can draw on paper without lifting the pen off the page."} \relax }}{183}{figure.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces We can control the thickness of the brushstroke, by taking the union of (uncountably) many lines.\relax }}{183}{figure.4.9}\protected@file@percent }
\newlabel{fig:thickbrush}{{4.9}{183}{We can control the thickness of the brushstroke, by taking the union of (uncountably) many lines.\relax }{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Assign the visible spectrum of light to $[0,1]$. Colour open sets according to perceptual addition of light, computing brightness by normalising the measure of the open set.\relax }}{183}{figure.4.10}\protected@file@percent }
\newlabel{prop:func}{{4.3.1}{184}{}{theorem.4.3.1}{}}
\newlabel{corr:top2contrel}{{4.3.2}{184}{}{theorem.4.3.2}{}}
\newlabel{prop:idrel}{{4.3.3}{184}{}{theorem.4.3.3}{}}
\newlabel{fig:copy}{{4.3}{184}{The category \textbf {ContRel}}{theorem.4.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The \emph  {copy} map $X^\tau \rightarrow X^\tau \times X^\tau $, $\{(x,\begin  {pmatrix} x   x \end  {pmatrix}) \ | \ x \in X\}$.\relax }}{184}{figure.4.11}\protected@file@percent }
\newlabel{prop:copy}{{4.3.4}{184}{}{theorem.4.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The category \textbf  {ContRel}}{184}{section.4.3}\protected@file@percent }
\newlabel{sec:contrelmath}{{4.3}{184}{The category \textbf {ContRel}}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Symmetric Monoidal structure}{184}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces An alternative proof of Proposition \ref  {prop:copy} follows from Proposition \ref  {prop:func}, Corollary \ref  {corr:top2contrel}, and the definition of the product topology as the coarsest topology that satisfies categorical product for the diagram above.\relax }}{185}{figure.4.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces The \emph  {everything} state is the relation $\{(\star ,x) \mid x \in X\}$, notated as above.\relax }}{185}{figure.4.13}\protected@file@percent }
\newlabel{fig:everything}{{4.13}{185}{The \emph {everything} state is the relation $\{(\star ,x) \mid x \in X\}$, notated as above.\relax }{figure.4.13}{}}
\newlabel{prop:everything}{{4.3.5}{185}{}{theorem.4.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces The \emph  {delete} test, $\{(x,\star ) \mid x \in X\}$.\relax }}{185}{figure.4.14}\protected@file@percent }
\newlabel{fig:delete}{{4.14}{185}{The \emph {delete} test, $\{(x,\star ) \mid x \in X\}$.\relax }{figure.4.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Rig category structure}{185}{subsection.4.3.2}\protected@file@percent }
\citation{comfortSheetDiagramsBimonoidal2020}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Copy and delete satisfy the above properties, expressed as diagrammatic equations.\relax }}{186}{figure.4.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Monoidal (co!)closure}{186}{subsection.4.3.3}\protected@file@percent }
\newlabel{prop:prodvsclos}{{4.3.17}{186}{}{theorem.4.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Relations that interact with copy and delete are nice, and we notate them with the same black dots as for copy and delete to mark them. States are singletons, or points, when they are copiable (and non-empty). Partial continuous functions are those that commute with copy. Left-total relations are those that commute with delete. Continuous functions are those that satisfy the latter two criteria.\relax }}{187}{figure.4.16}\protected@file@percent }
\newlabel{prop:fullrel}{{4.3.7}{187}{}{theorem.4.3.7}{}}
\newlabel{defn:coarsening}{{4.3.19}{187}{Coarsening}{theorem.4.3.19}{}}
\newlabel{defn:compare}{{4.3.20}{187}{Pseudo-compare}{theorem.4.3.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces  The \textbf  {empty state} $\bullet \rightarrow X^\tau $ and \textbf  {empty test} relate nothing. The \textbf  {empty relation} $X^\tau \rightarrow Y^\sigma $ is the composite of empty tests and states, and relates nothing: as a relation it is $\varnothing \subset X \times Y$.\relax }}{188}{figure.4.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces There are two scalars: the unit, and the zero scalar. Both the one and zero scalars are idempotent, which diagrammatically means that we may freely make and merge copies of them.\relax }}{188}{figure.4.18}\protected@file@percent }
\newlabel{defn:perfectclosure}{{4.3.21}{188}{}{theorem.4.3.21}{}}
\newlabel{prop:coclosure}{{4.3.22}{188}{\textbf {ContRel} is monoidal coclosed}{theorem.4.3.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces There is a zero-morphism for every input-output pair of objects in \textbf  {ContRel}, which is diagrammatically the composition of the empty test and state. Zero scalars turn any relation into a zero relation. Substituting the zero relation into the LHS of the above equation means that zero relations also spawn zero scalars.\relax }}{189}{figure.4.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces So, whenever a zero-process appears in a diagram, it spawns zero scalars which infect all other processes, turning them all into zero-processes. The same holds for whenever a zero-scalar appears; it makes copies of itself to infect all other processes.\relax }}{189}{figure.4.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Category-theoretic endnotes}{190}{subsection.4.3.4}\protected@file@percent }
\newlabel{lem:disccont}{{4.3.23}{190}{Any relation $R$ between discrete topologies is continuous}{theorem.4.3.23}{}}
\newlabel{lem:idadj}{{4.3.26}{190}{$RL = 1_{\textbf {Rel}}$}{theorem.4.3.26}{}}
\newlabel{lem:coarse}{{4.3.28}{190}{Coarsening is a continuous relation}{theorem.4.3.28}{}}
\newlabel{prop:reladj}{{4.3.29}{190}{$L \dashv R$}{theorem.4.3.29}{}}
\citation{nlabauthorsLocNLab}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Populating space with shapes using sticky spiders}{195}{section.4.4}\protected@file@percent }
\newlabel{sec:stickyspider}{{4.4}{195}{Populating space with shapes using sticky spiders}{section.4.4}{}}
\newlabel{ex:compnotspider}{{4.4.1}{195}{The copy-compare spiders of $\mathbf {Rel}$ are not always continuous}{theorem.4.4.1}{}}
\tcolorbox@label{48}{196}
\newlabel{prop:copydiscrete}{{4.4.3}{196}{}{theorem.4.4.3}{}}
\tcolorbox@label{49}{197}
\newlabel{defn:stickyspider}{{4.4.4}{197}{Sticky spiders}{theorem.4.4.4}{}}
\tcolorbox@label{50}{198}
\tcolorbox@label{51}{198}
\newlabel{cons:stickyfromsplit}{{4.4.6}{198}{Sticky spiders from split idempotents}{theorem.4.4.6}{}}
\tcolorbox@label{52}{199}
\newlabel{prop:splitmeanssticky}{{4.4.7}{199}{Every idempotent that splits through a discrete topology gives a sticky spider}{theorem.4.4.7}{}}
\tcolorbox@label{53}{200}
\tcolorbox@label{54}{201}
\newlabel{thm:stickygraphical}{{4.4.8}{201}{}{theorem.4.4.8}{}}
\tcolorbox@label{55}{202}
\newlabel{prop:counitdelete}{{4.4.9}{202}{comult/copy implies counit/delete}{theorem.4.4.9}{}}
\tcolorbox@label{56}{203}
\newlabel{lem:allornothing}{{4.4.10}{203}{All-or-Nothing}{theorem.4.4.10}{}}
\tcolorbox@label{57}{204}
\tcolorbox@label{58}{205}
\newlabel{prop:epointcopy}{{4.4.11}{205}{$e$ of any point is $e$-copiable}{theorem.4.4.11}{}}
\tcolorbox@label{59}{206}
\newlabel{prop:copiablebasis}{{4.4.12}{206}{The unit is the union of all $e$-copiables}{theorem.4.4.12}{}}
\tcolorbox@label{60}{207}
\newlabel{prop:decompidem}{{4.4.13}{207}{$e$-copiable decomposition of $e$}{theorem.4.4.13}{}}
\tcolorbox@label{61}{208}
\newlabel{prop:decompcounit}{{4.4.14}{208}{$e$-copiable decomposition of counit}{theorem.4.4.14}{}}
\tcolorbox@label{62}{209}
\newlabel{lem:match}{{4.4.15}{209}{$e$-copiables are orthogonal under multiplication}{theorem.4.4.15}{}}
\tcolorbox@label{63}{210}
\tcolorbox@label{64}{211}
\newlabel{lem:comatch}{{4.4.17}{211}{Co-match}{theorem.4.4.17}{}}
\tcolorbox@label{65}{211}
\newlabel{lem:ecopyfixpoint}{{4.4.18}{211}{e-copiables are e-fixpoints}{theorem.4.4.18}{}}
\tcolorbox@label{66}{212}
\newlabel{lem:ecopynormal}{{4.4.19}{212}{$e$-copiables are normal}{theorem.4.4.19}{}}
\tcolorbox@label{67}{212}
\newlabel{prop:decompmult}{{4.4.20}{212}{$e$-copiable decomposition of multiplication}{theorem.4.4.20}{}}
\tcolorbox@label{68}{213}
\newlabel{prop:decompcomult}{{4.4.21}{213}{$e$-copiable decomposition of comultiplication}{theorem.4.4.21}{}}
\tcolorbox@label{69}{214}
\tcolorbox@label{70}{215}
\tcolorbox@label{71}{216}
\tcolorbox@label{72}{216}
\newlabel{prop:core-core-exclusion}{{4.4.23}{216}{Core exclusion: Distinct cores cannot overlap}{theorem.4.4.23}{}}
\tcolorbox@label{73}{216}
\newlabel{prop:core-halo-exclusion}{{4.4.24}{216}{Core-halo exclusion: Each core only overlaps with its corresponding halo}{theorem.4.4.24}{}}
\tcolorbox@label{74}{217}
\tcolorbox@label{75}{218}
\tcolorbox@label{76}{219}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Sketches in iconic semantics}{221}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:sketches}{{5}{221}{Sketches in iconic semantics}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Preliminary concepts for the sketches}{222}{section.5.1}\protected@file@percent }
\newlabel{sec:prelims}{{5.1}{222}{Preliminary concepts for the sketches}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Open sets: concepts}{222}{subsection.5.1.1}\protected@file@percent }
\newpmemlabel{^_168}{222}
\newpmemlabel{^_169}{222}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Points in space are a useful mathematical fiction. Suppose we have a point on a unit interval. Consider how we might tell someone else about where this point is. We could point at it with a pudgy appendage, or the tip of a pencil, or give some finite decimal approximation. But in each case we are only speaking of a vicinity, a neighbourhood, an \emph  {open set in the borel basis of the reals} that contains the point. Identifying a true point on a real line requires an infinite intersection of open balls of decreasing radius; an infinite process of pointing again and again, which nobody has the time to do. In the same way, most language outside of mathematics is only capable of offering successively finer, finite approximations.}}{222}{figure.caption.161}\protected@file@percent }
\newlabel{fig:pointing}{{5.1}{222}{\@tufte@stored@shortcaption }{figure.caption.161}{}}
\tcolorbox@label{77}{223}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Using sticky spiders as location-tests}{223}{subsection.5.1.2}\protected@file@percent }
\newlabel{ex:chessboard}{{5.1.1}{223}{Where is a piece on a chessboard?}{theorem.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Copy: stative verbs and adjectives}{224}{subsection.5.1.3}\protected@file@percent }
\citation{wierzbickaSemanticsPrimesUniversals1996}
\citation{friedmanFOMCharacterizationSimple2005a}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}The unit interval}{225}{subsection.5.1.4}\protected@file@percent }
\newlabel{thm:Friedman}{{5.1.3}{225}{\citep {friedmanFOMCharacterizationSimple2005a}}{theorem.5.1.3}{}}
\newlabel{defn:lessthan}{{5.1.4}{225}{Less than}{theorem.5.1.4}{}}
\newlabel{defn:friedfunct}{{5.1.5}{225}{Friedman's function}{theorem.5.1.5}{}}
\newlabel{defn:bounds}{{5.1.7}{226}{Upper and lower bounds via endocombinators}{theorem.5.1.7}{}}
\newlabel{defn:sup}{{5.1.8}{226}{Suprema}{theorem.5.1.8}{}}
\newlabel{defn:endpoints}{{5.1.9}{227}{Endpoints}{theorem.5.1.9}{}}
\newlabel{def:simpconn}{{5.1.11}{228}{Simple connectivity}{theorem.5.1.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Metric structure}{229}{subsection.5.1.5}\protected@file@percent }
\newlabel{def:addition}{{5.1.12}{229}{Addition}{theorem.5.1.12}{}}
\newlabel{def:metric}{{5.1.13}{229}{Metric}{theorem.5.1.13}{}}
\newlabel{def:openball}{{5.1.14}{230}{Open balls}{theorem.5.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Relational homotopy}{231}{subsection.5.1.6}\protected@file@percent }
\newlabel{defn:homotopy}{{5.1.16}{232}{Relational Homotopy}{theorem.5.1.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.7}Coclosure: adverbs and adpositions}{233}{subsection.5.1.7}\protected@file@percent }
\newpmemlabel{^_170}{233}
\newpmemlabel{^_171}{233}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces  Recall that \textbf  {ContRel} is coclosed (Proposition \ref  {prop:coclosure}), which means that every dynamic verb may be expressed as the composite of a coevaluator and an open set on the space of homotopies. For instance, \texttt  {move} is an intransitive dynamic verb, which corresponds to a concept in the space of all movements. }}{233}{figure.caption.163}\protected@file@percent }
\newpmemlabel{^_172}{233}
\newpmemlabel{^_173}{233}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces  Adverb-boxes may be modelled as static restrictions in movement-space. For instance, \texttt  {straight} may restrict movements to just those that satisfy some notion of path-length minimality: e.g., given a metric in movement-space on path-lengths, we may construct an open ball (Definition \ref  {def:openball}) around the geodesic to model the adverb \texttt  {straight}. }}{233}{figure.caption.164}\protected@file@percent }
\newpmemlabel{^_174}{233}
\newpmemlabel{^_175}{233}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces  Similarly, adposition-boxes may be modelled as static restrictions on the product of the spaces of nouns and verbs. For instance, \texttt  {towards} may be modelled as an open set that pairs potential positions of the thing-being-moved-towards with movements in movement-space that indeed move towards the target. }}{233}{figure.caption.165}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.8}Nice spiders}{234}{subsection.5.1.8}\protected@file@percent }
\newlabel{defn:contractible}{{5.1.17}{234}{Contractibility}{theorem.5.1.17}{}}
\citation{bohnemeyerTemporalAnaphoraTenseless2009a}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Composition of dynamic verbs via temporal anaphora}{235}{section.5.2}\protected@file@percent }
\citation{jeffreyPremonoidalCategoriesFlow1998,romanStringDiagramsPremonoidal2024}
\citation{nativlangMayaMayaHow2019}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Iconic semantics for modal verbs}{240}{section.5.3}\protected@file@percent }
\newpmemlabel{^_176}{240}
\newpmemlabel{^_177}{240}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Two examples by Mordillo, an artist I liked as a child: a thought bubble representing a woman, where the context of a stranded man implies a want for companionship, and a thought bubble representing a chair, where the context of a climber on a tall summit implies a want for rest.}}{240}{figure.caption.166}\protected@file@percent }
\newlabel{fig:mordillo}{{5.5}{240}{\@tufte@stored@shortcaption }{figure.caption.166}{}}
\newpmemlabel{^_178}{240}
\newpmemlabel{^_180}{240}
\newpmemlabel{^_179}{241}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces On the left, a scene from the Simpsons showing the contents of Homer's mental-theatre. On the right, a depiction of two separate mental-theatres with a fisheye effect, taken from Steven Lahars "A Cartoon Epistemology" freely available online, which was also the initial inspiration for this sketch.}}{241}{figure.caption.167}\protected@file@percent }
\newlabel{fig:thinking}{{5.6}{241}{\@tufte@stored@shortcaption }{figure.caption.167}{}}
\newpmemlabel{^_181}{241}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces So the basic idea is to put representations of worlds inside bounded regions as containers, and in this way iconic semantics provides a univocal setting that displays all of the relevant worlds at once. We are free to pick visual conventions, as they are no more or less arbitrary than the assignment of indices and symbols such as $\mathfrak  {W}_{\texttt  {A}}$ to the contents of possible worlds. Here is a sketch convention for containers on an iconic representation of a person for different modal verbs: seeing, thinking, feeling, owning, and wanting. I sent this excitedly with little supporting context to Bob while I was writing my thesis. He was concerned. Then I got concerned. Childlike became creepy, and neither are good looks. I think I have supplied enough context to make this sensible, but there's no way I'm going to beat the crazy allegations.}}{241}{figure.caption.168}\protected@file@percent }
\newlabel{fig:creepy}{{5.7}{241}{\@tufte@stored@shortcaption }{figure.caption.168}{}}
\newpmemlabel{^_182}{242}
\newpmemlabel{^_183}{242}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The inside and the outside of a container with a solid blob inside are both homotopic to the space with a puncture. This is only approximately a continuous bijection because the unbounded outside space can only map to the open interior of the container. We can use such bijections as a bridge to establish connections between elements of different possible worlds.}}{242}{figure.caption.169}\protected@file@percent }
\newlabel{fig:creepy}{{5.8}{242}{\@tufte@stored@shortcaption }{figure.caption.169}{}}
\newpmemlabel{^_184}{242}
\newpmemlabel{^_185}{242}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Again from Cartoon Epistemology, on the unsatisfactory nature of infinitely-nested containers: \emph  {But who is the viewer of this internal theatre of the mind? For whose benefit is this internal performance produced? Is it the little man at the center who sees this scene? But then how does HE see? Is there yet another smaller man inside that little man's head, and so on to an infinite regress of observers within observers?}}}{242}{figure.caption.170}\protected@file@percent }
\newlabel{fig:infinity}{{5.9}{242}{\@tufte@stored@shortcaption }{figure.caption.170}{}}
\newpmemlabel{^_186}{243}
\newpmemlabel{^_187}{243}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Escher's "Print Gallery" lithograph alongside his working sketch of the vortex-grid geometry the work was built on. On the left of the lithograph, an observer examines a framed painting of a town. Going clockwise, we see more details of the town, which has in it a print gallery, within which is the original observer. The missing centre of the piece where Escher signed the work obscures what would have been infinite nesting; the right-hand-side of the frame would have spiraled along the vortex infinitely. Treating the frame as a container, here we have an example of a container that contains itself, where movement clockwise indicates going down a level, clockwise going up, yet no explicit infinities anywhere.}}{243}{figure.caption.171}\protected@file@percent }
\newlabel{fig:gallery}{{5.10}{243}{\@tufte@stored@shortcaption }{figure.caption.171}{}}
\citation{pavlovicProgramsDiagramsCategorical2023}
\citation{pavlovicMonoidalComputerBasic2012b,pavlovicMonoidalComputerII2014a,pavlovicMonoidalComputerIII2018}
\citation{cockettIntroductionTuringCategories2008}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Iconic semantics for general anaphora via Turing objects}{244}{section.5.4}\protected@file@percent }
\newlabel{sec:turing}{{5.4}{244}{Iconic semantics for general anaphora via Turing objects}{section.5.4}{}}
\citation{pavlovicProgramsDiagramsCategorical2023}
\newlabel{defn:turing}{{5.4.2}{245}{Turing object}{theorem.5.4.2}{}}
\newlabel{cons:unitencoding}{{5.4.8}{250}{Representing relations between sets and their composition within $\squarehvfill $}{theorem.5.4.8}{}}
\citation{coeckePicturingQuantumProcesses2017a,wang-mascianicaTalkingSpaceInference2021a}
\citation{coeckeCompositionalityWeSee2021}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Configuration spaces}{252}{section.5.5}\protected@file@percent }
\newlabel{sec:config}{{5.5}{252}{Configuration spaces}{section.5.5}{}}
\newlabel{defn:configurationspace}{{5.5.2}{252}{}{theorem.5.5.2}{}}
\citation{SpintronicsBuildMechanical}
\citation{richardridelMechanicalTuringMachine2015}
\citation{wikipediaauthorsJaquetDrozAutomata2022}
\citation{reddyConduitMetaphorCase}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Formal models of figurative language}{257}{section.5.6}\protected@file@percent }
\newlabel{sec:metaphor}{{5.6}{257}{Formal models of figurative language}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Temperature and colour: the Planckian Locus}{258}{subsection.5.6.1}\protected@file@percent }
\newlabel{ex:planck1}{{5.6.1}{258}{The Physicists' Planckian Locus}{theorem.5.6.1}{}}
\newpmemlabel{^_188}{258}
\newpmemlabel{^_189}{258}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The Planckian Locus in the CIE 1931 chromaticity diagram. Chromaticity refers only to the hue of a colour, without other domains such as saturation.}}{258}{figure.caption.172}\protected@file@percent }
\citation{lakoffMetaphorsWeLive2003a}
\newpmemlabel{^_190}{259}
\newpmemlabel{^_192}{259}
\newpmemlabel{^_191}{259}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Consider the unit square (depicted as a strip) as a fiber bundle over the unit interval representing temperature range. There is an injective continuous map from the strip into colourspace that is centered on the Planck Locus.}}{259}{figure.caption.173}\protected@file@percent }
\newpmemlabel{^_193}{259}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces The left leg is bijective in the image restriction, so any point or displacement in the offset-strip in colourspace can be lifted to a point in the apex strip, which is then projected down along with other points in the vertical fiber to a point in temperaturespace. So we have a decategorified cofunctor!}}{259}{figure.caption.174}\protected@file@percent }
\newpmemlabel{^_194}{260}
\newpmemlabel{^_196}{260}
\newpmemlabel{^_195}{260}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Starting from the right, the lifting property of the right leg is what lets us map "hotter and colder" temperature talk into the more abstract quantity-talk of "more and less" in the apex strip. Then the left functor sends quantity-talk into the colour domain, which allows "hotter and colder" to be used in the colour domain.}}{260}{figure.caption.175}\protected@file@percent }
\newpmemlabel{^_197}{260}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces The additional expressive power that the apex strip gives is the concept of vertical offset, which doesn't appear in the real line. So the apex strip allows talk of quantity and offset, and this offset, when translated into the colour domain, allows talk of offset towards and away from, for instance, green.}}{260}{figure.caption.176}\protected@file@percent }
\citation{lakoffMetaphorsWeLive2003a}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Time and Money: complex conceptual structure}{261}{subsection.5.6.2}\protected@file@percent }
\newpmemlabel{^_198}{262}
\newpmemlabel{^_200}{262}
\newpmemlabel{^_202}{262}
\newpmemlabel{^_204}{262}
\newpmemlabel{^_206}{262}
\newpmemlabel{^_208}{262}
\newpmemlabel{^_210}{262}
\newpmemlabel{^_199}{262}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces In the \texttt  {TRADE} signature, we define two roles as wires: \texttt  {TRADERS} and \texttt  {TRADEABLES}. There is one static relation \texttt  {HAS} to indicate a trader's ownership of a tradeable, which can be further elaborated with equations to indicate e.g. exclusivity of ownership by interpreting violations of exclusivity as a zero morphism, assumed but elided for brevity. There is one dynamic verb (treated as a homotopy) \texttt  {TRADE}, which at time 0 enforces a precondition that the traders have their respective tradables, and at time 1 (completion of the trade), the traders swap possession of their tradeables. The \texttt  {TRADE} signature contains all nominal instantiations of nouns with respect to roles, which will be illustrated shortly.}}{262}{figure.caption.177}\protected@file@percent }
\newlabel{fig:tradesig}{{5.16}{262}{\@tufte@stored@shortcaption }{figure.caption.177}{}}
\newpmemlabel{^_201}{263}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces We build the topological model from two sticky spiders in the Euclidean plane. The \texttt  {TRADER} spider will distinguish two regions of possession, so that \texttt  {HAS} may be interpreted as a region-test. The \texttt  {TRADEABLES} spider will specify four meeples or counters, three for time, and one for thesiswriting; we will use the configuration space of the \texttt  {TRADEABLES} spider to regulate their movement and distribution. Next, we have to specify what the discrete opfibration is doing. Recalling our functor box notation, we can consider the job of the discrete opfibration to be role assignment from the verb \texttt  {SPEND} in the utterance to the verb \texttt  {TRADE} in the conceptual domain.}}{263}{figure.caption.178}\protected@file@percent }
\newlabel{fig:topmodel}{{5.17}{263}{\@tufte@stored@shortcaption }{figure.caption.178}{}}
\newpmemlabel{^_203}{263}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces The opfibration forgets about role-assignments in its domain by sending them to the monoidal unit. The lift of the opfibration is a role-assignment. (Arguably) unambiguously in this example, \texttt  {Vincent} is the spender and the first trader, and \texttt  {A.M} is the cost and the first tradeable. However, there are two options to resolve \texttt  {writing} treated as a noun-phrase in the role of \texttt  {GOOD}. In the first lift, \texttt  {writing} is resolved as the other trader, and the implicit good as \texttt  {thesis}. In the second lift, \texttt  {writing} is the tradeable and something else is the trading counterparty, such as \texttt  {the world}.}}{263}{figure.caption.179}\protected@file@percent }
\newlabel{fig:fibroles}{{5.18}{263}{\@tufte@stored@shortcaption }{figure.caption.179}{}}
\newpmemlabel{^_205}{264}
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces The section of the opfibration over the \texttt  {SPEND} verb is a tabulation of all the ways in which conceptual roles in the \texttt  {TRADING} domain can be assigned. To continue the example, we will assume the first lift in Example \ref  {fig:fibroles} as our interpretation. The identity-on-objects functor part of the cofunctor maps our chosen interpretation into the following diagram in \textbf  {ContRel}. The configuration space of the \texttt  {TRADEABLES} spider is expanded via split idempotent so that all thin wires in the diagram are typed as the Euclidean plane. Recalling Example \ref  {ex:chessboard}, \texttt  {HAS} is interpreted as the intersection of the position of a counter with the possessive region of the respective trader.}}{264}{figure.caption.180}\protected@file@percent }
\newlabel{fig:interpret}{{5.19}{264}{\@tufte@stored@shortcaption }{figure.caption.180}{}}
\newpmemlabel{^_207}{264}
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces We may verify that the equations governing \texttt  {TRADE} cohere with our topological figures. At time 0, before the trade, we can calculate that the permissible figures have \texttt  {Vincent} in possession of \texttt  {A.M} and \texttt  {writing} in possession of \texttt  {thesis}.}}{264}{figure.caption.181}\protected@file@percent }
\newlabel{fig:time0}{{5.20}{264}{\@tufte@stored@shortcaption }{figure.caption.181}{}}
\newpmemlabel{^_209}{264}
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces At time 1, we may calculate that the permissible figures must be such that \texttt  {Vincent} is in possession of \texttt  {thesis} and \texttt  {writing} is in possession of what was previously my morning.}}{264}{figure.caption.182}\protected@file@percent }
\newlabel{fig:time1}{{5.21}{264}{\@tufte@stored@shortcaption }{figure.caption.182}{}}
\newpmemlabel{^_211}{265}
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces In a more detailed conceptual model of \texttt  {TIME is MONEY}, rather than just \texttt  {TRADE}, we might consider income, the spender's agency, and cost. In Euclidean 3-space, we might model income as a clock-gated mechanism that deposits time-tokens serially into Vincent's possession, along with his agency as a gated chute, and the time cost of writing a thesis as a dispenser that requires a certain number of tokens to release a thesis-token into Vincent's possession. In this sketch model, one obtains short films for \texttt  {He used to waste his mornings but now he spends them writing}, or \texttt  {He once spent an evening writing but made no progress}. One can then ascertain certain consequences truth-theoretically; for instance that there is at least one morning that was not spent on writing, or that there is at least one evening spent on writing but not inside the slot that would help a thesis-release mechanism trigger. In every case, cofunctoriality handles bookkeeping for role-interpretation choices and guarantees systematicity of the topological figure according to the signature at the apex model that models the organising concept.}}{265}{figure.caption.183}\protected@file@percent }
\newlabel{fig:fullermodel}{{5.22}{265}{\@tufte@stored@shortcaption }{figure.caption.183}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}A specimen problem sheet from an imaginary future}{266}{section.5.7}\protected@file@percent }
\bibstyle{alpha}
\bibdata{thesis_intro}
\bibcite{aquantumcomputerLudovicoQuanthoven}{{A q}{}{{}}{{}}}
\bibcite{abramskyAbstractScalarsLoops2009a}{{Abr09}{}{{}}{{}}}
\bibcite{anonymousquantinuumresearchersDisCoCircCompositionalDiscource}{{ano}{}{{}}{{}}}
\bibcite{BAbIMetaResearch}{{BAb}{}{{}}{{}}}
\bibcite{baezIntroductionNCategories1997}{{Bae97}{}{{}}{{}}}
\bibcite{barrNOTETHEOREMPUTNAMa}{{Bar}{}{{}}{{}}}
\bibcite{bastianGooglePaLMGiant2022}{{Bas22}{}{{}}{{}}}
\bibcite{bronsteinGeometricDeepLearning2021}{{BBCV21}{}{{}}{{}}}
\bibcite{boltInteractingConceptualSpaces2017b}{{BCG{$^{+}$}17}{}{{}}{{}}}
\bibcite{bonchiDiagrammaticAlgebraFirst2024a}{{BDGHS24}{}{{}}{{}}}
\bibcite{benabouAlgebreElementaireDans1964}{{Ben64}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bibliography}{273}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{benderClimbingNLUMeaning2020}{{BK20}{}{{}}{{}}}
\bibcite{buszkowskiPregroupGrammarsContextfreea}{{BM}{}{{}}{{}}}
\bibcite{baezOpenPetriNets2020}{{BM20}{}{{}}{{}}}
\bibcite{bohnemeyerTemporalAnaphoraTenseless2009a}{{Boh09}{}{{}}{{}}}
\bibcite{bonchiGraphicalAffineAlgebra2019a}{{BPSZ19}{}{{}}{{}}}
\bibcite{boisseauStringDiagrammaticElectrical2022}{{BS22}{}{{}}{{}}}
\bibcite{balkirDistributionalSentenceEntailment2015}{{BSC15}{}{{}}{{}}}
\bibcite{bonchiCategoricalSemanticsSignal2014}{{BSZ14}{}{{}}{{}}}
\bibcite{bonchiInteractingHopfAlgebras2017}{{BSZ17}{}{{}}{{}}}
\bibcite{carnapMeaningNecessityStudy1988}{{Car88}{}{{}}{{}}}
\bibcite{coeckeInteractingQuantumObservables2011}{{CD11}{}{{}}{{}}}
\bibcite{comfortSheetDiagramsBimonoidal2020}{{CDH20}{}{{}}{{}}}
\bibcite{coeckeThreeQubitEntanglement2011}{{CE11}{}{{}}{{}}}
\bibcite{czarneckiBidirectionalTransformationsCrossDiscipline2009}{{CFH{$^{+}$}09}{}{{}}{{}}}
\bibcite{chenXGBoostScalableTree2016a}{{CG16}{}{{}}{{}}}
\bibcite{coeckeQuantumPicturesNew2023}{{CG23}{}{{}}{{}}}
\bibcite{cruttwellCategoricalFoundationsGradientbased2022}{{CGG{$^{+}$}22}{}{{}}{{}}}
\bibcite{cockettIntroductionTuringCategories2008}{{CH08}{}{{}}{{}}}
\bibcite{chapmandavidNebulosityMeaningness2010}{{{Cha}pm}{}{{}}{{}}}
\bibcite{choiCompletelyPositiveLinear1975}{{Cho75}{}{{}}{{}}}
\bibcite{chomskyNewHorizonsStudy2000a}{{Cho00}{}{{}}{{}}}
\bibcite{churchSetPostulatesFoundation1933}{{Chu33}{}{{}}{{}}}
\bibcite{churchPendulumSwungToo2011}{{Chu11}{}{{}}{{}}}
\bibcite{coeckePicturingQuantumProcesses2017a}{{CK17}{}{{}}{{}}}
\bibcite{carboniCartesianBicategoriesII2007}{{CKWW07}{}{{}}{{}}}
\bibcite{clarkeDoubleCategoryLenses2023a}{{Cla23}{}{{}}{{}}}
\bibcite{chowdheryPaLMScalingLanguage2022}{{CND{$^{+}$}22}{}{{}}{{}}}
\bibcite{coeckeLogicEntanglement2004}{{Coe04}{}{{}}{{}}}
\bibcite{coeckeMathematicsTextStructure2020a}{{Coe20}{}{{}}{{}}}
\bibcite{coeckeCompositionalityWeSee2021}{{Coe21}{}{{}}{{}}}
\bibcite{coeckeClassicalQuantumStructuralism2009}{{CPP09}{}{{}}{{}}}
\bibcite{coeckeNewDescriptionOrthogonal2013c}{{CPV13}{}{{}}{{}}}
\bibcite{croftAutonomyFunctionalistLinguistics1995}{{Cro95}{}{{}}{{}}}
\bibcite{coeckeMathematicalFoundationsCompositional2010a}{{CSC10}{}{{}}{{}}}
\bibcite{carboniCartesianBicategories1987b}{{CW87}{}{{}}{{}}}
\bibcite{coeckeGrammarEquations2021a}{{CW21}{}{{}}{{}}}
\bibcite{davidadOpenAgencyArchitecture}{{dav}{}{{}}{{}}}
\bibcite{delpeuchAutonomizationMonoidalCategories2020a}{{Del20}{}{{}}{{}}}
\bibcite{DisplaCyDependencyVisualizer}{{Dis}{}{{}}{{}}}
\bibcite{dziriFaithFateLimits2023}{{DLS{$^{+}$}23}{}{{}}{{}}}
\bibcite{dornAssociativeCategories2023}{{Dor23}{}{{}}{{}}}
\bibcite{dudzikGraphNeuralNetworks2022}{{DV22}{}{{}}{{}}}
\bibcite{dewittZXcalculusIncompleteQuantum2014}{{dZ14}{}{{}}{{}}}
\bibcite{everettDonSleepThere2009}{{Eve09}{}{{}}{{}}}
\bibcite{fillmoreFrameSemanticsText2001}{{FB01}{}{{}}{{}}}
\bibcite{fritzFinettiTheoremCategorical2021}{{FGP21}{}{{}}{{}}}
\bibcite{fitchEvolutionLanguageFaculty2005}{{FHC05}{}{{}}{{}}}
\bibcite{firthStudiesLinguisticAnalysis1957}{{Fir57}{}{{}}{{}}}
\bibcite{floridiFourthRevolutionHow2014}{{Flo14}{}{{}}{{}}}
\bibcite{fodorConnectionismCognitiveArchitecture1988}{{FP88}{}{{}}{{}}}
\bibcite{fregegottlobSelbstConcreteDinge1884}{{{Fre}84}{}{{}}{{}}}
\bibcite{friedmanFOMCharacterizationSimple2005a}{{Fri05}{}{{}}{{}}}
\bibcite{fongHypergraphCategories2018}{{FS18}{}{{}}{{}}}
\bibcite{gardenforsGeometryMeaningSemantics2014}{{G{\"a}r14}{}{{}}{{}}}
\bibcite{gibbonsRelatingAlgebraicCoalgebraic2012}{{Gib12}{}{{}}{{}}}
\bibcite{goodfellowGenerativeAdversarialNetworks2014}{{GPM{$^{+}$}14}{}{{}}{{}}}
\bibcite{hadzihasanovicAlgebraEntanglementGeometry2017a}{{Had17}{}{{}}{{}}}
\bibcite{harrisLinguisticsWars1993}{{Har93}{}{{}}{{}}}
\bibcite{haroldabelsonLecture1AOverview2019}{{har19}{}{{}}{{}}}
\bibcite{hendrycksMeasuringMathematicalProblem2021}{{HBK{$^{+}$}21}{}{{}}{{}}}
\bibcite{hedgesStringDiagramsGame2015}{{Hed15}{}{{}}{{}}}
\bibcite{herculano-houzelRemarkableNotExtraordinary2012}{{{Her}12}{}{{}}{{}}}
\bibcite{heimSemanticsGenerativeGrammar1998b}{{HK98}{}{{}}{{}}}
\bibcite{heidemannZigzagNormalisationAssociative2022}{{HRV22}{}{{}}{{}}}
\bibcite{hochreiterLongShortTermMemory1997}{{HS97}{}{{}}{{}}}
\bibcite{haydonCompositionalDiagrammaticFirstOrder2020d}{{HS20}{}{{}}{{}}}
\bibcite{huExternalTracedMonoidala}{{Hu}{}{{}}{{}}}
\bibcite{heffordCategoriesSemanticConcepts2020b}{{HWW20}{}{{}}{{}}}
\bibcite{InquisitiveSemanticsInquisitive}{{Inq}{}{{}}{{}}}
\bibcite{jamiolkowskiLinearTransformationsWhich1972}{{Jam72}{}{{}}{{}}}
\bibcite{jeanChildsConceptionSpace1967}{{Jea67}{}{{}}{{}}}
\bibcite{jeffreyPremonoidalCategoriesFlow1998}{{Jef98}{}{{}}{{}}}
\bibcite{jacobsCausalInferenceString2019b}{{JKZ19}{}{{}}{{}}}
\bibcite{joshiIntroductionTreeAdjoining1987}{{Jos87}{}{{}}{{}}}
\bibcite{joyalGEOMETRYTENSORCALCULUSa}{{Joy}{}{{}}{{}}}
\bibcite{joyalGeometryTensorCalculus1991c}{{JS91}{}{{}}{{}}}
\bibcite{janssenMontagueSemantics2021a}{{JZ21}{}{{}}{{}}}
\bibcite{kanervaComputingHighDimensionalVectors2019}{{Kan19}{}{{}}{{}}}
\bibcite{kartsaklisLambeq2023}{{kar23}{}{{}}{{}}}
\bibcite{khanWhatAreTokens2023}{{Kha23}{}{{}}{{}}}
\bibcite{khatriAnatomyAttention2024}{{KLLW24}{}{{}}{{}}}
\bibcite{kerkhoffShortIntroductionClones2014a}{{KPS14}{}{{}}{{}}}
\bibcite{kriegeskorteGridCellsConceptual2016}{{KS16}{}{{}}{{}}}
\bibcite{koralusHumansHumansOut2023}{{KW23}{}{{}}{{}}}
\bibcite{lakoffInstrumentalAdverbsConcept1968}{{Lak68}{}{{}}{{}}}
\bibcite{lambekMathematicsSentenceStructure1958}{{Lam58}{}{{}}{{}}}
\bibcite{lambekCalculusSyntacticTypes1961}{{Lam61}{}{{}}{{}}}
\bibcite{laneCategoriesWorkingMathematician2010}{{Lan10}{}{{}}{{}}}
\bibcite{lietardLanguageModelsKnow2021}{{LAS21}{}{{}}{{}}}
\bibcite{lecunDeepLearning2015a}{{LBH15}{}{{}}{{}}}
\bibcite{liuSeeingBelievingBrainInspired2023}{{LGT23}{}{{}}{{}}}
\bibcite{linuspaulingmemoriallectureseriesNeuroscienceLanguageThought2018}{{lin18}{}{{}}{{}}}
\bibcite{liuLanguageCircuits2021a}{{Liu21}{}{{}}{{}}}
\bibcite{lakoffMetaphorsWeLive2003a}{{LJ03}{}{{}}{{}}}
\bibcite{lorenzQNLPPracticeRunning2023}{{LPM{$^{+}$}23}{}{{}}{{}}}
\bibcite{lorenzCausalModelsString2023}{{LT23}{}{{}}{{}}}
\bibcite{maclaneNaturalAssociativityCommutativity1963}{{Mac63}{}{{}}{{}}}
\bibcite{marrArtificialIntelligencePersonal1977a}{{Mar77}{}{{}}{{}}}
\bibcite{marrVisionComputationalInvestigation2010}{{Mar10}{}{{}}{{}}}
\bibcite{melliesFunctorialBoxesString2006b}{{Mel06}{}{{}}{{}}}
\bibcite{merryReasoningGraphs2014a}{{Mer14}{}{{}}{{}}}
\bibcite{marsdenCustomHypergraphCategories2017}{{MG17}{}{{}}{{}}}
\bibcite{meyerModellingLexicalAmbiguity2020}{{ML20}{}{{}}{{}}}
\bibcite{mcshaneLinguisticsAgeAI2021}{{MN21}{}{{}}{{}}}
\bibcite{montagueUniversalGrammar1970a}{{Mon70}{}{{}}{{}}}
\bibcite{montagueProperTreatmentQuantification1973}{{Mon73}{}{{}}{{}}}
\bibcite{moortgatTypelogicalGrammar2014b}{{Moo14}{}{{}}{{}}}
\bibcite{mollicaHumansStoreMegabytes2019}{{MP19}{}{{}}{{}}}
\bibcite{nativlangMayaMayaHow2019}{{{Nat}19}{}{{}}{{}}}
\bibcite{nouwenDynamicSemantics2022}{{NBvV22}{}{{}}{{}}}
\bibcite{narangPathwaysLanguageModel2022}{{NC22}{}{{}}{{}}}
\bibcite{ngCompletenessZWZX2018}{{Ng18}{}{{}}{{}}}
\bibcite{nlabauthorsHomotopyIoNLab}{{{nLa}a}{}{{}}{{}}}
\bibcite{nlabauthorsLocNLab}{{{nLa}b}{}{{}}{{}}}
\bibcite{nlabauthorsPROPNLab}{{{nLa}c}{}{{}}{{}}}
\bibcite{nlabauthorsStabilizationHypothesisNLab}{{{nLa}d}{}{{}}{{}}}
\bibcite{openaiChatGPTOptimizingLanguage2022}{{{Ope}22}{}{{}}{{}}}
\bibcite{paquetteCategoricalQuantumComputation2008}{{Paq08}{}{{}}{{}}}
\bibcite{parteeBriefHistorySyntaxSemantics2014}{{Par14}{}{{}}{{}}}
\bibcite{pavlovicMonoidalComputerBasic2012b}{{Pav12}{}{{}}{{}}}
\bibcite{pavlovicMonoidalComputerII2014a}{{Pav14}{}{{}}{{}}}
\bibcite{pavlovicProgramsDiagramsCategorical2023}{{Pav23}{}{{}}{{}}}
\bibcite{parteeChapterMontagueGrammar1997}{{PH97}{}{{}}{{}}}
\bibcite{portnerFormalSemanticsEssential2008}{{PP08}{}{{}}{{}}}
\bibcite{petersNoteUniversalBase1969}{{PR69}{}{{}}{{}}}
\bibcite{putnamProblemReference1981}{{Put81}{}{{}}{{}}}
\bibcite{poorCompletenessArbitraryFinite2023}{{PWS{$^{+}$}23}{}{{}}{{}}}
\bibcite{pavlovicMonoidalComputerIII2018}{{PY18}{}{{}}{{}}}
\bibcite{quickdavidLogic2015}{{Qui15}{}{{}}{{}}}
\bibcite{reddyConduitMetaphorCase}{{Red}{}{{}}{{}}}
\bibcite{rodatzPatternLanguageMachine2024}{{RFL{$^{+}$}24}{}{{}}{{}}}
\bibcite{richardridelMechanicalTuringMachine2015}{{{Ric}15}{}{{}}{{}}}
\bibcite{RileyGoodsideGoodside2022}{{Ril22}{}{{}}{{}}}
\bibcite{rumelhartLearningInternalRepresentations1987}{{RM87}{}{{}}{{}}}
\bibcite{romanOpenDiagramsCoend2021a}{{Rom21}{}{{}}{{}}}
\bibcite{romanStringDiagramsPremonoidal2024}{{RS24}{}{{}}{{}}}
\bibcite{reutterHighlevelMethodsHomotopy2019c}{{RV19}{}{{}}{{}}}
\bibcite{sadrzadehFrobeniusAnatomyWord2013a}{{SCC13}{}{{}}{{}}}
\bibcite{sadrzadehFrobeniusAnatomyWord2016a}{{SCC16}{}{{}}{{}}}
\bibcite{searleMindsBrainsPrograms1980a}{{Sea80}{}{{}}{{}}}
\bibcite{selingerSurveyGraphicalLanguages2010d}{{Sel10}{}{{}}{{}}}
\bibcite{suitsGrasshopperGamesLife2005}{{SH05}{}{{}}{{}}}
\bibcite{sobocinskiGraphicalLinearAlgebra2015}{{Sob15}{}{{}}{{}}}
\bibcite{sogaardGroundingVectorSpace2023}{{S{\o }g23}{}{{}}{{}}}
\bibcite{SpintronicsBuildMechanical}{{Spi}{}{{}}{{}}}
\bibcite{suttonBitterLesson2019}{{Sut19}{}{{}}{{}}}
\bibcite{szaboCompositionalitySupervenience2000}{{Sza00}{}{{}}{{}}}
\bibcite{teddy[@teddynpc]MadeChatGPTTake2022}{{{ted}22}{}{{}}{{}}}
\bibcite{taorirohanStanfordCRFM2023}{{TGZ{$^{+}$}23}{}{{}}{{}}}
\bibcite{thompsonGPT3IQTesting2022}{{Tho22}{}{{}}{{}}}
\bibcite{tomgoldstein[@tomgoldsteincs]TrainingPaLMTakes2022}{{{Tom}22}{}{{}}{{}}}
\bibcite{urquhartFineArbitraryObjects2020}{{Urq20}{}{{}}{{}}}
\bibcite{vaswaniAttentionAllYou2017}{{VSP{$^{+}$}17}{}{{}}{{}}}
\bibcite{wangGraphicalGrammarGraphical2019}{{Wan19}{}{{}}{{}}}
\bibcite{wang-mascianicaInternalWirings}{{WC}{}{{}}{{}}}
\bibcite{wang-mascianicaTalkingSpaceInference2021a}{{WC21}{}{{}}{{}}}
\bibcite{wilsonStringDiagramsNonstrict2022a}{{WGZ22}{}{{}}{{}}}
\bibcite{wilsonSafariUpdateStructures2021a}{{WHBW21}{}{{}}{{}}}
\bibcite{wierzbickaSemanticsPrimesUniversals1996}{{Wie96}{}{{}}{{}}}
\bibcite{wikipediaauthorsJaquetDrozAutomata2022}{{wik22}{}{{}}{{}}}
\bibcite{wang-mascianicaDistillingTextCircuits2023a}{{WLC23}{}{{}}{{}}}
\bibcite{wolframNewKindScience2002}{{Wol02}{}{{}}{{}}}
\bibcite{weiChainofThoughtPromptingElicits2023}{{WWS{$^{+}$}23}{}{{}}{{}}}
\bibcite{yauColoredOperads2016}{{Yau16}{}{{}}{{}}}
\bibcite{yeungCCGBasedVersionDisCoCat2021b}{{YK21}{}{{}}{{}}}
\bibcite{zamdzhievRewritingContextfreeFamilies2017a}{{Zam17}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\ttl@finishall
\gdef \@abspage@last{286}
