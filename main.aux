\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newpmemlabel{^_1}{1}
\newpmemlabel{^_3}{2}
\newpmemlabel{^_2}{3}
\tcolorbox@label{1}{6}
\citation{wang-mascianicaDistillingTextCircuits2023a}
\tcolorbox@label{2}{7}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Context and synopsis}{9}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nouwenDynamicSemantics2022}
\citation{nouwenDynamicSemantics2022}
\citation{sobocinskiGraphicalLinearAlgebra2015,bonchiInteractingHopfAlgebras2017,bonchiGraphicalAffineAlgebra2019a}
\citation{haydonCompositionalDiagrammaticFirstOrder2020d,bonchiDiagrammaticAlgebraFirst2024a}
\citation{lorenzCausalModelsString2023,jacobsCausalInferenceString2019b}
\citation{bonchiCategoricalSemanticsSignal2014}
\citation{boisseauStringDiagrammaticElectrical2022}
\citation{hedgesStringDiagramsGame2015}
\citation{baezOpenPetriNets2020}
\citation{fritzFinettiTheoremCategorical2021}
\citation{cruttwellCategoricalFoundationsGradientbased2022,khatriAnatomyAttention2024,rodatzPatternLanguageMachine2024}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Let's say that \textbf  \emph  {{the meaning of text is how it updates a model.}} So we start with some model of the way things are, modelled as data on a wire.\relax }}{10}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Text updates that model; like a gate updates the data on a wire.\relax }}{10}{figure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Text is made of sentences; like a circuit is made of gates and wires.\relax }}{10}{figure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Let's say that \textbf  {\emph  {The meaning of a sentence is how it updates the meanings of its parts.}} As a first approximation, let's say that the \emph  {parts} of a sentence are the nouns it contains or refers to. Noun data is carried by wires. Collections of nouns are related by gates, which play the roles of verbs and adjectives.\relax }}{10}{figure.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What this thesis is about}{10}{section.1.1}\protected@file@percent }
\citation{joyalGeometryTensorCalculus1991c,joyalGEOMETRYTENSORCALCULUSa,maclaneNaturalAssociativityCommutativity1963,laneCategoriesWorkingMathematician2010,selingerSurveyGraphicalLanguages2010d}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Gates can be related by higher order gates, which play the roles of adverbs, adpositions, and conjunctions; anything that modifies the data of first order gates like verbs.\relax }}{11}{figure.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces In practice, higher order gates may be implemented as gates that modify parameters of other gates. Grammar, and \emph  {function words} -- words that operate on meanings -- are in principle absorbed by the geometry of the diagram. These diagrams are natural vehicles for \emph  {dynamic semantics} \citep  {nouwenDynamicSemantics2022}, broadly construed, where states are prior contexts and sentences-as-processes update prior contexts.\relax }}{11}{figure.1.6}\protected@file@percent }
\citation{vaswaniAttentionAllYou2017}
\citation{openaiChatGPTOptimizingLanguage2022}
\citation{bastianGooglePaLMGiant2022}
\citation{teddy[@teddynpc]MadeChatGPTTake2022}
\citation{thompsonGPT3IQTesting2022}
\citation{mcshaneLinguisticsAgeAI2021}
\citation{churchPendulumSwungToo2011}
\citation{hendrycksMeasuringMathematicalProblem2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Nouns are represented by wires, each `distinct' noun having its own wire.\relax }}{12}{figure.1.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces We represent adjectives, intransitive verbs, and transitive verbs by gates acting on noun-wires. Since a transitive verb has both a subject and an object noun, that will then be two noun-wires, while adjectives and intransitive verbs only have one.\relax }}{12}{figure.1.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Adverbs, which modify verbs, we represent as boxes with holes in them, with a number of dangling wires in the hole indicating the shape of gate expected, and these should match the input- and output-wires of the box with the whole.\relax }}{12}{figure.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}\textbf  {Question:} What is the practical value of studying language when Large Language Models exist?}{12}{section.1.2}\protected@file@percent }
\citation{floridiFourthRevolutionHow2014}
\citation{suttonBitterLesson2019}
\citation{chomskyNewHorizonsStudy2000a}
\citation{mollicaHumansStoreMegabytes2019}
\citation{herculano-houzelRemarkableNotExtraordinary2012}
\citation{chowdheryPaLMScalingLanguage2022,narangPathwaysLanguageModel2022}
\citation{khanWhatAreTokens2023}
\citation{tomgoldstein[@tomgoldsteincs]TrainingPaLMTakes2022}
\citation{taorirohanStanfordCRFM2023}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Similarly, adpositions also modify verbs, by moreover adding another noun-wire to the right.\relax }}{13}{figure.1.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces For verbs that take sentential complements and conjunctions, we have families of boxes to accommodate input circuits of all sizes. They add another noun-wire to the left of a circuit.\relax }}{13}{figure.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}\textbf  {First Reply:} Interpretability, maybe.}{13}{section.1.3}\protected@file@percent }
\citation{fodorConnectionismCognitiveArchitecture1988}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Conjunctions are boxes that take two circuits which might share labels on some wires.\relax }}{14}{figure.1.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Of course filled up boxes are just gates.\relax }}{14}{figure.1.13}\protected@file@percent }
\citation{fregegottlobSelbstConcreteDinge1884}
\citation{coeckeCompositionalityWeSee2021}
\citation{lecunDeepLearning2015a}
\citation{rumelhartLearningInternalRepresentations1987}
\citation{hochreiterLongShortTermMemory1997}
\citation{vaswaniAttentionAllYou2017}
\citation{bronsteinGeometricDeepLearning2021}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Gates compose sequentially by matching labels on some of their noun-wires and in parallel when they share no noun-wires, to give \underline  {text circuits}.\relax }}{15}{figure.1.14}\protected@file@percent }
\citation{chapmandavidNebulosityMeaningness2010}
\citation{wolframNewKindScience2002}
\citation{marrArtificialIntelligencePersonal1977a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces To summarise: composition by nesting corresponds to grammatical structure within sentences. Sentences correspond to filled gates, boxes with fixed arity correspond to first-order modifiers such as adverbs and adpositions, and boxes with variable arity correspond to sentential-level modifiers such as conjunctions and verbs with sentential complements.\relax }}{16}{figure.1.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}\textbf  {Objection:} You're forgetting the bitter lesson.}{16}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}\textbf  {Objection:} GOFAI? GO-F-yourself!}{16}{subsection.1.3.2}\protected@file@percent }
\citation{sogaardGroundingVectorSpace2023}
\citation{benderClimbingNLUMeaning2020}
\citation{searleMindsBrainsPrograms1980a}
\citation{lietardLanguageModelsKnow2021}
\citation{kriegeskorteGridCellsConceptual2016}
\citation{gardenforsGeometryMeaningSemantics2014}
\citation{davidadOpenAgencyArchitecture}
\citation{weiChainofThoughtPromptingElicits2023}
\citation{koralusHumansHumansOut2023}
\citation{kanervaComputingHighDimensionalVectors2019}
\citation{liuSeeingBelievingBrainInspired2023}
\citation{goodfellowGenerativeAdversarialNetworks2014}
\citation{chenXGBoostScalableTree2016a}
\citation{coeckeMathematicalFoundationsCompositional2010a}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Composition by connecting wires corresponds to identifying coreferences in discourse. We obtain the same circuit for multiple text presentations of the same content, e.g. \texttt  {Sober Alice who sees drunk Bob clumsily dance laughs at him.} yields the same circuit as the text \texttt  {Alice is sober. She sees Bob clumsily dance. Bob is drunk. She laughs at him.}\relax }}{17}{figure.1.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}\textbf  {Objection:} How does any of this improve capabilities?}{17}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}\textbf  {Second Reply:} LLMs don't help us understand language; how might string diagrams help?}{18}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}\textbf  {Objection:} Isn't the better theory the one with better predictions?}{18}{subsection.1.4.1}\protected@file@percent }
\citation{dziriFaithFateLimits2023}
\citation{RileyGoodsideGoodside2022}
\citation{marrVisionComputationalInvestigation2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Why Category Theory?}{20}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}\textbf  {Objection:} Aren't string diagrams just graphs?}{21}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Synopsis of the thesis}{22}{section.1.5}\protected@file@percent }
\citation{maclaneNaturalAssociativityCommutativity1963,benabouAlgebreElementaireDans1964}
\citation{wang-mascianicaTalkingSpaceInference2021a}
\citation{jeanChildsConceptionSpace1967}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Process Theories}{24}{section.1.6}\protected@file@percent }
\newlabel{sec:proctheory}{{1.6}{24}{Process Theories}{section.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}What does it mean to copy and delete?}{27}{subsection.1.6.1}\protected@file@percent }
\newlabel{relcopy}{{1.6.13}{27}{Sets and relations}{theorem.1.6.13}{}}
\citation{coeckePicturingQuantumProcesses2017a}
\newlabel{ex:copyablestate}{{1.6.14}{28}{Not all states are copyable}{theorem.1.6.14}{}}
\newlabel{ft:determinism}{{1.6.15}{28}{}{theorem.1.6.15}{}}
\citation{czarneckiBidirectionalTransformationsCrossDiscipline2009}
\citation{gibbonsRelatingAlgebraicCoalgebraic2012}
\citation{wilsonSafariUpdateStructures2021a,heffordCategoriesSemanticConcepts2020b}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}What is an update?}{29}{subsection.1.6.2}\protected@file@percent }
\newlabel{ss:update}{{1.6.2}{29}{What is an update?}{subsection.1.6.2}{}}
\citation{selingerSurveyGraphicalLanguages2010d,joyalGeometryTensorCalculus1991c,joyalGEOMETRYTENSORCALCULUSa}
\citation{haroldabelsonLecture1AOverview2019}
\citation{paquetteCategoricalQuantumComputation2008}
\citation{nlabauthorsPROPNLab}
\citation{fongHypergraphCategories2018}
\tcolorbox@label{3}{33}
\tcolorbox@label{4}{34}
\citation{moortgatTypelogicalGrammar2014b}
\citation{lambekMathematicsSentenceStructure1958}
\citation{lambekCalculusSyntacticTypes1961}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Previously, on DisCoCat}{35}{section.1.7}\protected@file@percent }
\newlabel{sec:previously}{{1.7}{35}{Previously, on DisCoCat}{section.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Lambek's Linguistics}{35}{subsection.1.7.1}\protected@file@percent }
\newpmemlabel{^_4}{35}
\newpmemlabel{^_5}{35}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces In English, we may consider a noun to have type $n$, and a transitive verb $(n/s)\setminus n$, to yield a well-formedness proof of \texttt  {Bob drinks beer}. The type formation rules for such a grammar are intuitive. Apart from a stock of basic types $\PazoBB  {B}$ that contains special final types to indicate sentences, we have two type formation operators $(- \ / \ =)$ and $(- \ \setminus \ =)$, which along with their elimination rules establish a requirement that grammatical categories require other grammatical categories to their left or right. This is the essence of Lambek's calculi \textbf  {NL} and \textbf  {L}. CCGs keep the same minimal type-formations, but include extra sequent rules such as type-raising and cross-composition.}}{35}{figure.caption.7}\protected@file@percent }
\newpmemlabel{^_6}{35}
\newpmemlabel{^_8}{35}
\newpmemlabel{^_7}{36}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces We can notice an asymmetry in the above formulation when we examine the transitive verb type $(n/s)\setminus n$ again; it asks first for a noun to the right, and then a noun to the left. We could just as well have asked for the nouns in the other order with the typing $(n/s)\setminus n$ and obtained all of the same proofs.}}{36}{figure.caption.8}\protected@file@percent }
\newpmemlabel{^_9}{37}
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces To eliminate this asymmetry, Lambek devised pregroup grammars. Whereas a group is a monoid with inverses up to left- and right-multiplication, a pregroup weakens the requirement for inverses so that all elements have distinct left- and right- inverses, denoted $x^{-1}$ and $^{-1}x$ respectively. Eliminating or introducing inverses is a non-identity relation on elements of the pregroup, so we have axioms of the form e.g. $x \cdot ^{-1}x \ \rightarrow \ 1 \ \rightarrow \ ^{-1}x \cdot x$. In this formulation, denoting the multiplication with a dot, both $(n/s)\setminus n$ and $(n/s)\setminus n$ become $^{-1}n \cdot s \cdot n^{-1}$, which just wants a noun to the left and a noun to the right in whatever order to eliminate the flanking inverses to reveal the embedded sentence type. Now we can obtain the same proof of correctness as a series of algebraic reductions.}}{37}{figure.caption.9}\protected@file@percent }
\citation{coeckeLogicEntanglement2004}
\citation{coeckeLogicEntanglement2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Coecke's Composition}{38}{subsection.1.7.2}\protected@file@percent }
\newpmemlabel{^_10}{38}
\newpmemlabel{^_11}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Meanwhile, an underground grunge vagabond moonlighting as a quantum physicist moonlighting as a computer scientist was causing a shortage of cigars and whiskey in a small English town. He noticed a funny thing about the composition of multiple non-destructive measurements of a quantum system, which was that information could be carried, or flow, between them. So he wrote a paper \citep  {coeckeLogicEntanglement2004}, which contained informal diagrams that looked like this.}}{38}{figure.caption.10}\protected@file@percent }
\newpmemlabel{^_12}{38}
\newpmemlabel{^_13}{39}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces There were two impressive things about these diagrams. First, the effects such as transparencies for text boxes and curved serifs for angled arrows give a modern feel, but they were done manually in MacDraw, the diagrammatic equivalent of sticks and stones. Second, though the diagrams were informal, they provided a way to visualise and reason about entanglement that was impossible by staring at the equivalent matrix formulation of the same composite operator. The most important diagram for our story was this one, which captures the information flow of quantum teleportation.}}{39}{figure.caption.11}\protected@file@percent }
\citation{abramskyAbstractScalarsLoops2009a}
\citation{abramskyAbstractScalarsLoops2009a}
\citation{choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}
\citation{choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}
\citation{coeckeClassicalQuantumStructuralism2009}
\citation{coeckeNewDescriptionOrthogonal2013c}
\citation{coeckeClassicalQuantumStructuralism2009}
\citation{coeckeNewDescriptionOrthogonal2013c}
\citation{coeckeInteractingQuantumObservables2011}
\citation{coeckeThreeQubitEntanglement2011,dewittZXcalculusIncompleteQuantum2014}
\citation{hadzihasanovicAlgebraEntanglementGeometry2017a,ngCompletenessZWZX2018,poorCompletenessArbitraryFinite2023}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\citation{coeckeInteractingQuantumObservables2011}
\citation{coeckeThreeQubitEntanglement2011,dewittZXcalculusIncompleteQuantum2014}
\citation{hadzihasanovicAlgebraEntanglementGeometry2017a,ngCompletenessZWZX2018,poorCompletenessArbitraryFinite2023}
\citation{coeckePicturingQuantumProcesses2017a,coeckeQuantumPicturesNew2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Categorical quantum mechanics}{40}{subsection.1.7.3}\protected@file@percent }
\newpmemlabel{^_14}{40}
\newpmemlabel{^_15}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Category theorists and physicists such as Abramsky and Baez were excited about these diagrams, which looked like string diagrams waiting to be made formal. The graphical cups and caps in the important diagram were determined to correspond to a special form of symmetric monoidal closed category called strong compact closed \citep  {abramskyAbstractScalarsLoops2009a}.}}{40}{figure.caption.12}\protected@file@percent }
\newpmemlabel{^_16}{40}
\newpmemlabel{^_17}{40}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces Diagrammatically, reasoning in a strongly compact closed category amounts to ignoring the usual requirement of processiveness and forgetting the distinction between inputs and outputs, so that "future" outputs could curl back and be "past" inputs. This formulation also gave insight into the structure of quantum mechanics. For example, the process-state duality of strong compact closure manifested as the Choi–Jamiołkowski isomorphism \citep  {choiCompletelyPositiveLinear1975,jamiolkowskiLinearTransformationsWhich1972}.}}{40}{figure.caption.13}\protected@file@percent }
\newpmemlabel{^_18}{40}
\newpmemlabel{^_20}{40}
\newpmemlabel{^_19}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces However, dealing with superpositions necessitated using summation operators within diagrams, which is cumbersome to write especially when dealing with even theoretically simple Bell states. An elegant diagrammatic simplification arose with the observation that special-$\dagger $-frobenius algebras \citep  {coeckeClassicalQuantumStructuralism2009}, or spiders, correspond to choices of orthonormal bases \citep  {coeckeNewDescriptionOrthogonal2013c} in \textbf  {FdHilb}, the ambient setting of finite-dimensional hilbert spaces. Not only did this remove the need for summation operators, it also revealed that strong compact closure was a derived, rather than fundamental structure, since spiders induce compact closed structure.}}{41}{figure.caption.14}\protected@file@percent }
\newpmemlabel{^_21}{41}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces And so the stage was set for a pu