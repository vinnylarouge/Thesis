\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {0}Synopsis}{7}{chapter.0}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{joyal_geometry_1991,joyal_geometry_nodate,maclane_natural_1963,lane_categories_2010,selinger_survey_2010}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Let's say that \textbf  \emph  {{the meaning of text is how it updates a model.}} So we start with some model of the way things are, modelled as data on a wire.}}{8}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Text updates that model; like a gate updates the data on a wire.}}{8}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Text is made of sentences; like a circuit is made of gates and wires.}}{8}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Let's say that \textbf  {\emph  {The meaning of a sentence is how it updates the meanings of its parts.}} As a first approximation, let's say that the \emph  {parts} of a sentence are the nouns it contains or refers to. Noun data is carried by wires. Collections of nouns are related by gates, which play the roles of verbs and adjectives.}}{8}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gates can be related by higher order gates, which play the roles of adverbs, adpositions, and conjunctions; anything that modifies the data of first order gates like verbs.}}{8}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}What this thesis is about}{8}{section.0.1}\protected@file@percent }
\citation{vaswani_attention_2017}
\citation{openai_chatgpt_2022}
\citation{bastian_google_2022}
\citation{teddy_teddynpc_i_2022}
\citation{thompson_gpt-35_2022}
\citation{mcshane_linguistics_2021}
\citation{church_pendulum_2011}
\citation{hendrycks_measuring_2021}
\citation{}
\citation{}
\citation{floridi_fourth_2014}
\citation{sutton_bitter_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces In practice, higher order gates may be implemented as gates that modify parameters of other gates. Parameters are depicted as additional inputs to gates.}}{9}{section.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Grammar, and \emph  {function words} -- words that operate on meanings -- are absorbed by the geometry of the diagram.}}{9}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.2}\textbf  {Question:} What is the practical value of studying language when Large Language Models exist?}{9}{section.0.2}\protected@file@percent }
\citation{chomsky_new_2000}
\citation{mollica_humans_2019}
\citation{herculano-houzel_remarkable_2012}
\citation{chowdhery_palm_2022,narang_pathways_2022}
\citation{khan_what_2023}
\citation{tom_goldstein_tomgoldsteincs_training_2022}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{deleted_user_stack_2018}
\citation{}
\citation{}
\citation{anderson_end_2008}
\citation{pietsch_epistemology_2022,desai_epistemological_2022}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{bender_climbing_2020}
\citation{searle_minds_1980}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\newpmemlabel{^_1}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  A caricature \citep  {deleted_user_stack_2018} that summarises the opposing epistemic stances of the symbolic/connectionist divide at a glance. For those unfamiliar, here is a recap. The symbolic side is synonymous with Good-old-fashioned-artificial-intelligence (GOFAI), research programme from the 70s to create general artificial intelligence. However, programming this explicitly turned out to be very hard because it was tantamount to systematising all of reasoning and knowledge \citep  {}frameproblem, which is why GOFAI is sometimes described as knowledge-based. In the meantime connectionist methods -- today synonymous with Deep Learning \citep  {} -- leapfrogged GOFAI, for reasons explored in more detail in Section \ref  {}. What distinguished connectionism was a reliance on data and compute rather than explicit programming, so it is sometimes described as knowledge-lean. A bullish sentiment arose among connectionists that cranking the handle to increase the size of the computer and the amount of training data would suffice to eventually obtain general artificial intelligence \citep  {anderson_end_2008}. Debates surrounding this position fall generally under the umbrella of the epistemology of Data Science \citep  {pietsch_epistemology_2022,desai_epistemological_2022}. In the case of LLMs specifically, modern debates of the bullish sentiment are developing, often rapidly. For example, in a thorough survey of LLM capabilities, \citep  {} warned against a fallacious conflation of linguistic and cognitive abilities, while observing several failure modes of GPT3 in cognitive domains. By the time the paper was published, those observations no longer held for GPT3's successor, ChatGPT \citep  {}, which patched the failures with the introduction of reinforcement learning. }}{13}{Item.3}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}\textbf  {Question:} How do string diagrams help us understand language better?}{15}{section.0.3}\protected@file@percent }
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Synopsis of the thesis}{18}{section.0.4}\protected@file@percent }
\bibstyle{plain}
\bibdata{thesis_intro}
\bibcite{anderson_end_2008}{{1}{}{{}}{{}}}
\bibcite{bastian_google_2022}{{2}{}{{}}{{}}}
\bibcite{bender_climbing_2020}{{3}{}{{}}{{}}}
\bibcite{chomsky_new_2000}{{4}{}{{}}{{}}}
\bibcite{chowdhery_palm_2022}{{5}{}{{}}{{}}}
\bibcite{church_pendulum_2011}{{6}{}{{}}{{}}}
\bibcite{deleted_user_stack_2018}{{7}{}{{}}{{}}}
\bibcite{desai_epistemological_2022}{{8}{}{{}}{{}}}
\bibcite{floridi_fourth_2014}{{9}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Bibliography}{21}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{hendrycks_measuring_2021}{{10}{}{{}}{{}}}
\bibcite{herculano-houzel_remarkable_2012}{{11}{}{{}}{{}}}
\bibcite{joyal_geometry_1991}{{12}{}{{}}{{}}}
\bibcite{khan_what_2023}{{13}{}{{}}{{}}}
\bibcite{lane_categories_2010}{{14}{}{{}}{{}}}
\bibcite{maclane_natural_1963}{{15}{}{{}}{{}}}
\bibcite{mcshane_linguistics_2021}{{16}{}{{}}{{}}}
\bibcite{mollica_humans_2019}{{17}{}{{}}{{}}}
\bibcite{narang_pathways_2022}{{18}{}{{}}{{}}}
\bibcite{openai_chatgpt_2022}{{19}{}{{}}{{}}}
\bibcite{pietsch_epistemology_2022}{{20}{}{{}}{{}}}
\bibcite{searle_minds_1980}{{21}{}{{}}{{}}}
\bibcite{selinger_survey_2010}{{22}{}{{}}{{}}}
\bibcite{sutton_bitter_2019}{{23}{}{{}}{{}}}
\bibcite{teddy_teddynpc_i_2022}{{24}{}{{}}{{}}}
\bibcite{thompson_gpt-35_2022}{{25}{}{{}}{{}}}
\bibcite{tom_goldstein_tomgoldsteincs_training_2022}{{26}{}{{}}{{}}}
\bibcite{vaswani_attention_2017}{{27}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{25}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:stringdiagrams}{{2}{25}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A Partial History of String Diagrams}{26}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Formal visual representation}{27}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Convergent Evolution}{27}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Formal visual reasoning}{27}{subsection.2.1.3}\protected@file@percent }
\citation{}
\citation{}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Process Theories}{28}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}What does it mean to copy and delete?}{31}{subsection.2.2.1}\protected@file@percent }
\newlabel{relcopy}{{2.2.6}{31}{Sets and relations}{theorem.2.2.6}{}}
\newlabel{cocom}{{2.1}{32}{What does it mean to copy and delete?}{equation.2.2.1}{}}
\newlabel{ex:copyablestate}{{2.2.7}{32}{Not all states are copyable}{theorem.2.2.7}{}}
\newlabel{ft:determinism}{{2.2.8}{32}{}{theorem.2.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}What is an update?}{32}{subsection.2.2.2}\protected@file@percent }
\newlabel{ss:update}{{2.2.2}{32}{What is an update?}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Spatial predicates}{33}{subsection.2.2.3}\protected@file@percent }
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Processes, Sets, Computers}{34}{subsection.2.2.4}\protected@file@percent }
\citation{}
\newlabel{sec:proctheory}{{2.2.4}{35}{Processes, Sets, Computers}{subsection.2.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Defining String Diagrams}{36}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Symmetric Monoidal Categories}{36}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}PROPs}{36}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}1-object 4-categories}{36}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}A brief diagrammatic introduction to Neural Nets}{37}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}A brief history of formal linguistics from the categorial perspective}{40}{section.2.5}\protected@file@percent }
\newlabel{sec:linghist}{{2.5}{40}{A brief history of formal linguistics from the categorial perspective}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Curry-Howard-Lambek}{40}{subsection.2.5.1}\protected@file@percent }
\newpmemlabel{^_2}{40}
\newpmemlabel{^_3}{40}
\newpmemlabel{^_4}{40}
\citation{montague1970universal}
\citation{montague1973proper}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}What did Montague consider grammar to be?}{41}{subsection.2.5.2}\protected@file@percent }
\newlabel{sec:monty}{{2.5.2}{41}{What did Montague consider grammar to be?}{subsection.2.5.2}{}}
\newlabel{algdata}{{2.5.1}{42}{Generating data of an Algebra}{theorem.2.5.1}{}}
\newlabel{ids}{{2.5.2}{42}{Identities}{theorem.2.5.2}{}}
\newlabel{constants}{{2.5.3}{42}{Constants}{theorem.2.5.3}{}}
\newlabel{comp}{{2.5.4}{42}{Composition}{theorem.2.5.4}{}}
\newlabel{polyop}{{2.5.5}{42}{Polynomial Operations}{theorem.2.5.5}{}}
\newlabel{homo}{{2.5.6}{42}{Homomorphism o