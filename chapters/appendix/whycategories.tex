\section{Why Category Theory?}

The modeller mediates the gap between mathematics and reality by a necessarily subjective process.\\

If formal linguistics is a hobby, then the choice of mathematics used is merely a matter of taste, and there is no need for further discussion.\\

If however formal -- explicitly \emph{mathematical} -- linguistics 

(something about "poetic activity")

Our capacity for language is one of the oldest and sophisticated pieces of compositional technology, maybe even the foundation of compositional thought. So, linguists are veteran students of compositionality and modularity. How does syntax compose meaning? How do the constraints and affordances of language interact? Concern number one for the formal study of language is having a metalanguage in which to build models and theories, and here for the moment we find our $\lambda$s and sequents and whatever else.\\

Linguistics embodies a encyclopaedic record of how compositionality works "in the field", just as botanists record flowers, early astronomers the planetary motions, or stamp-collectors stamps. But a disparate collection of observations encoded in different formats does not a theory make; we will inevitably wish to bring it all together. Accordingly, concern number two for the formal study of language is having a metametalanguage with which to relate the various metalanguages. Obviously, the metametalanguage is set theory, which is the gold standard that backs everything else.\\

It may seem tempting at first to rescue the dignity and importance of formal (i.e. mathematical) semantics (in all its forms, present work included) by appeal to some kind of scientific process, but I want to show that this is either scientism, or else aspiring to this stance seriously means at minimum doing formal linguistics with the methods I use.

Formal theories that aspire to the status of science can be roughly split into ideal-type and empirical-type, depending on their relationship to the empirical phenomena. Ideal-type theories, in the constrained optimisation of generality, precision, and realism [CITE lewin], the former are favoured. This is not altogether a bad strategy: famously in physics, Newton's theory of gravity is an ideal-type theory that won out big empirically later on.

\subsection{Formal linguistics as an idealised science}


\subsection{Formal linguistics as an empirical science}
The appeal purports a standard of searching for explanatory principles for empirical data. The prototype is when a briefly stated principle X within formal framework Y accounts for data so-and-so in the remit of the phenomena Z, hence principle X constitutes an understanding of phenomena Z. I want to grant a narrow conception of "understanding" to precisely that sort of briefly communicable X couched within a Y, and I will limit consideration to good-faith theory, where for instance nobody is touting the simplicity of a principle X while trying to hide the complexity of Y and the well-definedness of Z. Even granting all this, many factors still imperil the inference to "X constitutes an understanding of Z", for example: the complexity of X and Y, the adequacy of Y as a model of Z, and essentially any aspect of the supporting empirical data, such as its origin, quantity, and quality. As a general and easily observable rule, for any given complexity of framework Y, breadth and depth of data of Z is at odds with sharp principles X, and more generally tradeoffs on this pareto front of imperilling factors is decided by extra-rational values and priorities. Whatever that priority in the aggregate is called, it manifests in formal semanticists as a revealed preference for simple principles X and simple building blocks for baroque frameworks Y, which results in a tendency to limit consideration to narrow, often armchair-farmed data of narrowly-defined Z. This situation produces an epistemic hazard characteristic of fields where people who know a little maths think that they can swallow the ocean of the real world: the theory and data originate from the same armchair, so the data is more an appendix of Y than anything to do with Z "out there", and the result is that all talk of principle X and the machinations of Y constitutes only a self-referential understanding which is conflated with knowledge of the world. The way out of course is to go broad and deep and rich with data, and with our current infrastructures for harvesting and processing human judgement data and everything else besides, not to expand in this way is not defensibly a matter of unavailability of access or anything that cannot be overcome. To actually interact via data with the real world, something about X or Y has to give. If we are willing to give up on simple principles X, we lose something human about science, and we also have no need of truth-conditions: of course the truth-conditional approach is adequately expressive for general computation, so it is logically possible to expand Y to eventually account for all the data Z that a typical large language model is trained on, and this procedure would be exactly machine learning, except over a hypothesis space of frameworks Y and done laboriously by hand. Keeping principles X simple and accounting for big data of Z forces more abstract Y, and that is what the use of category theory enables: the notion of formal linguistics in this work is one that is big-data compatible while retaining structural insights. I personally prefer to avoid the issue of scientific or empirical obligations altogether by considering all formal (i.e. mathematical) linguistics (present work included) to be a special-interest hobby.