\begin{fullwidth}
\begin{centering}

\section{What is this thesis about?}
\newthought{This thesis is about a new way to see language using formal diagrams}

\newthought{The diagrams look like circuits.}\\
Let's say that \textbf\emph{{the meaning of text is how it updates a model.}}\\
So we start with some model of the way things are.
\[\tikzfig{intro/model}\]
Text updates that model;
like a gate updates the data on a wire.
\[\tikzfig{intro/model2}\]
Text is made of sentences;
like a circuit is made of gates and wires.
\[\tikzfig{intro/model3}\]
Let's say that \textbf{\emph{The meaning of a sentence is how it updates the meanings of its parts.}}\\
As a first approximation, let's say that the \emph{parts} of a sentence are the nouns it contains or refers to.\\
Noun data is carried by wires.\\
Collections of nouns are related by gates.
\[\tikzfig{intro/model4}\]
Gates can be related by higher order gates.
\[\tikzfig{intro/model5}\]
Higher order gates may be implemented as gates that modify the parameters of other gates.
\[\tikzfig{intro/model6}\]
Every gate corresponds to a \emph{content word} -- a word with a dictionary meaning.
\[placeholder\]
Grammar, and \emph{function words} -- words that operate on meanings -- are absorbed by the geometry of the diagram.
\[placeholder\]

\newthought{Text diagrams are formal compositional blueprints.}\\
These compositional blueprints may be instantiated by classical or quantum computers.\\
We know how grammar composes meanings in language.\\
We can quotient out grammar using these formal diagrams.\\
It turns big black boxes into composites of small black boxes,\\
which leaves smaller pieces for machines to learn representations for.\\
We introduce the mathematics, history, and philosophy of these diagrams in Chapter \ref{chapter:stringdiagrams}.

\section{What is the shape of language?}
\newthought{Text circuits make different ways of thinking about language look the same}\\
Category Theory is good at defining and bookkeeping structure.\\
Chapter \ref{chapter:textcircuits} is about formal correspondences between different shapes of language.\\
We show how text circuits bridge several existing formalisms of language.\\
And we recount a proof of the expressive capacity of text circuits.
\[\scalebox{0.75}{\tikzfig{intro/multiplex}}\]

\newthought{Discourse Representation Theory}\\
Text is composed of sentences as circuits are composed from gates.\\
Text circuits are composed by connecting noun wires.\\
This can happen when two sentences refer to the same noun.\\
\texttt{Alice likes Bob. Bob hates Claire.}\\
\[placeholder\]
Or when a pronoun is used to refer to another noun.
\texttt{Alice likes the flowers that Bob gives Claire.}
\[placeholder\]
So if you know how to resolve pronoun references,\\
and you know how to represent sentences as gates acting on noun wires,\\
then you may represent text as circuits.\\
There are several ways to obtain gates from grammars for sentences.\\

\newthought{Context-free and Tree-adjoining grammars}\\
In a context-free grammar, a sentence symbol $\texttt{S}$ is expanded by replacing individual symbols with strings of symbols to obtain a well-formed sentence.\\
The shape of these expansions is a tree.\\
That tree gives the shape of gates in a sentence.\\
\[\tikzfig{intro/model7}\]
In Chapter \ref{}, we characterise the generative grammar of text circuits in terms of a context-free grammar with additional structure.

\newthought{Typelogical grammars}\\
A typelogical grammar is like the inverse of a context-free grammar.\\
The latter is the grammar of the speaker: a full sentence is produced from $\texttt{S}$.\\
The former is the grammar of the listener: the sentence type $s$ must be derived from the words of the sentence.\\
In a typelogical grammar such as a pregroup grammar, words are assigned types, and a logical proof witnesses the sentence type $s$:
\[
\AxiomC{$\texttt{Alice} : n$}
\AxiomC{$\texttt{secretly} : (^{-1}n \cdot s \cdot n^{-1}) \cdot (^{-1}n \cdot s \cdot n^{-1})^{-1}$}
\AxiomC{$\texttt{likes} : ^{-1}n \cdot s \cdot n^{-1}$}
\BinaryInfC{$\texttt{secretly\textvisiblespace likes} : ^{-1}n \cdot s \cdot n^{-1}$}
\RightLabel{$[x^{-1} \cdot x \rightarrow 1]$}
\BinaryInfC{$\texttt{Alice\textvisiblespace secretly\textvisiblespace likes} : s \cdot n^{-1}$}
\RightLabel{$[x \cdot ^{-1}x \rightarrow 1]$}
\AxiomC{$\texttt{Bob} : n$}
\BinaryInfC{$\texttt{Alice\textvisiblespace secretly\textvisiblespace likes\textvisiblespace Bob} : s$}
\RightLabel{$[x^{-1} \cdot x \rightarrow 1]$}
\DisplayProof
\]
Such proofs of syntactic correctness can be re-expressed as diagrams:
\[\scalebox{0.5}{\tikzfig{intro/pregroup0}}\]
To obtain a circuit, we make the sentence type $s$ dependent on the words that occur in the sentence.\\
To do this, wee refine the structure on each word by introducing \emph{internal wirings} for word-states.\\
Internal wirings make use of multiwires called spiders.\\
\[\scalebox{0.5}{\tikzfig{intro/model9}}\]
A certain choice of spider make these internal wiring diagrams topologically equivalent to the circuits we want.
\[\scalebox{0.4}{\tikzfig{intro/model9b}} \quad\quad\quad\quad \raisebox{3.2cm}{=} \quad\quad\quad\quad \tikzfig{intro/model9c}\]

\newthought{Texts with the same meaning become the same circuit}



\section{What new things can we do?}
In Chapter 

\end{centering}
\end{fullwidth}

\section{Other questions, such as:}
\subsection{Why is this thesis landscape?} Why should it be otherwise? Screens are landscape. My diagrams are needy of space, unlike text. Fat margins for formal and informal asides.

\subsection{What is this thesis not about?}

This thesis is about a particular idealised and computer-friendly conception of natural language syntax and semantics, stated using the as-of-yet uncommon mathematical dialect of applied category theory, and informed by natural language but bound in intent to empirical capture. This means there is no serious consideration of phonetics, phonology, morphology, pragmatics, and the historical, social and psychological dimensions of language. Text circuits do not provide grammaticality judgements upon one-dimensional strings of language, so in some sense they are not even a theory of syntax. There is, moreover, no correct way to instantiate the abstract gates of a text circuit, so in some sense they are not even a theory of semantics.\footnote{
So how are text circuits about syntax and semantics? Text circuits are a "natural" metasyntax for natural language. We start from two assumptions. First, that linguistic meanings are compositional -- after all, we have finite dictionaries for words but not for sentences, and we can understand infinitely many novel sentences -- and second, that the syntax of a sentence (whatever that might be) directs the composition of the meaning of the words. In Section \ref{sec:monty}, we see that taking syntax to be operadic (tree-shaped) and composition to be via lambda calculus, we arrive at Montague semantics. Text circuits arise by generalising from operadic syntax to more expressive shapes (circuit-shaped), and specialising from Turing-complete lambda calculus to interpretation in symmetric monoidal categories, a constrained family of settings better suited to representing and reasoning about interacting computational or physical processes. The consequences, elaborated in the same section, are that superficial differences in the surface form of text are eliminated: text that mean the same, look and compute the same.
}\\

While the application of this mathematical perspective to the puzzles of natural language is relatively fresh, there are no new mathematical techniques developed here. Nominally the most complicated mathematical gadgetry used will be finitely presented associative n-categories, but they are mostly useful to us as a combinatorial handle on symmetric monoidal categories with regions and rewrites, in the same way one would use a PROP with relations for just symmetric monoidal categories.\\

There will be no code, no demonstrations, nothing practical. I am only concerned with showing that certain things are achievable in principle here, and how they may be achieved. Besides, since this is fundamental research, I can think of no demonstration that can outshine the state-of-the-art.

\subsection{Why should I read this?}
\newthought{I wouldn't want to be a formal linguist today.}
What would linguistics look like if it began today?
Large language models (LLMs) such as GPT and PaLM would appear to us as oracles;
wise, superhumanly capable at language, but inscrutable.
Similar to how most people effortlessly use language without a formal understanding which they can express.
So the fundamental mystery would remain: we would still seek to understand language, because understanding LLMs is completely different from understanding language\footnote{
Suppose you knew the insides of a mechanical calculator by heart.
Does that mean you \emph{understand} arithmetic?
At best, obliquely.
Implementing ideal arithmetic means compromises;
the calculator is full of inessentialities and tricks against the constraints of physics.
You would not know where the tricks begin and the essence ends.
Suppose you knew every line of code and every piece of data used to train an LLM.
How could one delineate what is essential to language, and what is accidental?}.\\

So the presence of LLMs should not totally discourage our endeavour to understand language, but their sheer ability to do impressive things presents strong constraints about the acceptability of theories of language, by the usual standards of veridicality and utility\footnote{The explanatory value of a theory by itself -- the "aha!" -- is comparable to the aesthetic pleasure of art, but we can make up such stories all day. Explanatory value paired with veridicality and utility is the gold standard.}. In constast to the idealised and partial nature of formal theories, the nature of LLMs is that they are trained on empirical data about language that captures the friction of the real world. So, in terms of raw predictive power, we should naturally expect the LLMs to have an advantage over principled theories\footnote{Just as the Ptolemaic geocentric model of the solar system was more empirically precise than the heliocentric Copernican, though the latter was "more correct". This was because Ptolemaic epicycles can overfit to approximate any observed trajectory of planets. It took until Einstein's relativity to explain the perihelion of mercury, which at last aligned theoretical understanding with empirical observation.}. Here is a problem: if the better theory is one that lets you do more things, many theories of language are knocked out of the game by an LLM before they can even get started.\\

There are other, innate, tensions to playing the game of formal linguistics. I struggle to reconcile the empirical and scientific character to the study of language with the lack of usefulness; evidence of the success of the physical sciences are all around us, but where are the practical applications of formal linguistics? I have certainly felt like I was doing a kind of empirical observation by consulting my own acceptability judgements and intuitions when constructing models of English grammar. However, the laws and structure of language are smothered by a nebulous foliage of "unless" and "it depends" and other hedges. Probing for the elusive empirical truth of language by the objective process of asking others for grammaticality judgements helps a little, until you start to agree all the time, at which point there is a suspicion we could be fooling ourselves. To summarise: nobody can be sure whether they are truly isolating the essence of language or bullshitting, and to top it off, nowadays either case is irrelevant in practice!\\

I hope to have convinced you of the need for meaningful standards to pursue formal linguistics by, in these depressing circumstances. I have been tempted to give up and treat the activity of formal linguistics as a form of art; harmless doodling for its own sake. But theorybuilding is distinguishable from storytelling by other standards beyond veridicality and utility. I have tried to hold myself to the following, which I think are reasonable:

\begin{itemize}
\item{\textbf{If you can't beat them, make room for them.} There is value in synthesis. If your theory of language can neither empirically outperform nor work in tandem with neural methods in principle, it is a practical nonstarter.}
\item{\textbf{If you can't beat them, be simpler.} There is value in a theory that provides unified understanding even if it is simplified, practically yet unrealised, or momentarily empirically embarrassed.}
\item{\textbf{If you can't beat them, play a different game.} There is value in breaking new ground and extending our reach. If your theory of language can't make room and isn't simple and also has no force of originality beyond the reach of an LLM, it is boring.}
\item{\textbf{If you can't beat them, smile and look pretty.} There is value in art. But if your theory of language is practically inferior, complicated, does nothing new, and on top of that it's \emph{ugly}? You don't have a linguistic theory, you have a conspiracy theory.}
\end{itemize}

\newthought{If I had to be a formal linguist today, I would use applied category theory.}
Our capacity for language is one of the oldest and sophisticated pieces of compositional technology; maybe even the foundation of compositional thought. So, linguists are veteran students of compositionality and modularity. How does syntax compose meaning? How do the constraints and affordances of language interact? The discipline embodies a encyclopaedic record of how compositionality works "in the field", just as botanists record flowers, early astronomers the planetary motions, or stamp collectors stamps.\\

Here I argue that most formal linguists have a metalanguage problem. Set-theoretical foundations of mathematics are not well suited for complex and interacting moving parts. Let's call such systems \emph{jungle-systems}. To specify a set-theoretic model necessitates providing complete detail of how every part looks on the inside\footnote{This is a foundational, innate problem of set theory. Consider the case of the cartesian product of sets, one of the basic constructions. $A \times B$ is the "set of ordered pairs" $(a,b)$ of elements from the respective sets, but there are many ways of encoding that are equivalent in spirit but not in syntax; a sign that the syntax is a hindrance. What we really want of the product is the property that $(a,b) = (c,d)$ just when $a = c$ and $b = d$. Now here is a small sampling of different ways to encode an ordered pair. Kuratowski's definition is
\[A \times B := \bigg\{ \{\{a\},\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
Which could have just as easily been:
\[A \times B := \bigg\{ \{a,\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
And here is Wiener's definition:
\[A \times B := \bigg\{ \{\{a,\varnothing\},b\} \ | \ a \in A \ , \ b \in B \bigg\}\]} -- because only then can you define how the functions map elements to elements. Abstraction becomes unnecessarily difficult, which becomes a hindrance when working with jungle-systems. Computer scientists study abstractions, and being one, I would say that having good abstractions means easier debugging and interoperability -- the entirety of Chapter \ref{chapter:textcircuits} is an example that shows how coding different systems in the right metalanguage makes it easy to compare, relate, and extend those theories. This jungle-native metalanguage is (Applied) Category Theory. This train of thought is continued in Section \ref{sec:proctheory}.

\subsection{What's wrong with our historical tools?}

Here is a fair challenge from a hypothetical formal semanticist: \emph{"We already have a general-purpose metalanguage solution for composition: the lambda-calculus. What can we do with category theory that we cannot already do?"} Shortest answer: Punchcard machines are turing-complete, so the answer is that there is nothing that a high-level language like python on a modern laptop can compute that a punchcard machine cannot, and we are all free to program how we like. A more constructive and historically-situated answer is in Section \ref{sec:linghist}.

\subsection{What are the novel contributions?}
\begin{itemize}
\item{A brief characterisation of Montague's conception of syntax in modern mathematical terminology, and diagrammatically.}
\item{A theory of text structure compatible with quantum and classical data-driven modelling.}
\item{A structural correspondence between tree-adjoining grammars, typelogical grammars, and discourse representation theories.}
\item{A recapitulation of the characterisation of text circuits by a controlled fragment of English from [longpaper], this time including the formal n-categorical signatures.}
\item{A string-diagrammatic presentation of a category of continuous relations, which serves to enable concrete calculations for the following:
\begin{itemize}
\item{Formal semantics for interacting spatially-embodied agents.}
\item{A method to formally model textual metaphors.}
\item{A linguistic characterisation of o-minimal structures.}
\end{itemize}
}
\end{itemize}