\begin{fullwidth}
\begin{centering}

\section{"What is this thesis about?"}
\newthought{This thesis is about a new way to see the shape of language using formal diagrams}

\newthought{The diagrams look like circuits.}\\
Let's say that \textbf\emph{{the meaning of text is how it updates a model.}}\\
So we start with some model of the way things are.
\[\tikzfig{intro/model}\]
Text updates that model;
like a gate updates the data on a wire.
\[\tikzfig{intro/model2}\]
Text is made of sentences;
like a circuit is made of gates and wires.
\[\tikzfig{intro/model3}\]
Let's say that \textbf{\emph{The meaning of a sentence is how it updates the meanings of its parts.}}\\
As a first approximation, let's say that the \emph{parts} of a sentence are the nouns it contains or refers to.\\
Noun data is carried by wires.\\
Collections of nouns are related by gates.
\[\tikzfig{intro/model4}\]
Gates can be related by higher order gates.
\[\tikzfig{intro/model5}\]
Higher order gates may be implemented as gates that modify the parameters of other gates.
\[\tikzfig{intro/model6}\]
Every gate corresponds to a \emph{content word} -- a word with a dictionary meaning.
\[placeholder\]
Grammar, and \emph{function words} -- words that operate on meanings -- are absorbed by the geometry of the diagram.
\[placeholder\]

\newthought{Text diagrams are formal compositional blueprints.}\\
These compositional blueprints may be instantiated by classical or quantum computers.\\
We know how grammar composes meanings in language.\\
We can quotient out grammar using these formal diagrams.\\
It turns big black boxes into composites of small black boxes,\\
which leaves smaller pieces for machines to learn representations for.\\
We introduce the mathematics, history, and philosophy of these diagrams in Chapter \ref{chapter:stringdiagrams}.\\

\section{"What is the shape of language?"}
\newthought{Text circuits make different ways of thinking about language look the same}\\
Category Theory is good at defining and bookkeeping structure.\\
Chapter \ref{chapter:textcircuits} is about formal correspondences between different shapes of language.\\
We show how text circuits bridge several existing formalisms of language.\\
And we recount a proof of the expressive capacity of text circuits.
\[\scalebox{0.75}{\tikzfig{intro/multiplex}}\]

\newthought{Discourse Representation Theory}\\
Text is composed of sentences as circuits are composed from gates.\\
Text circuits are composed by connecting noun wires.\\
This can happen when two sentences refer to the same noun.\\
\texttt{Alice likes Bob. Bob hates Claire.}\\
\[placeholder\]
Or when a pronoun is used to refer to another noun.
\texttt{Alice likes the flowers that Bob gives Claire.}
\[placeholder\]
So if you know how to resolve pronoun references,\\
and you know how to represent sentences as gates acting on noun wires,\\
then you may represent text as circuits.\\
There are several ways to obtain gates from grammars for sentences.\\

\newthought{Context-free and Tree-adjoining grammars}\\
In a context-free grammar, a sentence symbol $\texttt{S}$ is expanded by replacing individual symbols with strings of symbols to obtain a well-formed sentence.\\
The shape of these expansions is a tree.\\
That tree gives the shape of gates in a sentence.\\
\[\tikzfig{intro/model7}\]
In Chapter \ref{}, we characterise the generative grammar of text circuits in terms of a context-free grammar with additional structure.

\newthought{Typelogical grammars}\\
A typelogical grammar is like the inverse of a context-free grammar.\\
The latter is the grammar of the speaker: a full sentence is produced from $\texttt{S}$.\\
The former is the grammar of the listener: the sentence type $s$ must be derived from the words of the sentence.\\
In a typelogical grammar such as a pregroup grammar, words are assigned types.\\
These types come with reduction rules, that combine the types of multiple words.\\
A sequent calculus proof using the reduction rules witnesses the sentence type $s$:
\[
\AxiomC{$\texttt{Alice} : n$}
\AxiomC{$\texttt{secretly} : (^{-1}n \cdot s \cdot n^{-1}) \cdot (^{-1}n \cdot s \cdot n^{-1})^{-1}$}
\AxiomC{$\texttt{likes} : ^{-1}n \cdot s \cdot n^{-1}$}
\BinaryInfC{$\texttt{secretly\textvisiblespace likes} : ^{-1}n \cdot s \cdot n^{-1}$}
\RightLabel{$[x^{-1} \cdot x \rightarrow 1]$}
\BinaryInfC{$\texttt{Alice\textvisiblespace secretly\textvisiblespace likes} : s \cdot n^{-1}$}
\RightLabel{$[x \cdot ^{-1}x \rightarrow 1]$}
\AxiomC{$\texttt{Bob} : n$}
\BinaryInfC{$\texttt{Alice\textvisiblespace secretly\textvisiblespace likes\textvisiblespace Bob} : s$}
\RightLabel{$[x^{-1} \cdot x \rightarrow 1]$}
\DisplayProof
\]
Such proofs of syntactic correctness can be re-expressed as diagrams:
\[\scalebox{0.5}{\tikzfig{intro/pregroup0}}\]
To obtain a circuit, we make the sentence type $s$ dependent on the words that occur in the sentence.\\
To do this, we refine the structure on each word by introducing \emph{internal wirings} for word-states.\\
Internal wirings make use of multiwires called spiders.\\
\[\scalebox{0.5}{\tikzfig{intro/model9}}\]
A certain choice of spider make these internal wiring diagrams topologically equivalent to the circuits we want.
\[\scalebox{0.4}{\tikzfig{intro/model9b}} \quad\quad\quad\quad \raisebox{3.2cm}{=} \quad\quad\quad\quad \tikzfig{intro/model9c}\]

\newthought{Texts with the same meaning become the same circuit!}

\[placeholder\]

\end{centering}
\end{fullwidth}

\section{"What is this thesis not about?"}
This thesis is about a particular idealised and computer-friendly conception of natural language syntax and semantics, stated using the as-of-yet uncommon mathematical dialect of applied category theory, and informed by natural language but bound in intent to empirical capture. This means there is no serious consideration of phonetics, phonology, morphology, pragmatics, and the historical, social and psychological dimensions of language. Text circuits do not provide grammaticality judgements upon one-dimensional strings of language, so in some sense they are not even a theory of syntax. There is, moreover, no correct way to instantiate the abstract gates of a text circuit, so in some sense they are not even a theory of semantics.\footnote{
So how are text circuits about syntax and semantics? Text circuits are a "natural" metasyntax for natural language. We start from two assumptions. First, that linguistic meanings are compositional -- after all, we have finite dictionaries for words but not for sentences, and we can understand infinitely many novel sentences -- and second, that the syntax of a sentence (whatever that might be) directs the composition of the meaning of the words. In Section \ref{sec:monty}, we see that taking syntax to be operadic (tree-shaped) and composition to be via lambda calculus, we arrive at Montague semantics. Text circuits arise by generalising from operadic syntax to more expressive shapes (circuit-shaped), and specialising from Turing-complete lambda calculus to interpretation in symmetric monoidal categories, a constrained family of settings better suited to representing and reasoning about interacting computational or physical processes. The consequences, elaborated in the same section, are that superficial differences in the surface form of text are eliminated: text that mean the same thing also look and compute the same.
}\\

While the application of this mathematical perspective to the puzzles of natural language is relatively fresh, there are no new mathematical techniques developed here. Nominally the most complicated mathematical gadgetry used will be finitely presented associative n-categories, but they are mostly useful to us as a combinatorial handle on symmetric monoidal categories with regions and rewrites, in the same way one would use a PROP with relations for just symmetric monoidal categories.\\

There will be no code, no demonstrations, nothing practical. I am only concerned with showing that certain things are achievable in principle here, and how they may be achieved. So I am taking refuge under the banner of "fundamental research", and for the usual reason: I can think of no demonstration that can outshine the state-of-the-art.

\section{"I'm a no-nonsense practical person, tell me about real-world impact."}
\newthought{Expressing grammar as composition of processes may yield practical benefits.}\\
Simplified, half of the problem of learning language is learning the meaning of words. The meanings change over time and are  context-dependent, and the words are always increasing in number. Encoding these meanings by hand is a sisyphean task. Data-driven learning methods are a good fit: the patterns to be learnt are complex and nebulous, and there is a lot of data. However, data-driven methods may be weaker at the second half of the problem: learning how the rules of syntax work to compose meanings. It is difficult to do justice to compositionality in any practical setting, and the issue with data-driven architectures is either that we know immediately that they cannot be compositional, or their innards are too large and their workings too opaque to tell with confidence. Unfortunately, the issue of compositionality is a Maginot line erected by classical linguists and philosophers against the growing achievements of data-driven methods. It is a form of alienation we must all become accustomed to: that a lifetime's work is made irrelevant by a pile of linear algebra with an internet connection. Returning to the issue, I hope this framework can be a bridge, a way to split the cake fairly between the two halves of the problem: meanings for the machines, compositionality for the commons. Syntax is still difficult and quite vast, but the rules are finite and relatively static. We can break the black-box by reexpressing syntax as the composition of smaller black-boxes. We all stand to benefit: we may give machines an easier time -- now they only have to learn the meanings of words well -- and we can have confidence that the internal representations of the machine -- their "mind's eye" -- contains something we can probe and understand.

\newthought{Expressing grammar as composition of processes is a step towards better human-machine relations.}\\
There is something fundamentally inhuman and behavioural about treating the production of language as a string of words drawn from a probability distribution. In practice, this is what large language models (LLMs) do. When \emph{you} use language, do you feel like a diceroll? Even if we grant that the latent space of a data-driven architecture is an analog for the space of internal mental states of a human user of language, how can we know whether the spaces are structurally analogous to the extent that human-machine partnership through the interface of natural language is safe? So here again is the possible solution: we can guarantee that the latent-space representation of the machine is built up in the same way we build up a mental representation when we read a book or watch a film. At the end of this chapter, after I have introduced the relevant mathematical formalisms, I'll show how we may build this bridge.

\section{"I know a thing or two about language already, why should I read this?"}
\newthought{I wouldn't want to be a formal linguist today.}
What would linguistics look like if it began today? LLMs would appear to us as oracles; wise, superhumanly capable at language, but inscrutable. Similar to how most people effortlessly use language without a formal understanding which they can express. So the fundamental mystery would remain: we would still seek to understand language, because understanding LLMs is completely different from understanding language\footnote{
Suppose you knew the insides of a mechanical calculator by heart.
Does that mean you \emph{understand} arithmetic?
At best, obliquely.
Implementing ideal arithmetic means compromises;
the calculator is full of inessentialities and tricks against the constraints of physics.
You would not know where the tricks begin and the essence ends.
Suppose you knew every line of code and every piece of data used to train an LLM.
How could one delineate what is essential to language, and what is accidental?}. So the presence of LLMs should not totally discourage our endeavour to understand language, but their sheer ability to do impressive things presents strong constraints about the acceptability of theories of language, by the usual standards of veridicality and utility\footnote{The explanatory value of a theory by itself -- the "aha!" -- is comparable to the aesthetic pleasure of art, but we can make up such stories all day. Explanatory value paired with veridicality and utility is the gold standard.}. In constast to the idealised and partial nature of formal theories, the nature of LLMs is that they are trained on empirical data about language that captures the friction of the real world. So, in terms of raw predictive power, we should naturally expect the LLMs to have an advantage over principled theories\footnote{Just as the Ptolemaic geocentric model of the solar system was more empirically precise than the heliocentric Copernican, though the latter was "more correct". This was because Ptolemaic epicycles can overfit to approximate any observed trajectory of planets. It took until Einstein's relativity to explain the perihelion of mercury, which at last aligned theoretical understanding with empirical observation.}. Here is a problem: if the better theory is one that lets you do more things, many theories of language are knocked out of the game by an LLM before they can even get started.

\newthought{There are other, innate, tensions to playing the game of formal linguistics.}\\
I struggle to reconcile the empirical and scientific character to the study of language with the lack of usefulness; evidence of the success of the physical sciences are all around us, but where are the practical applications of formal linguistics? I have certainly felt like I was doing a kind of empirical observation by consulting my own acceptability judgements and intuitions when constructing models of English grammar. However, the laws and structure of language are smothered by a nebulous foliage of "unless" and "it depends" and other hedges. Probing for the elusive empirical truth of language by the objective process of asking others for grammaticality judgements helps a little, until you start to agree all the time, at which point there is a suspicion we could be fooling ourselves. To summarise: nobody can be sure whether they are truly isolating the essence of language or bullshitting, and to top it off, nowadays either case is irrelevant in practice! I hope to have convinced you of the need for meaningful standards to pursue formal linguistics by, in these depressing circumstances. I have been tempted to give up and treat the activity of formal linguistics as a form of art; harmless doodling for its own sake. But theorybuilding is distinguishable from fiction by other standards beyond veridicality and utility. I have tried to hold myself to the following standards, which I think are reasonable:
\begin{itemize}
\item{\textbf{If you can't beat them, make room for them.} There is value in synthesis. If your theory of language can neither empirically outperform nor work in tandem with neural methods in principle, it is a practical nonstarter.}
\item{\textbf{If you can't beat them, be simpler.} There is value in a theory that provides unified understanding even if it is simplified, practically yet unrealised, or momentarily empirically embarrassed.}
\item{\textbf{If you can't beat them, play a different game.} There is value in breaking new ground and extending our reach. If your theory of language can't make room and isn't simple and also has no force of originality beyond the reach of an LLM, it is boring.}
\item{\textbf{If you can't beat them, smile and look pretty.} There is value in art. But if your theory of language is practically inferior, complicated, does nothing new, and on top of that it's \emph{ugly}? You don't have a linguistic theory, you have a conspiracy theory.}
\end{itemize}

\newthought{I argue that most formal linguists have a metalanguage problem.}\\
Set-theoretical foundations of mathematics are not well suited for complex and interacting moving parts. The chief drawback is that if you want to specify a function, you have to spell out how it behaves on the domain and codomain, which means spelling out what the innards of the domain and codomain are; to specify a set-theoretic model necessitates providing complete detail of how every part looks on the inside\footnote{This is a foundational, innate problem of set theory. Consider the case of the cartesian product of sets, one of the basic constructions. $A \times B$ is the "set of ordered pairs" $(a,b)$ of elements from the respective sets, but there are many ways of encoding that are equivalent in spirit but not in syntax; a sign that the syntax is a hindrance. What we really want of the product is the property that $(a,b) = (c,d)$ just when $a = c$ and $b = d$. Now here is a small sampling of different ways to encode an ordered pair. Kuratowski's definition is
\[A \times B := \bigg\{ \{\{a\},\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
Which could have just as easily been:
\[A \times B := \bigg\{ \{a,\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
And here is Wiener's definition:
\[A \times B := \bigg\{ \{\{a,\varnothing\},b\} \ | \ a \in A \ , \ b \in B \bigg\}\]}. As you may be familiar with, this is a nightmare when dealing with a complex system: you have to specify all the implementation details from start to finish, bottom-up. This leads to at least three problems. First, the sociological problem is that this makes things difficult to understand unless you have invested a lot of time into mathematics. Second, interoperability is tricky. When a programmer wants to use a data structure or algorithm, they do not always write it from scratch or copy code from stackoverflow; they may use a library that provides them structures and methods they can call without worrying about how those structures and methods are implemented all the way down. However, if you building a complex theory by spelling out implementations set-theoretically from the start, incorporating a new module from elsewhere becomes difficult if that module has encoded things in sets differently. A lot of busywork must go into translating foundations of formalisms at an analogous level to machine code, which is time better spent building upwards. A computer scientist might say that some abstraction is needed, and being one, I say so. Third, and related to the second, is that set-theory is not the native language for the vast majority of practical computation. Often in the design of complex theories, we do not care about how precisely representations are implemented, instead we only care about placing constraints or guarantees on the behaviour of interacting interactions -- that is, we care about operational semantics.

\newthought{If I had to be a formal linguist today, I would use applied category theory.}\\
A broad theme of this thesis is to illustrate the economy and reach of applied category theory for dealing with compositional phenomena. Our capacity for language is one of the oldest and sophisticated pieces of compositional technology, maybe even the foundation of compositional thought. So, linguists are veteran students of compositionality and modularity. How does syntax compose meaning? How do the constraints and affordances of language interact? The discipline embodies a encyclopaedic record of how compositionality works "in the field", just as botanists record flowers, early astronomers the planetary motions, or stamp collectors stamps. But a disparate collection of observations does not a theory make; we will inevitably wish to bring it all together, one way or another. The problems I have mentioned above are obstacles, and I hope to show that using applied category theory as a metalanguage may be a solution. We may work with formal diagrams that our visual cortexes have built-in rules to manipulate, and we are free to work at the level of abstraction we choose, so that we may easily incorporate other modules and find implementations in a variety of settings. Later in this chapter, we introduce these concepts again via a worked example of getting and putting values into a database using a family of operations bound by equational relations, and the consequence of this view is that it does not matter whether the operations are realised using pencil and paper, bits, qubits, or anything else; as long as they obey our equational constraints, they will behave precisely as getters and putters for a database entry.

\subsection{"What's wrong with the Montagovian $\lambda$-calculus approach to compositionality?"}\\
Here is a fair challenge from a hypothetical formal semanticist: \emph{"We already have a general-purpose metalanguage solution for composition: the $\lambda$-calculus. What can we do with category theory that we cannot already do?"} Shortest answer: Punchcard machines are turing-complete, so the answer is that there is nothing that a high-level language like python on a modern laptop can compute that a punchcard machine cannot, and we are all free to program how we like. A more constructive and historically-situated answer is in Section \ref{sec:linghist}. The $\lambda$-calculus is about plugging expressions into contexts of other expressions, and the Montagovian approach is the observation that you can match the shape of how you plug things together with the shape of syntax, therefore obtaining semantics composed out of grammar. To be clear, we are staying in the business of information-plumbing, and we are keeping Montague's core insight that there is a homomorphism -- a structure preserving map -- from syntax to semantics. The difference lies in formalising these homomorphisms as functors, which allows us to replace the $\lambda$-calculus with other, more expressive playgrounds on the syntax and semantics ends. This merits an immediate illustration. Consider that language is one-dimensional as an artefact of the spoken word coming before the written. On the other hand, meanings -- whatever they are -- are definitely not shaped like lists of words, they have more structure to them. So the relation between syntax and semantics can be viewed as the solution to the problem of encoding and decoding complex structured meanings as one-dimensional strings. There cannot be a unique solution to the problem of encoding and decoding, or else our internal mental lives would be isomorphic to lists of words. So -- as we can see from all the natural languages in the world -- there are multiple solutions, multiple ways of encoding the same meaning as a string of words and decoding them to get back the same meaning. We find such redundancy even in the same language. For example, if I say that \texttt{Alice likes the flowers that Bob gave Claire.}, I would ideally like a grammatical equation telling me that my utterance means the same thing as something like \texttt{Bob gave Claire flowers. Alice likes those flowers.} But if all meanings are modelled as single-output functions and function-application, both utterances will have the same featureless meaning type -- there is no foothold to write an equation there. So instead perhaps:

\[\llbracket \texttt{that} \rrbracket := \lambda \ \texttt{N}_1 \ \texttt{N}_2 \ \texttt{N}_3 \ \texttt{N}_4 \ \texttt{TV} \ \texttt{DV} \ \ . \ \ \texttt{TV}(\texttt{N}_1,\texttt{N}_2) \wedge \texttt{DV}(\texttt{N}_3,\texttt{N}_4,\texttt{N}_2)\]

A problem persists. What if I continue uttering some story about Alice and Bob and Claire and only much later mention the flowers again? It would be a lot of trouble to devise a system of combinators like the one above that keeps everything organised. The usual solution to this train of thought -- as we will see in Section \ref{}, rediscovered independently in logic, computer science, relativity theory, and quantum mechanics -- is to power up function expressions to give multiple outputs using a system of indices. This way we can use indices to keep track of \texttt{the flowers} and plug them in where they are needed later on in the text. The usual observation that follows after this trick is that sequential composition of two pairs of parallel functions is equivalent to the parallel composition of two pairs of sequentially composed functions, i.e., writing parallel composition as "$\otimes$" and sequential composition as "$;$", we have:

\[(f_i \otimes g_j) ; (^{i}h \otimes ^{j}k) = (f_i ; ^{i}h) \otimes (g_j ; ^{j}k)\]

This rule, like the meanings behind language, does not want to live on a one-dimensional string. When we find the right place for expressions to live, the bureaucracy of syntax melts away, absorbed by the geometry of the medium. The mathematics of sequential and parallel composition wants to live in two dimensions, like this:

\[placeholder\]

Applied category theory for diagrams, put another way, is the mathematics of exploiting this connection between algebra and geometry. We can express syntax and semantics in monoidal categories, and relate them as Montague would by a monoidal functor. The (typed) $\lambda$-calculus corresponds to a specific kind of monoidal category, a cartesian closed one, whereas most practical things live in the more general setting of symmetric monoidal categories. Later in this chapter, I will show you how it is done.

\section{"So what can you do that we couldn't already? What are the novel contributions?"}

\newthought{I don't claim originality for content but...} I also do not care if anything I present is a rediscovery, which is inevitable because breadth is the point; the real trick is that I use the same small handful of formal tools as a unified approach. All I have done is a kind of intellectual arbitrage by applying simple methods from one field in another. Here's a list of what's possibly new:

\begin{itemize}
\item{A brief prehistory of string diagrams.}
\item{A brief characterisation of Montague's conception of syntax in modern mathematical terminology.}
\item{A theory of text structure compatible with quantum and classical data-driven modelling.}
\item{A structural correspondence between tree-adjoining grammars, typelogical grammars, and discourse representation theories.}
\item{A recapitulation of the characterisation of text circuits by a controlled fragment of English from [longpaper], this time including the formal n-categorical signatures.}
\item{An investigation of a category of continuous relations, culminating in a string-diagrammatic characterisation of basic linguistic topological concepts.}
\item{Formal semantics for interacting spatially-embodied agents and higher-order predication.}
\item{A formal correspondence between monoidal computer [] and a generalisation of anaphora beyond nouns and events.}
\item{A method to formally model and compute textual metaphors.}
\item{A linguistic characterisation of o-minimal structures.}
\end{itemize}

To elaborate, in Chapter \ref{chapter:textcircuits}, I will explain a small miracle, that certain generative grammars that produce sentences and typelogical grammars that deduce syntactic correctness -- the grammars of speech and listening -- don't just produce the same set of sentences, they are structurally related to the bone; as they must be, or else we would be unable to transmit meanings by encoding and decoding in language. Further, I show how symmetric monoidal structure is a natural setting to interpret higher-order predicates -- put simply, the manner in which \texttt{quickly} modifies the word \texttt{runs} can be viewed as the action of a process that modifies a parameter of another process. After setting a formal stage to perform calculations with basic linguistic spatial relations in Chapter \ref{chapter:contrel}, In Chapter \ref{chapter:sketches} I will; escape the confines of truth-conditional semantics to give formal semantics to the conduit metaphor specifically and textual metaphor in general, thus making sense of utterances like \texttt{to put an idea into words}; provide an elementary and possible-worlds-free recipe to formalise the intensional semantics of words such as \texttt{want} in terms of containers; provide a toy mathematical setting to generalise anaphora beyond nouns and events to any meaningful component of text; bridge natural language to foundational mathematics by outlining formal relationship between intuitive "tame topologies", computational learning theory, and spatial language via text circuits for o-minimal structures. All this is to suggest that...

\newthought{... this way of reckoning with language puts a unified theory of language within reach.} Maybe. I am young and na\"{i}ve enough at the time of writing to be a little dramatic, a little hopeful for the future. In truth I would be deeply unhappy if any of what I write is taken too seriously and rigidly, without a sense of play; I want to demonstrate how mathematics can be used to create and explore meaning rather than subjugate it. I would be happy and consider this thesis a success if you, reader, simply enjoyed my diagrams for their aesthetic value alone. I would be marginally happier if I manage to touch you in a way that changes how you look at language.

\section{Why is this thesis landscape?} Why should it be otherwise? A lot of screens are landscape. My diagrams are needy of space, and it is nice to be able to read them from left to right like text. The fat margins are good for citations, and asides both formal and informal.
