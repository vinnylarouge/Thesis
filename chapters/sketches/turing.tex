\section{Iconic semantics for general anaphora via Turing objects}

This sketch complements the sketch on modals, as it relies on the same container-trick. I would like to explain here how iconic semantics in \textbf{ContRel} might model untyped-boxes --- these are conjunctions and verbs with sentential complements --- as well as the more general linguistic phenomena of \emph{entification} and \emph{general anaphora} --- where arbitrary discourse elements up to collections of sentences may be packaged up as if they were nouns and referred to. I suggest that the mathematical property of \textbf{ContRel} that enables this is that it contains \textbf{FinRel} equipped with a \emph{Turing object}.

\newthought{Entification is the process of turning words and phrases that aren't nouns into nouns.} We are familiar with morphological operations in English, such as \emph{inflections} that turn the singular \texttt{cat} into the plural \texttt{cats}, by adding a suffix \texttt{-s}. Another morphological operation generally called \emph{derivation} changes the grammatical category of a word: for example, the adjective \texttt{happy} derives the noun \texttt{happiness}. With suffixes such as \texttt{-ness} and \texttt{-ing}, just about any lexical word in English can be turned into a noun, as if lexical words have some semantic content that is independent of the grammatical categories they might wear as a guise. With more complex discourse prefixes such as \texttt{the fact that}, we may also disguise sentences and text as nouns.

\begin{example}{Generalised anaphora as entification.}
\[\texttt{Jono is paid minimum wage. He didn't mind \textcolor{cyan}{it}.}\]
\[\tikzfig{spatialencoding/jono}\]
An example of entification. It may be argued that \textcolor{cyan}{\texttt{it}} refers to \texttt{the fact that Jono was paid minimum wage}. Graphically, we might want to depict the gloss as a circuit with a lasso that gives another noun-wire that encodes the information of the lassoed part of the circuit.
\end{example}

The problem at hand is finding an appropriate mathematical setting to interpret and calculate with such lassos. In principle, any meaningful (possibly composite) part of text can be referred to as if it were a noun. For syntax, this is a boon; having entification around means that there is no need to extend the system to accommodate wires for anything apart from nouns, so long as there is a gadget that can turn anything into a noun and back. For semantics this is a challenge, since this requires noun-wires to "have enough space in them" to accommodate full circuits operating on other noun-wires, which suggests a very structured sort of infinity. Computer science has had a perfectly serviceable model of this kind of noun-wire for a long time. What separates a computer from other kinds of machine is that a computer can do whatever any other kind of machine could do --- modulo church-turing on computability and the domain of data manipulation --- so long as the computer is running the right \emph{program}. Programs are (for our purposes) processes that manipulate variously formatted --- or typed --- data, such as integers, sounds, and images. They can operate in sequence and in parallel, and wires can be swapped over each other, so programs form a process theory, where we can reason about the extensional equivalence of different programs --- whether two programs behave the same with respect to mapping inputs to outputs. What makes computer programs special is that on real computers, they are specified by \emph{code}. Programs that are equivalent in their extensional behavior may have many different implementations in code: for example, there are many sorting algorithms, though all of them map the same inputs to the same outputs. Conversely, every possible program in a process theory of programs must have some implementation as code. Importantly, code is just another format of data. The process-theoretic characterisation of the code-wire in a process-theory of computation is this:
\marginnote{
Another observation we could have made is that since computers really just manipulate code, every data format is a kind of restricted form of the same Turing object $\Xi$, but this turns out to be a mathematical consequence of the above equation (and the presence of a few other operations such as copy and compare that form a variant of frobenius algebra), demonstrated in Pavlovic's forthcoming monoidal computer book \citep{pavlovicProgramsDiagramsCategorical2023}, which is prefigured by a trilogy \citep{pavlovicMonoidalComputerBasic2012b,pavlovicMonoidalComputerII2014a,pavlovicMonoidalComputerIII2018}. I would be remiss to leave out Cockett's work on Turing categories \citep{cockettIntroductionTuringCategories2008}, from which I took the name Turing object. Both approaches to a categorical formulation of computability theory share the common starting ground of a special form of closure (monoidal closure in the case of monoidal computer and exponentiation in Turing categories) where rather than having dependent exponential types $\mathbb{A} \multimap \mathbb{B}$ or $\mathbb{B}^\mathbb{A}$, there is a single "code-object" $\Xi$. They differ in the ambient setting; Pavlovic works in the generic symmetric monoidal category, and Cockett with cartesian restriction categories, which generalise partial functions. I work with Pavlovics' formalism because I prefer string diagrams to commuting diagrams.}
\begin{defn}[Turing object]\label{defn:turing}
A \emph{Turing object} $\Xi$ in a process-theory is equipped with evaluation morphisms $\text{ev}^{A}_{B} : A \otimes \Xi \rightarrow B$ for all pairs of objects $A,B$ such that for all morphisms $f: A \rightarrow B$, there exists a state $\ulcorner \! f \urcorner_{: I \rightarrow \Xi}$ of the Turing object such that partial evaluation with that state is equal to $f$. The diagrammatic convention and visual pun \citep{pavlovicProgramsDiagramsCategorical2023} for such code-states and evaluators is to depict the state-triangle as if it is cut out from the rectangle of the evaluator.
\[
\forall A,B \in Ob(\mathcal{C}) \ \exists \text{ev}^{ A}_{B} \ \forall f \ \exists \ulcorner \! f \urcorner\]
\[
\tikzfig{spatialencoding/eval}
\]
\end{defn}

Any programming language is a model for text circuits, using the code-data format as the noun wire and Turing object. In \textbf{ContRel}, the unit square suffices as a Turing object for finite sets and relations, as we can use the container-trick of modals.

\begin{proposition}[Sticky spiders on the open unit square model \textbf{FinRel} equipped with a Turing object]
Using the open unit square with its usual topology as the Turing object, there is a subcategory of \textbf{ContRel} which behaves as the category of countable sets and relations equipped with a Turing object
\begin{proof}
By Construction \ref{cons:unitencoding}, which we work towards.
\end{proof}
\end{proposition}

\begin{lemma}[$(0,1) \times (0,1)$ splits through any countable set $X$]
For any countable set $X$, the open unit square $\squarehvfill$ has a sticky spider that splits through $X^\star$ --- the discrete topology on $X$.
\begin{proof}
Proof by construction. Assume we work with nice spiders, so we only have to highlight the copiable open sets. Take some circle and place axis-aligned open squares evenly along them, one for each element of $X$. The centres of the open squares lie on the circumference of the circle, and we may shrink each square as needed to fit all of them.
\[\scalebox{1}{\tikzfig{spatialencoding/circencodingconstruct}}\]
\end{proof}
\end{lemma}

\begin{defn}[Morphism of sticky spiders]
A morphism between sticky spiders (here cyan and magenta) is any continuous relation that satisfies the following equation.
\[\scalebox{1}{\tikzfig{spatialencoding/stickymorphismdefn}}\]
\end{defn}

\begin{lemma}[Morphisms of sticky spiders encode relations]
For arbitrary split idempotents through $A^\star$ and $B^\star$, the morphisms between the two resulting sticky spiders are in bijection with relations $R: A \rightarrow B$.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/arbsetclaim}}\]
\begin{proof}
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/arbset}}\]
\end{proof}
\end{lemma}

\begin{construction}[Representing sets in their various guises within $\squarehvfill$]
We can represent the direct sum of two $\squarehvfill$-representations of sets as follows.
\[\scalebox{0.75}{\tikzfig{spatialencoding/directsumconstruct}}\]
The important bit of technology is the homeomorphism that losslessly squishes the open unit square into one half of the unit square. The decompressions are partial continuous functions, with domain restricted to the appropriate half of the unit square.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions}}\]
We express the ability of these relations to encode and decode the unit square in just either half by the following graphical equations.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions2}}\]
\end{construction}
Now, to put the two halves together and to take them apart, we introduce the following two relations. In tandem with the squishing and stretching we have defined, these will behave just as the projections and injections for the direct-sum biproduct in \textbf{Rel}.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/leftrightcompressions3}}\]
The following equation tells us that we can take any two representations in $\squarehvfill$, put them into a single copy of $\squarehvfill$, and take them out again.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions4}}\]

We encode the tensor product $A \otimes B$ of representations by placing copies of $B$ in each of the open boxes of $A$.
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap}}\]
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap2}}\]
\[\scalebox{0.75}{\tikzfig{spatialencoding/tensorconstruct}}\]
The important bit of technology here is a family of homeomorphisms of $\squarehvfill$ parameterised by axis-aligned open boxes, that allow us to squish and stretch spaces. Thus for every representation of a set in $\squarehvfill$ by a sticky-spider, where each element corresponds to an axis-aligned open box, we can associate each element with a squish-stretch homeomorphism via the parameters of the open box, which we notate with a dot above the name of the element.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/compresstogether}}\]

Now we can define the "tensor $X$ on the left" relation $\_ \rightarrow X \otimes \_$ and its corresponding cotensor.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/tensordetensor}}\]
The tensor and cotensor behave as we expect from proof nets for monoidal categories.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/tensordetensor2}}\]
And by construction, the (co)tensors and (co)pluses interact as we expect, and they come with all the natural isomorphisms between representations we expect. For example, below we exhibit an explicit associator natural isomorphism between representations.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor3}}\]

\begin{construction}[Representing relations between sets and their composition within $\squarehvfill$]\label{cons:unitencoding}
With all the above, we can establish a special kind of process-state duality; relations as processes are isomorphic to states of $\squarehvfill$, up to the representation scheme we have chosen. This is part of the condition for Turing objects. What remains to be demonstrated is that the duality coheres with sequential and parallel relational composition.
\end{construction}
\[\scalebox{0.9}{\tikzfig{spatialencoding/relcomp1}}\]
Under this duality, we have continuous relations that perform sequental composition of relations as follows.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/relcomp2}}\]
And similarly, parallel composition. Therefore, we have demonstrated that the unit square behaves as a Turing object for the category of countable sets and relations.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/relcomp3}}\]