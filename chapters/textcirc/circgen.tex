\newpage

\section{A generative grammar for text circuits}

\subsection{A circuit-growing grammar}

\marginnote{
\begin{defn}[Lexicon]\label{defn:lex}
We define a limited lexicon $\mathcal{L}$ to be a tuple of disjoint finite sets $(\mathbf{N}, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_{\texttt{S}}, \mathbf{A}_{\texttt{N}}, \mathbf{A}_{\texttt{V}}, \mathbf{C})$
\end{defn}
}

\marginnote{
Where:
\begin{itemize}
\item $\mathbf{N}$ is a set of \emph{proper nouns}
\item $\mathbf{V}_1$ is a set of \emph{intransitive verbs}
\item $\mathbf{V}_2$ is a set of \emph{transitive verbs}
\item $\mathbf{V}_{\texttt{S}}$ is a set of \emph{sentential-complement verbs}
\item $\mathbf{A}_{\texttt{N}}$ is a set of \emph{adjectives}
\item $\mathbf{A}_{\texttt{V}}$ is a set of \emph{adverbs}
\item $\mathbf{C}$ is a set of \emph{conjunctions}
\end{itemize}
}

\begin{marginfigure}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/howtoread}}
\]
\caption{\textbf{How to read the diagrams in this section:} we will be making heavy use of pink and purple bubbles as frames to construct circuits. We will depict the bubbles horizontally, as we are permitted to by compact closure, or by reading diagrams with slightly skewed axes.}
\end{marginfigure}

There are many different ways to write a weak $n$-categorical signature that generates circuits. Mostly as an illustration of expressivity, I will provide a signature where the terms "surface" and "deep" structure are taken literally as metaphors; the generative grammar will grow a line of words in syntactic order, and like mushrooms on soil, the circuits will behave as the mycelium underneath the words. It won't be the most efficient way to do it in terms of the number of rules to consider, but it will look nice and we'll be able to reason about it easily.\\

\newthought{Simplifications and limitations}: For now we only consider word types as in Definition \ref{defn:lex}, though we will see how to engineer extensions later. We only deal with propositional statements, without determiners, in only one tense, with no morphological agreement between nouns and their verbs and referring pronouns, and we assume that adverbs, adjectives stack indefinitely and without further order requirements: e.g. \texttt{Alice happily secretly finds red big toy shiny car that he gives to Bob} is a sentence we consider grammatical enough. For now, we consider only the case where adjectives and adverbs appear before their respective noun or verb. Note that all of these limitations apart from the limited lexicon can principle be overcome by the techniques we developed in Section \ref{sec:ncat} for restricted tree-adjoining and links. As a historical remark, generative-transformational grammars fell out of favour linguistically due to the problem of overgeneration: the generation of nonsense or unacceptable sentences in actual language use. We're undergenerating and overgenerating at the same time, but we're also not concerned with empirical capture: we only require a concrete mathematical basis to build interesting things on top of. On a related note, there's zero chance that this particular circuit-growing grammar even comes close to how language is actually produced by humans, and I have no idea whether a generalised graph-rewriting approach is cognitively realistic.

\newthought{Mathematical assumptions}: We work in a dimension where wires behave symmetric monoidally by homotopy, and further assume strong compact closure rewrite rules for all wire-types. Our strategy will be to generate "bubbles" for sentences, within which we can grow circuit structure piecemeal. We will only express the rewrite rules; the generators of lower dimension are implicit. We aim to recover the linear ordering of words in text (essential to any syntax) by traversing the top surface of a chain of bubbles representing sentence structure in text -- this order will be invariant up to compact closed isomorphisms. The diagrammatic consequence of these assumptions is that we will be working with a conservative generalisation of graph-rewriting defined by local rewriting rules. The major distinction is that locality can be redefined up to homotopy, which allows locally-defined rules to operate in what would be a nonlocal fashion in terms of graph neighbourhoods, as in Figure \ref{fig:locality}. The minor distinction is that rewrite rules are sensitive to twists in wires and the radial order in which wires emanate from nodes, though it is easy to see how these distinctions can be circumvented by additional by imposing the equivalent of commutativity relations as bidirectional rewrites. It is worth remarking that one can devise weak n-categorical signatures to simulate turing machines, where output strings are e.g. 0-cells on a selected 1-cell, so rewrite systems of the kind we propose here are almost certainly expressively sufficient for anything; the real benefit is the interpretable geometric intuitions of the diagrams.

\begin{figure}[h!]\label{fig:locality}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/locality}}
\]
\caption{In this toy example, obtaining the same rewrite that connects the two yellow nodes with a purple wire using only graph-theoretically-local rewrites could potentially require an infinite family of rules for all possible configurations of pink and cyan nodes that separate the yellow, or would otherwise require disturbing other nodes in the rewrite process. In our setting, strong compact closure homotopies handle navigation between different spatial presentations so that a single rewrite rule suffices: the source and target notated by dotted-black circles. Despite the expressive economy and power of finitely presented signatures, we cannot "computationally cheat" graph isomorphism: formally we must supply the compact-closure homotopies as part of the rewrite, absorbed and hidden here by the $\simeq$ notation.}
\end{figure}

\newthought{The plan}: We start with simple sentences that only contain a single intransitive or transitive verb. Then we consider more general sentences. For these two steps, we characterise the expressive capacity of our rules in terms of a context-sensitive grammar that corresponds to the surface structure of the derivations. Then we introduce text structure as lists of sentences with coreferential structure on nouns, along with a mathematical characterisation of coreferential structure and a completeness result of our rules with respect to them. Then we (re)state and prove the text circuit theorem: that the fragment of language we have built with the syntax surjects onto text circuits. Finally we examine how we may model extensions to the expressive capacity of text circuits by introduction of new rewrite rules.

\clearpage

\subsection{Simple sentences}

\marginnote{
\begin{defn}[CSG for simple sentences]\label{dfn:simpCSG}
We may gauge the expressivity of simple sentences with the following context sensitive grammar.
\end{defn}
}

\marginnote{For verbs, adjectives, and modifiers, depicted unsaturated nouns as dotted and saturated with solid black lines, we have:
\[
\resizebox{\marginparwidth}{!}{\tikzfig{mushroom/simpleCFG}}
\]
}

\marginnote{
Adpositions require several helper-generators; we depict for example the beginning of the sequence of derivations that result from appending adpositions to an intransitive verb (the generators are implicit in the derivations):
\[
\resizebox{0.75\marginparwidth}{!}{\tikzfig{mushroom/simpleADP}}
\]
}

\marginnote{
\begin{proposition}\label{prop:simpsent}
Up to labels, the simple-sentence rules yield the same simple sentences as the CSG for simple sentences.
\begin{proof}
By graphical correspondence; viewing nodes on the pink surface as 1-cells, each rewrite rule yields a 2-cell. For example, for the \textcolor{green}{\texttt{IV}}-intro:
\[
\resizebox{0.5\marginparwidth}{!}{\tikzfig{mushroom/simpcorr}}
\]
\end{proof}
\end{proposition}
}

Simple sentences are sentences that only contain a single intransitive or transitive verb. Simple sentences will contain at least one noun, and may optionally contain adjectives, adverbs, and adpositions. The rules for generating simple sentences are as follows:

\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/simplesentences}}
\]

The $\texttt{N}_\uparrow$-intro rule introduces new unsaturated nouns from the end of a simple sentence. The \textcolor{green}{\texttt{IV}}-intro rule applies when there is precisely one unsaturated noun in the sentence, and the \textcolor{green}{\texttt{TV}}-intro rule applies when there are precisely two. Both verb-introduction rules saturate their respective nouns, which we depict with a black bulb. Adjectives may be introduced immediately preceding saturated nouns, and adverbs may be introduced immediately preceding any kind of verb. The position of adpositions in English is context-sensitive. To capture this, the $\textcolor{blue}{\texttt{ADP}}_{\textcolor{green}{\texttt{V}}}$-tendril rule allows an unsaturated adposition to appear immediately after a verb; a bulb may travel by homotopy to the right, seeking an unsaturated noun. Conversely, the bidirectional $\textcolor{blue}{\texttt{ADP}}_{\texttt{N}}$-tendril rule sends a mycelic tendril to the left, seeking a verb. The two pass-rules allow unsaturated adpositions to swap past saturated nouns and adjectives; note that by construction, neither verbs nor adverbs will appear in a simple sentence to the right of a verb, so unsaturated adpositions will move right until encountering an unsaturated noun. In case it doesn't, the tendril- and pass- rules are bidirectional and reversible.

\clearpage

\subsection{Complex sentences}

Now we consider two refinements; conjunctions, and verbs that take sentential complements. we may have two sentences joined by a conjunction, e.g. \texttt{Alice dances \underline{while} Bob drinks}. We may also have verbs that take a sentential complement rather than a noun phrase, e.g. \texttt{Alice \underline{sees} Bob dance}; these verbs require nouns, which we depict as wires spanning bubbles.

\begin{figure}[h!]\label{fig:sentbestiary}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/Sbestiary}}
\]
\caption{The dotted-blue wires do not contentfully interact with anything else, but this noninteraction disallows overgeneration cases where adpositional phrases might interject between \texttt{SCV} verbs and their sentential complement, e.g. \textcolor{red}{\texttt{Alice sees \underline{at lunch} Bob drink}}. The dotted-blue wires also indicate a diagrammatic strategy for extensions to accommodate noun phrases, to be explored later.}
\end{figure}

\marginnote{
\begin{defn}[Sentence structure]\label{dfn:sentCSG}
A sentence can be:
\begin{itemize}
\item a simple sentence, which...
\item ... may generate unsaturated nouns from the right.
\item a pair of sentences with a conjunction in between.
\item (if there is a single unsaturated noun) a sentence with a sentential-complement verb that scopes over a sentence.
\end{itemize}
As a CSG, these considerations are respectively depicted as:
\end{defn}
\[
\resizebox{\marginparwidth}{!}{\tikzfig{mushroom/complexsentenceCSG}}
\]
}

\marginnote{
\begin{proposition}\label{prop:compsent}
Up to labels, the rules so far yield the same sentences as the combined CSG of Definitions \ref{dfn:simpCSG} and \ref{dfn:sentCSG}.
\begin{proof}
Same correspondence as Proposition \ref{prop:simpsent}, ignoring the dotted-blue guards.
\end{proof}
\end{proposition}
}

\begin{figure}[h!]\label{fig:soberA}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/soberA}}
\]
\caption{
\begin{example}[\texttt{sober} $\alpha$ \texttt{sees drunk} $\beta$ \texttt{clumsily dance.}]
Now we can see our rewrites in action for sentences. As a matter of convention -- reflected in how the various pass- rules do not interact with labels -- we assume that labelling occurs after all of the words are saturated. We have still not introduced rules for labelling nouns: we delay their consideration until we have settled coreferential structure. For now they are labelled informally with greeks.
\end{example}
}
\end{figure}

\begin{figure}[h!]\label{fig:Alaughs}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/Alaughs}}
\]
\caption{
\begin{example}[$\alpha$ \texttt{laughs at} $\beta$]
Adpositions form by first sprouting and connecting tendrils under the surface. Because the tendril- and pass- rules are bidirectional, extraneous tendrils can always be retracted, and failed attempts for verbs to find an adpositional unsaturated noun argument can be undone. Though this seems computationally wasteful, it is commonplace in generative grammars to have the grammar overgenerate and later define the set of sentences by restriction, which is reasonable so long as computing the restriction is not computationally hard. In our case, observe that once a verb has been introduced and its argument nouns have been saturated, only the introduction of adpositions can saturate additionally introduced unsaturated nouns. Therefore we may define the finished sentences of the circuit-growing grammar to be those that e.g. contain no unsaturated nodes on the surface, which is a very plausible linear-time check by traversing the surface.
\end{example}
}
\end{figure}

\clearpage

\subsection{Text structure and noun-coreference}

\begin{figure}[h!]
\centering
\[
\tikzfig{mushroom/sintro}
\]
\caption{Only considering words, text is just a list of sentences. However, for our purposes, text additionally has \emph{coreferential structure}. Ideally, we would like to connect "the same noun" from distinct sentences as we would circuits.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\tikzfig{mushroom/circuitplan}
\]
\caption{We choose the convention of connecting from left-to-right and from bottom-to-top, so that we might read circuits as we would text: the components corresponding to words will be arranged left-to-right and top-to-bottom. Connecting nouns across distinct sentences presents no issue, but a complication arises when connecting nouns within the same sentence as with reflexive pronouns e.g. \texttt{Alice likes herself}.}
\end{figure}

\begin{figure}[h!]\label{fig:reflcomp}
\centering
\[
\tikzfig{mushroom/reflcomplication}
\]
\caption{Reflexive coreference would violate of the processivity condition of string diagrams for symmetric monoidal categories. Not all symmetric monoidal categories possess the appropriate structure to interpret such reflexive pronouns, but there exist interpretative options. From left to right in roughly decreasing stringency, compact closed categories are the most direct solution. More weakly, traced symmetric monoidal categories also suffice. If there are no traces, so long as the noun wire possesses a monoid and comonoid, a convolution works. If all else fails, one can just specify a new gate. We will define coreference structure to exclude such reflexive coreference and revisit the issue as an extension.}
\end{figure}

\newpage

Now we will deal with coreferential structure and noun-labels. \bR TODO: need more linked-intro variants to handle leaving right-side of conjunction, nested SCV, CNJ to SCV, and SCV to CNJ. Consider using the same graywire convention to simplify case analysis? \e
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounbestiary_newnew}}
\]

The linked-\texttt{N}-intro rules introduce a new unsaturated noun in the next sentence that coreferences the noun in the previous sentence that generated it. The \texttt{N}-shift rules allow any unsaturated noun to move into the next sentence. For both of the previous rules, the $\beta$ variant handles the case where the next sentence is related to the first by a conjunction. Observe that nouns with a forward coreference have two dotted-black wires leaving the root of their wires, which distinguishes them from nouns that only have a backward coreference or no coreference at all, which only have a single dotted-black wire leaving the root of their wire.\\

The \texttt{N}-swap rule variants allow a unsaturated noun with no forward coreferences to swap places with any unsaturated noun that immediately succeeds it.\\

\begin{figure}[h!]\label{fig:nounkinds}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounkinds}}
\]
\caption{At this point, it is worth establishing some terminology about the kinds of unsaturated nouns we have in play. The kinds of nouns are distinguished by their tails. \emph{Lonely} nouns have no coreferences, their tails connect to nothing. \emph{Head} nouns have a forward coreference in text; they have two tails, one that connects to nothing and the other to a noun later in text. \emph{Middle} nouns have a forward and backward coreference; they have two tails, one that connects to a noun in some preceding sentence, and one that connects forward to a noun in a succeeding sentence. \emph{Foot} nouns only have a backward coreference; they have a single tail connecting to a noun in some preceding sentence.}
\end{figure}

When the structure of coreferences is set, we propagate noun labels from the head of each list. The rules for noun-label propagation are as follows:

\begin{figure}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounbestiary_newnew2}}
\]
\caption{The $n \in \mathbf{N}$ notation indicates a family of rewrites (and generators) for each noun in the lexicon. Link-label assigns a noun to a diagrammatically linked collection of coreferent nouns, and link-propagation is a case analysis that copies a link label and distributes is across coreferent nouns. Link-rise is a case analysis to connect labels to the surface, and finally \texttt{N}-label allows a saturated noun to inherit the label of its coreference class, which may either be a noun \texttt{n} or a pronoun appropriate for the noun, notated $^\texttt{*}\texttt{n}$}
\end{figure}

\begin{example}[\texttt{sober Alice sees Bob clumsily dance. She laughs at him.}]
\end{example}
\begin{figure}[h!]\label{fig:corefex1}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex1}}
\]
\caption{We start the derivation by setting up the sentence structure using \texttt{S}- and \texttt{SCV}-intro rules, and two instances of \texttt{N}-intro, one for Alice, and one for Bob. Observe how the \texttt{N}-intro for Bob occurs within the subsentence scoped over by the \texttt{SCV}-rule.}
\end{figure}

\begin{figure}[h!]\label{fig:corefex2}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex2}}
\]
\caption{By homotopy, we can rearrange the previous diagram to obtain the source of the linked-\texttt{N}-intro rewrite in the dashed-box visual aid. Observe how we drag in the root of what is to be Alice's wire. Then we use the \texttt{IV}-intro in the second sentence, which sets up the surface structure \texttt{she laughs}, and the deep structure for bookkeeping that \texttt{she} refers to \texttt{Alice}.}
\end{figure}

\begin{figure}[h!]\label{fig:corefex3}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex3}}
\]
\caption{By homotopy again, we can do the same for Bob, this time setting up for the $\gamma$ variant of linked-\texttt{N}-intro which handles the case when the spawning noun is within the scope of an SCV. Then by applying a series of $\texttt{N}_\uparrow$-swaps, the unsaturated noun is placed to the right of the intransitive verb phrase.}
\end{figure}

\begin{figure}[h!]\label{fig:corefex4}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex4}}
\]
\caption{We've already done the surface derivation for the two sentences separately in Figures \ref{fig:soberA} and \ref{fig:Alaughs}; since neither of those derivations touch the roots of noun-wires, we can emulate those derivations and skip ahead to the first diagram. }
\end{figure}

\newpage

\subsection{Text circuit theorem}

\marginnote{
\begin{defn}[Text Circuits]
\emph{Text circuits} are made up of three ingredients:
\begin{itemize}
\item wires
\item boxes, or gates
\item boxes with holes that fit a box, or 2nd order gates
\end{itemize}
\end{defn}
}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/nounwiresABN} 
\]
\caption{Nouns are represented by wires, each `distinct' noun having its own wire.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/ADJgate} \quad\quad\quad \tikzfig{textcirc/IVgate} \quad\quad\quad \tikzfig{textcirc/TVgate}
\]
\caption{We represent adjectives, intransitive verbs, and transitive verbs by gates acting on noun-wires. Since a transitive verb has both a subject and an object noun, that will then be two noun-wires, while adjectives and intransitive verbs only have one.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/ADVbox}
\]
\caption{Adverbs, which modify verbs, we represent as boxes with holes in them, with a number of dangling wires in the hole indicating the shape of gate expected, and these should match the input- and output-wires  of the box with the whole.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/ADPIVbox}
\]  
\caption{Similarly, adpositions also modify verbs, by moreover adding another noun-wire to the right.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/SCVbox}
\]
\caption{For verbs that take sentential complements and conjunctions, we have families of boxes to accommodate input circuits of all sizes. They add another noun-wire to the left of a circuit.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/CNJbox2}
\]
\caption{Conjunctions are boxes that take two circuits which might share labels on some wires.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/ADPIVgate}
\]
\caption{Of course filled up boxes are just gates.}
\end{marginfigure}

\begin{marginfigure}
\centering
\[
\tikzfig{textcirc/gatecompex1}  
\]
\caption{Gates compose sequentially by matching labels on some of their noun-wires and in parallel when they share no noun-wires, to give \underline{text circuits}.}
\end{marginfigure}

Now we demonstrate that \emph{finished} text diagrams yield unique text circuits up to homotopy. Text circuits are presented again in the margins as a reminder. The strategy will be to present new rewrites that transform coreferential structure into symmetric monoidal wire-connectivity.

\begin{defn}[Finished text diagram]
The circuit-growing grammar produces \emph{text diagrams}. We call a text diagram \emph{finished} if all surface nodes are labelled.
\end{defn}

\begin{proposition}
Finished text diagrams yield text, up to interpreting distinct sentences as concatenated with punctuation \texttt{.}, \texttt{,}, contentless conjunctions or complementisers -- such as \texttt{and}, or \texttt{that} respectively.
\begin{proof}
Sentence-wise grammaticality is gauged by Propositions \ref{prop:simpsent} and \ref{prop:compsent}. When multiple sentences occur within the scope of a \texttt{SCV} we might prefer the use of contentless complementisers and conjunctions, e.g. \texttt{Alice sees( Bob draws Charlie drinks ) Dennis dances} is grammatically preferable but meaningfully equivalent to \texttt{Alice sees \underline{that} Bob draws \underline{and} Charlie drinks \underline{, and} Dennis dances \underline{.}} For our purposes it makes no difference whether surface text has these decorations, as the deep structure of text diagrams encodes all the information we care to know.
\end{proof}
\end{proposition}

\begin{lemma}\label{prop:linkedlist}
The unsaturated noun kinds listed in Figure \ref{fig:nounkinds} are exhaustive, hence nouns that share a coreference are organised as a diagrammatic linked-list.
\begin{proof}
The \texttt{N}-intro rule creates lonely nouns. Head nouns can only be created by the linked-\texttt{N}-intro applied to a lonely noun. Any new noun created by linked-\texttt{N}-intro is a foot noun. The linked-\texttt{N}-intro rule turns foot nouns into middle nouns. These two intro- rules are the only ones that introduce unsaturated nouns, so it remains to demonstrate that no other rules can introduce noun-kinds that fall outside our taxonomy. The \texttt{N}-shift rule changes relative position of either a lonely or foot noun but cannot change its kind. The \texttt{N}-swap rule may start with either a lonely or foot noun on the left and either a head or middle noun on the right, but the outcome of the rule cannot change the starting kinds as tail-arity is conserved and the local nature of rewrites cannot affect the ends of tails.
\end{proof}
\end{lemma}

\begin{lemma}\label{prop:norefl}
No nouns within the same sentence are coreferentially linked.
\begin{proof}
Novel linked nouns can only be obtained from the linked-\texttt{N}-intro rule, which places them in succeeding sentences. The swap rules only operate within the same sentence and keep the claim invariant. The \texttt{N}-shift rules only apply to nouns with no forward coreferences; nouns with both forward and backward coreferences cannot leave the sentence they are in. Moreover, \texttt{N}-shift is unidirectional and only allows the rightmost coreference in a linked-list structure to move to later sentences. So there is no danger of an \texttt{N}-shift breaking the invariant.
\end{proof}
\end{lemma}

\newpage

\begin{construction}[Text to circuit]\label{cons:wirejoin}
\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/sentjoin}} 
\]
\caption{We turn finished text diagrams into text circuits by operating \emph{in situ}, with extra rules outside the grammatical system that handle connecting noun wires.
}
\end{figure}
\end{construction}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/sentjoinex}} 
\]
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/sentjoinex2}} 
\]
\caption{
In the first step, by Lemmas \ref{prop:linkedlist} and \ref{prop:norefl}, we can always rearrange a finished text diagram such that the noun wires are processive.\\

In the second step, use the first rewrite of Construction \ref{cons:wirejoin} to prepare the wires for connection.\\

In the third step, we just ignore the existence of the bubble-scaffolding and the loose scalars. We could in principle add more rewrites to melt the scaffolding away if we wanted, but who cares?\\

In the fourth step, we apply the second and third rewrites of Construction \ref{cons:wirejoin} to connect the wires and eliminate nodules underneath labels. We can also straighten up the wires a bit and make them look proper.\\

At this point, we're actually done, because the resulting diagram \emph{is already a text circuit up to a choice of notation}.
}
\end{figure}

\begin{proposition}[Finished text diagrams yield unique-up-to-processive-isotopy text circuits]\label{prop:text2circ}
\begin{proof}
Every sentence corresponds to a gate up to notation, and we have a handle on sentences via Propositions \ref{prop:simpsent} and \ref{prop:compsent}. Lemmas \ref{prop:linkedlist} and \ref{prop:norefl} guarantee processivity. Uniqueness-up-to-processive-isotopy is inherited: text diagrams themselves are already specified up-to-connectivity, which is strictly more general than processive isotopy. Therefore, for any circuit $C$ obtained from a text diagram $T$ by Construction \ref{cons:wirejoin}, $T$ can be modified up to processive-isotopy on noun wires to yield $T'$ and another circuit $C'$ that only differs from $C$ up to processive isotopy, and all $C'$ can be obtained in this way.
\end{proof}
\end{proposition}

The converse of Proposition \ref{prop:text2circ} would be that any text circuit that can be formed by the composition of symmetric monoidal categories and of plugging gates into boxes yields a text diagram. This would mean that text circuit composition is acceptable as a generative grammar for text. Establishing this converse requires elaboration of some conventions.

\marginnote{
\begin{remark}
There are some oddities about our conventions that will make sense later when we consider semantics. For example, Convention \ref{conv:twist} an acceptable thing to ask for syntactically but quite odd to think about at the semantic level, where we would like to think that distinct nouns manifest as different states on the same noun-wire-type. A semantic interpretation that makes use of this convention will become clearer in Section \bR configspace \e. Similarly, Convention \ref{conv:exists} wouldn't be true if we consider the order of text to reflect the chronological ordering of events, in which case there are implicit \texttt{... and then ...} conjunctions that distinguish ordered gates from parallel gates conjoined by an implicit \texttt{... while ...}; this particular complication is handled at the semantic level in Section \bR statesactionmanner \e. The distinction in Convention \ref{conv:gaps} between typed and "untyped" higher-order processes will be given a suitable semantic interpretation in Section \bR lassos \e.
\end{remark}
}

\begin{figure}[h!]
\centering
\[
\tikzfig{textcirc/gatecompextgt}
\]
\caption{
\begin{convention}[Wire twisting]\label{conv:twist}
\end{convention}
Wires are labelled by nouns. We consider two circuits the same if their gate-connectivity is the same. In particular, this means that we can eliminate unnecessary twists in wires to obtain diagrammatically simpler representations.
}
\end{figure}

\begin{figure}[h!]
\centering
\[
\tikzfig{textcirc/gateeqslide} 
\]
\caption{
\begin{convention}[Sliding]\label{conv:sliding}
\end{convention}
Since only gate-connectivity matters, we consider circuits the same if all that differs is the horizontal positioning of gates composed in parallel.
\begin{convention}[Reading text circuits]\label{conv:reading}
\end{convention}
Text circuits ought to be presented so that they can be read from top to bottom and from left to right, like English text.
}
\end{figure}

\begin{convention}[Arbitary vs. fixed holes]\label{conv:gaps}
Diagrammatically, adverbs and adpositions are depicted with no gap between the bounding box and their contents, whereas conjunctions and verbs with sentential complement are depicted with a gap; this is a visual indication that the former are type-sensitive, and the latter can take any circuit.
\end{convention}

\begin{convention}[Contentless conjunctions]\label{conv:and}
Conventions \ref{conv:sliding} and \ref{conv:reading} require something else to allow them to work at the same time. The left text circuit may be read \texttt{C hates D. A likes B.}, and the right as \texttt{A likes B. C hates D.} The middle text circuit can be rewritten as either the left or right, which can be done if we're happy to make such choices. A choiceless approach requires something to distinguish the fact that the gates are parallel: a "contentless" conjunction, such as \texttt{and}, or \texttt{while}. In terms of text diagrams, we want a rewrites that introduces such contentless conjunctions and witnesses their associativity, like this:
\[
\tikzfig{mushroom/contentlesscnj} 
\]
\end{convention}

\begin{convention}[Lonely wires]\label{conv:exists}
There's really only a single kind of text circuit we can draw that doesn't obviously correspond to a text diagram, and that's one where gates are missing.
\[
\tikzfig{textcirc/lonelywires} 
\]
In process theories, wires are identity processes that do nothing to their inputs. We require a text diagram analogue, and an intransitive "null-verb" in English that seems to work is \texttt{is}, in the sense of \texttt{exists}. In terms of text diagrams, we want a rewrite that introduces such contentless verbs, like this:
\[
\tikzfig{mushroom/exists} 
\]
\end{convention}

\begin{construction}[Circuit to text]\label{prop:circ2text}
In the presence of additional rewrites from Conventions \ref{conv:and} and \ref{conv:exists}, every text circuit is obtainable from some text diagram, up to Conventions \ref{conv:twist} and \ref{conv:sliding}.

\begin{figure}[h!]
\centering
\[
\resizebox{0.75\textwidth}{!}{\tikzfig{textcirc/sunny1}}
\]
\caption{Starting with a circuit, we may use Convention \ref{conv:twist} to arrange the circuit into alternating slices of twisting wires and (possibly tensored) circuits, and this arrangement recurses within boxes. Slices with multiple tensored gates will be treated using Convention \ref{conv:and}. By convention \ref{conv:exists}, we decorate lonely wires with formal \texttt{exists} gates, as in the \texttt{Frank sees} box. Observe how verbs with sentential complement are depicted with grey gaps, whereas the adverb and adposition combination of \texttt{Mac crazily laughs at Cricket} is gapless, according to Convention \ref{conv:gaps}.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{textcirc/sunny2}}
\]
\caption{We then linearise the slices, representing top-to-bottom composition as left-to-right. Twist layers are eliminated, replaced instead by dotted connections indicating processive connectivity. The dashed vertical line distinguishes slices. This step of the procedure always behaves well, guaranteed by Proposition \ref{prop:linkedlist}. Noun wires that do not participate in earlier slices can be shifted right until the slice they are introduced.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{textcirc/sunny3}}
\]
\caption{We recurse the linearisation procedure within boxes until there are no more sequentially composed gates. The linearisation procedure evidently terminates for finite text circuits. At this point, we have abstracted away connectivity data, and we are left with individual gates.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{textcirc/sunny4}}
\]
\caption{By Proposition \ref{prop:compsent}, gates are equivalent to sentences up to notation, so we swap notations \emph{in situ}. Conventions \ref{conv:and} and \ref{conv:exists} handle the edge cases of parallel gates and lonely wires. Observe that the blue-dotted wiring in text diagrams delineates the contents of boxes that accept sentences.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{textcirc/sunny5}}
\]
\caption{Recursing notation swaps outwards and connecting left-to-right slices as sentence-bubbles connect yields a text circuit, up to the inclusion of rewrites from Conventions \ref{conv:and} and \ref{conv:exists}: applying the reverse of those rewrites and the reverse of text-diagram rewrites yields a valid text-diagram derivation, by Propositions \ref{prop:compsent} and \ref{prop:linkedlist}.}
\end{figure}
\end{construction}

\newpage
\subsection{Extensions I: relative and reflexive pronouns}

\newthought{Subject relative pronouns}

\begin{example}

\end{example}

\newthought{Object relative pronouns}

\begin{example}

\end{example}

\newthought{Reflexive pronouns}

\begin{example}

\end{example}

\subsection{Extensions II: grammar equations}

\newthought{Attributive vs. predicative modifiers}

\begin{example}

\end{example}

\newthought{Copulas}

\begin{example}

\end{example}

\newthought{Possessive pronouns}

\begin{example}

\end{example}

\subsection{Extensions III: higher-order modifiers}

\newthought{Intensifiers}

\begin{example}

\end{example}

\newthought{Comparatives}

\begin{example}

\end{example}

\subsection{Equivalence to internal wirings}

\subsection{Related work}