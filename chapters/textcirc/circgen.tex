\newpage

\section{A generative grammar for text circuits}

\subsection{A circuit-growing grammar}

\marginnote{
\begin{defn}[Lexicon]\label{defn:lex}
We define a limited lexicon $\mathcal{L}$ to be a tuple of disjoint finite sets $(\mathbf{N}, \mathbf{V}_1, \mathbf{V}_2, \mathbf{V}_{\texttt{S}}, \mathbf{A}_{\texttt{N}}, \mathbf{A}_{\texttt{V}}, \mathbf{C})$
\end{defn}
}

\marginnote{
Where:
\begin{itemize}
\item $\mathbf{N}$ is a set of \emph{proper nouns}
\item $\mathbf{V}_1$ is a set of \emph{intransitive verbs}
\item $\mathbf{V}_2$ is a set of \emph{transitive verbs}
\item $\mathbf{V}_{\texttt{S}}$ is a set of \emph{sentential-complement verbs}
\item $\mathbf{A}_{\texttt{N}}$ is a set of \emph{adjectives}
\item $\mathbf{A}_{\texttt{V}}$ is a set of \emph{adverbs}
\item $\mathbf{C}$ is a set of \emph{conjunctions}
\end{itemize}
}

\begin{marginfigure}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/howtoread}}
\]
\caption{\textbf{How to read the diagrams in this section:} we will be making heavy use of pink and purple bubbles as frames to construct circuits. We will depict the bubbles horizontally, as we are permitted to by compact closure, or by reading diagrams with slightly skewed axes.}
\end{marginfigure}

There are many different ways to write a weak $n$-categorical signature that generates circuits. Mostly as an illustration of expressivity, I will provide a signature where the terms "surface" and "deep" structure are taken literally as metaphors; the generative grammar will grow a line of words in syntactic order, and like mushrooms on soil, the circuits will behave as the mycelium underneath the words. It won't be the most efficient way to do it in terms of the number of rules to consider, but it will look nice and we'll be able to reason about it easily.\\

\newthought{Simplifications and limitations}: For now we only consider word types as in Definition \ref{defn:lex}, though we will see how to engineer extensions later. We only deal with propositional statements, without determiners, in only one tense, with no morphological agreement between nouns and their verbs and referring pronouns, and we assume that adverbs, adjectives stack indefinitely and without further order requirements: e.g. \texttt{Alice happily secretly finds red big toy shiny car that he gives to Bob} is a sentence we consider grammatical enough. For now, we consider only the case where adjectives and adverbs appear before their respective noun or verb. Note that all of these limitations apart from the limited lexicon can principle be overcome by the techniques we developed in Section \ref{sec:ncat} for restricted tree-adjoining and links. As a historical remark, generative-transformational grammars fell out of favour linguistically due to the problem of overgeneration: the generation of nonsense or unacceptable sentences in actual language use. We're undergenerating and overgenerating at the same time, but we're also not concerned with empirical capture: we only require a concrete mathematical basis to build interesting things on top of. On a related note, there's zero chance that this particular circuit-growing grammar even comes close to how language is actually produced by humans, and I have no idea whether a generalised graph-rewriting approach is cognitively realistic.

\newthought{Mathematical assumptions}: We work in a dimension where wires behave symmetric monoidally by homotopy, and further assume strong compact closure rewrite rules for all wire-types. Our strategy will be to generate "bubbles" for sentences, within which we can grow circuit structure piecemeal. We will only express the rewrite rules; the generators of lower dimension are implicit. We aim to recover the linear ordering of words in text (essential to any syntax) by traversing the top surface of a chain of bubbles representing sentence structure in text -- this order will be invariant up to compact closed isomorphisms. The diagrammatic consequence of these assumptions is that we will be working with a conservative generalisation of graph-rewriting defined by local rewriting rules. The major distinction is that locality can be redefined up to homotopy, which allows locally-defined rules to operate in what would be a nonlocal fashion in terms of graph neighbourhoods, as in Figure \ref{fig:locality}. The minor distinction is that rewrite rules are sensitive to twists in wires and the radial order in which wires emanate from nodes, though it is easy to see how these distinctions can be circumvented by additional by imposing the equivalent of commutativity relations as bidirectional rewrites. It is worth remarking that one can devise weak n-categorical signatures to simulate turing machines, where output strings are e.g. 0-cells on a selected 1-cell, so rewrite systems of the kind we propose here are almost certainly expressively sufficient for anything; the real benefit is the interpretable geometric intuitions of the diagrams.

\begin{figure}[h!]\label{fig:locality}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/locality}}
\]
\caption{In this toy example, obtaining the same rewrite that connects the two yellow nodes with a purple wire using only graph-theoretically-local rewrites could potentially require an infinite family of rules for all possible configurations of pink and cyan nodes that separate the yellow, or would otherwise require disturbing other nodes in the rewrite process. In our setting, strong compact closure homotopies handle navigation between different spatial presentations so that a single rewrite rule suffices: the source and target notated by dotted-black circles. Despite the expressive economy and power of finitely presented signatures, we cannot "computationally cheat" graph isomorphism: formally we must supply the compact-closure homotopies as part of the rewrite, absorbed and hidden here by the $\simeq$ notation.}
\end{figure}

\newthought{The plan}: We start with simple sentences that only contain a single intransitive or transitive verb. Then we consider more general sentences. For these two steps, we characterise the expressive capacity of our rules in terms of a context-sensitive grammar that corresponds to the surface structure of the derivations. Then we introduce text structure as lists of sentences with coreferential structure on nouns, along with a mathematical characterisation of coreferential structure and a completeness result of our rules with respect to them. Then we (re)state and prove the text circuit theorem: that the fragment of language we have built with the syntax surjects onto text circuits. Finally we examine how we may model extensions to the expressive capacity of text circuits by introduction of new rewrite rules.

\clearpage

\subsection{Simple sentences}

\marginnote{
\begin{defn}[CSG for simple sentences]\label{dfn:simpCSG}
We may gauge the expressivity of simple sentences with the following context sensitive grammar.
\end{defn}
}

\marginnote{For verbs, adjectives, and modifiers, depicted unsaturated nouns as dotted and saturated with solid black lines, we have:
\[
\resizebox{\marginparwidth}{!}{\tikzfig{mushroom/simpleCFG}}
\]
}

\marginnote{
Adpositions require several helper-generators; we depict for example the beginning of the sequence of derivations that result from appending adpositions to an intransitive verb (the generators are implicit in the derivations):
\[
\resizebox{0.75\marginparwidth}{!}{\tikzfig{mushroom/simpleADP}}
\]
}

\marginnote{
\begin{proposition}\label{prop:simpsent}
Up to labels, the simple-sentence rules yield the same simple sentences as the CSG for simple sentences.
\begin{proof}
By graphical correspondence; viewing nodes on the pink surface as 1-cells, each rewrite rule yields a 2-cell. For example, for the \textcolor{green}{\texttt{IV}}-intro:
\[
\resizebox{0.5\marginparwidth}{!}{\tikzfig{mushroom/simpcorr}}
\]
\end{proof}
\end{proposition}
}

Simple sentences are sentences that only contain a single intransitive or transitive verb. Simple sentences will contain at least one noun, and may optionally contain adjectives, adverbs, and adpositions. The rules for generating simple sentences are as follows:

\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/simplesentences}}
\]

The $\texttt{N}_\uparrow$-intro rule introduces new unsaturated nouns from the end of a simple sentence. The \textcolor{green}{\texttt{IV}}-intro rule applies when there is precisely one unsaturated noun in the sentence, and the \textcolor{green}{\texttt{TV}}-intro rule applies when there are precisely two. Both verb-introduction rules saturate their respective nouns, which we depict with a black bulb. Adjectives may be introduced immediately preceding saturated nouns, and adverbs may be introduced immediately preceding any kind of verb. The position of adpositions in English is context-sensitive. To capture this, the $\textcolor{blue}{\texttt{ADP}}_{\textcolor{green}{\texttt{V}}}$-tendril rule allows an unsaturated adposition to appear immediately after a verb; a bulb may travel by homotopy to the right, seeking an unsaturated noun. Conversely, the bidirectional $\textcolor{blue}{\texttt{ADP}}_{\texttt{N}}$-tendril rule sends a mycelic tendril to the left, seeking a verb. The two pass-rules allow unsaturated adpositions to swap past saturated nouns and adjectives; note that by construction, neither verbs nor adverbs will appear in a simple sentence to the right of a verb, so unsaturated adpositions will move right until encountering an unsaturated noun. In case it doesn't, the tendril- and pass- rules are bidirectional and reversible.

\clearpage

\subsection{Complex sentences}

Now we consider two refinements; conjunctions, and verbs that take sentential complements. we may have two sentences joined by a conjunction, e.g. \texttt{Alice dances \underline{while} Bob drinks}. We may also have verbs that take a sentential complement rather than a noun phrase, e.g. \texttt{Alice \underline{sees} Bob dance}; these verbs require nouns, which we depict as wires spanning bubbles.

\begin{figure}[h!]\label{fig:sentbestiary}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/Sbestiary}}
\]
\caption{The dotted-blue wires do not contentfully interact with anything else, but this noninteraction disallows overgeneration cases where adpositional phrases might interject between \texttt{SCV} verbs and their sentential complement, e.g. \textcolor{red}{\texttt{Alice sees \underline{at lunch} Bob drink}}. The dotted-blue wires also indicate a diagrammatic strategy for extensions to accommodate noun phrases, to be explored later.}
\end{figure}

\marginnote{
\begin{defn}[Sentence structure]\label{dfn:sentCSG}
A sentence can be:
\begin{itemize}
\item a simple sentence, which...
\item ... may generate unsaturated nouns from the right.
\item a pair of sentences with a conjunction in between.
\item (if there is a single unsaturated noun) a sentence with a sentential-complement verb that scopes over a sentence.
\end{itemize}
As a CSG, these considerations are respectively depicted as:
\end{defn}
\[
\resizebox{\marginparwidth}{!}{\tikzfig{mushroom/complexsentenceCSG}}
\]
}

\marginnote{
\begin{proposition}\label{prop:compsent}
Up to labels, the rules so far yield the same sentences as the combined CSG of Definitions \ref{dfn:simpCSG} and \ref{dfn:sentCSG}.
\begin{proof}
Same correspondence as Proposition \ref{prop:simpsent}, ignoring the dotted-blue guards.
\end{proof}
\end{proposition}
}

\begin{figure}[h!]\label{fig:soberA}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/soberA}}
\]
\caption{
\begin{example}[\texttt{sober} $\alpha$ \texttt{sees drunk} $\beta$ \texttt{clumsily dance.}]
Now we can see our rewrites in action for sentences. As a matter of convention -- reflected in how the various pass- rules do not interact with labels -- we assume that labelling occurs after all of the words are saturated. We have still not introduced rules for labelling nouns: we delay their consideration until we have settled coreferential structure. For now they are labelled informally with greeks.
\end{example}
}
\end{figure}

\begin{figure}[h!]\label{fig:Alaughs}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/Alaughs}}
\]
\caption{
\begin{example}[$\alpha$ \texttt{laughs at} $\beta$]
Adpositions form by first sprouting and connecting tendrils under the surface. Because the tendril- and pass- rules are bidirectional, extraneous tendrils can always be retracted, and failed attempts for verbs to find an adpositional unsaturated noun argument can be undone. Though this seems computationally wasteful, it is commonplace in generative grammars to have the grammar overgenerate and later define the set of sentences by restriction, which is reasonable so long as computing the restriction is not computationally hard. In our case, observe that once a verb has been introduced and its argument nouns have been saturated, only the introduction of adpositions can saturate additionally introduced unsaturated nouns. Therefore we may define the finished sentences of the circuit-growing grammar to be those that e.g. contain no unsaturated nodes on the surface, which is a very plausible linear-time check by traversing the surface.
\end{example}
}
\end{figure}

\clearpage

\subsection{Text structure and noun-coreference}

\begin{figure}[h!]
\centering
\[
\tikzfig{mushroom/sintro}
\]
\caption{Only considering words, text is just a list of sentences. However, for our purposes, text additionally has \emph{coreferential structure}. Ideally, we would like to connect "the same noun" from distinct sentences as we would circuits.}
\end{figure}

\begin{figure}[h!]
\centering
\[
\tikzfig{mushroom/circuitplan}
\]
\caption{We choose the convention of connecting from left-to-right and from bottom-to-top, so that we might read circuits as we would text: the components corresponding to words will be arranged left-to-right and top-to-bottom. Connecting nouns across distinct sentences presents no issue, but a complication arises when connecting nouns within the same sentence as with reflexive pronouns e.g. \texttt{Alice likes herself}.}
\end{figure}

\begin{figure}[h!]\label{fig:reflcomp}
\centering
\[
\tikzfig{mushroom/reflcomplication}
\]
\caption{Reflexive coreference would violate of the processivity condition of string diagrams for symmetric monoidal categories. Not all symmetric monoidal categories possess the appropriate structure to interpret such reflexive pronouns, but there exist interpretative options. From left to right in roughly decreasing stringency, compact closed categories are the most direct solution. More weakly, traced symmetric monoidal categories also suffice. If there are no traces, so long as the noun wire possesses a monoid and comonoid, a convolution works. If all else fails, one can just specify a new gate. We will define coreference structure to exclude such reflexive coreference and revisit the issue as an extension.}
\end{figure}

\newpage

Now we will deal with coreferential structure and noun-labels.
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounbestiary_newnew}}
\]

The linked-\texttt{N}-intro rules introduce a new unsaturated noun in the next sentence that coreferences the noun in the previous sentence that generated it. The \texttt{N}-shift rules allow any unsaturated noun to move into the next sentence. For both of the previous rules, the $\beta$ variant handles the case where the next sentence is related to the first by a conjunction. Observe that nouns with a forward coreference have two dotted-black wires leaving the root of their wires, which distinguishes them from nouns that only have a backward coreference or no coreference at all, which only have a single dotted-black wire leaving the root of their wire.\\

The \texttt{N}-swap rule variants allow a unsaturated noun with no forward coreferences to swap places with any unsaturated noun that immediately succeeds it.\\

\begin{figure}[h!]\label{fig:nounkinds}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounkinds}}
\]
\caption{At this point, it is worth establishing some terminology about the kinds of unsaturated nouns we have in play. The kinds of nouns are distinguished by their tails. \emph{Lonely} nouns have no coreferences, their tails connect to nothing. \emph{Head} nouns have a forward coreference in text; they have two tails, one that connects to nothing and the other to a noun later in text. \emph{Middle} nouns have a forward and backward coreference; they have two tails, one that connects to a noun in some preceding sentence, and one that connects forward to a noun in a succeeding sentence. \emph{Foot} nouns only have a backward coreference; they have a single tail connecting to a noun in some preceding sentence.}
\end{figure}

\marginnote{
\begin{lemma}\label{prop:linkedlist}
The unsaturated noun kinds listed in Figure \ref{fig:nounkinds} are exhaustive, hence nouns that share a coreference are organised as a diagrammatic linked-list.
\begin{proof}
The \texttt{N}-intro rule creates lonely nouns. Head nouns can only be created by the linked-\texttt{N}-intro applied to a lonely noun. Any new noun created by linked-\texttt{N}-intro is a foot noun. The linked-\texttt{N}-intro rule turns foot nouns into middle nouns. These two intro- rules are the only ones that introduce unsaturated nouns, so it remains to demonstrate that no other rules can introduce noun-kinds that fall outside our taxonomy. The \texttt{N}-shift rule changes relative position of either a lonely or foot noun but cannot change its kind. The \texttt{N}-swap rule may start with either a lonely or foot noun on the left and either a head or middle noun on the right, but the outcome of the rule cannot change the starting kinds as tail-arity is conserved and the local nature of rewrites cannot affect the ends of tails.
\end{proof}
\end{lemma}
}

When the linked-list structure of coreferences is set, we propagate noun labels from the head of each list. The rules for noun-label propagation are as follows:

\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/nounbestiary_newnew2}}
\]

\bR FILL IN DESCRIPTIONS \e

\begin{example}[\texttt{sober Alice sees Bob clumsily dance. She laughs at him.}]
\end{example}
\begin{figure}[h!]\label{fig:corefex1}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex1}}
\]
\caption{\bR FILL IN DESCRIPTIONS \e}
\end{figure}

\begin{figure}[h!]\label{fig:corefex2}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex2}}
\]
\caption{\bR FILL IN DESCRIPTIONS \e}
\end{figure}

\begin{figure}[h!]\label{fig:corefex3}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex3}}
\]
\caption{\bR FILL IN DESCRIPTIONS \e}
\end{figure}

\begin{figure}[h!]\label{fig:corefex4}
\centering
\[
\resizebox{\textwidth}{!}{\tikzfig{mushroom/corefex4}}
\]
\caption{\bR FILL IN DESCRIPTIONS \e}
\end{figure}


\newpage
\subsection{Extensions I: relative and reflexive pronouns}

\newthought{Subject relative pronouns}

\begin{example}

\end{example}

\newthought{Object relative pronouns}

\begin{example}

\end{example}

\newthought{Reflexive pronouns}

\begin{example}

\end{example}

\subsection{Extensions II: grammar equations}

\newthought{Attributive vs. predicative modifiers}

\begin{example}

\end{example}

\newthought{Copulas}

\begin{example}

\end{example}

\newthought{Possessive pronouns}

\begin{example}

\end{example}

\subsection{Extensions III: higher-order modifiers}

\newthought{Intensifiers}

\begin{example}

\end{example}

\newthought{Comparatives}

\begin{example}

\end{example}

\subsection{Equivalence to internal wirings}

\subsection{Text circuit theorem}

\subsection{Related work}