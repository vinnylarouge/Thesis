\section{Text circuits: details, demos, developments}\label{sec:circs}

\marginnote{An example from Task 1, "single supporting fact", is:\\
\[\texttt{Mary went to the bathroom.}\]
\[\texttt{John moved to the hallway.}\]
\[\texttt{Mary travelled to the office.}\]
\[\texttt{\textcolor{blue}{(Query:) Where is Mary?}}\]
\[\texttt{(Answer:) office.}\]
Translating the setup of each task into a circuit of neural nets-to-be-learnt, and queries into appropriately typed measurements-to-be-learnt, each bAbi task becomes a training condition: the depicted composite process ought to be equal to the \texttt{office} state.
\[
\scalebox{0.75}{\tikzfig{textcirc/babiex}}  
\]
Solving bAbi in this way also means that each word gate has been learnt in a conceptually-compliant manner, insofar as the grounded meanings of words are reflected in how words interact and modify one another. One may argue that the verb \texttt{to go} and synonyms have been learnt-from-data in a way that coheres with human conceptions by appeal to the bAbi dataset.
}

This section covers some practical developments, conventions, references for technical details of text circuits. The most striking demonstration to date is that circuits are defined over a large enough fragment of language to \emph{leverage} several bAbi tasks \bR CITE \e, which are a family of 20 general-reasoning text tasks -- the italicised choice of wording will be elaborated shortly. Each family of tasks consists of tuples of text in simple sentences concluded by a question, along with an answer. It was initially believed that world models were required for the solution of these tasks, but they have been solved using transformer architectures. While there is no improvement in capabilities by solving bAbi using text circuits, the bAbi tasks have been used as a dataset to learn word gates from data, in a conceptually compliant and compositional manner. Surprisingly, despite the low-data, low-compute regime, the tasks for which the current theory has the expressive capacity to cover are solved better by text circuits than by LSTMs; a proof-of-concept that with the aid of appropriate mathematics, not only might fundamental linguistic considerations help rather than hinder NLP, but also that explainability and capability are not mutually exclusive. Experimental details are elaborated in a forthcoming report \bR CITE \e. While there are expressivity constraints contingent on theoretical development, this price buys a good amount of flexibility within the theoretically established domain: text circuits leave room for both learning-from-data and "hand-coded" logical constraints expressed process-theoretically, and naturally accommodate previously computed vector embeddings of words.\\

In practice, the process of obtaining transparently computable text goes through two phases. First, one has to obtain text circuits from text, which is conceptually simple: typelogical parsers for sentences can be modified to produce circuit-components rather than trees, and a separate pronomial resolution module dictates symmetric monoidal compositionality; details are in the same forthcoming report \bR CITE \e. Second, one implements the text circuits on a computer. On quantum computers, boxes are modelled as quantum combs \bR CITE \e. On classical computers, boxes are sandwiches of generic vector-manipulation neural nets, and boxes with 'dot dot dot' typing are interpreted as families of processes, which can be factored for instance as a pair of content-carrying gates along with a monoid+comonoid convolution to accommodate multiplicity of wires. The theoretical-to-practical upshot of text circuits when compared to DisCoCat is that the full gamut of compositional techniques, variations, and implementation substrates of symmetric monoidal categories may be used for modelling, compared to the restrictions inherent in hypergraph and strongly compact closed categories.\\

In terms of underpinning mathematical theory, the `dot dot dot' notation within boxes is graphically formal \citep{wilson_string_2022}, and interpretations of such boxes were earlier formalised in \citep{merry_reasoning_2014,quick_-logic_2015,zamdzhiev_rewriting_2017}. The two forms of interacting composition, one symmetric monoidal and the other by nesting is elsewhere called \emph{produoidal}, and the reader is referred to \bR CITE \e for formal treatment and a coherence theorem. Boxes with holes may be interpreted in several different ways. Firstly, boxes may be considered syntactic sugar for higher-order processes in monoidal closed categories, and boxes are diagrammatically preferable to combs in this regard, since the latter admits a typing pathology where two mutually facing combs interlock. Secondly, boxes need not be decomposable as processes native to the base category, admitting for instance an interpretation as elementwise inversion in linear maps, which specialises in the case of \textbf{Rel} (viewed as \textbf{Vect} over the boolean ring) to negation-by-complement. In some sense, none of these formalities really matter, on the view that text circuits are algebraic jazz for interpreting text, where facets are open to interpretation and modification.

\subsection{Avenues I: syncategorematicity and structural logic}

A useful heuristic for the application of text diagrams is to treat individual text circuits as analogous to propositional contents, and certain logical or temporal connectives as structural operations upon circuits -- rewrites -- that must be applied in order to obtain a purely propositional format.

\begin{example}[\texttt{Alice and Bob dance}]

\end{example}

\begin{example}[\texttt{Alice and Bob drink beer and wine respectively}]

\end{example}

\begin{example}[\texttt{Alice arrived after Bob}]

\end{example}

\subsection{Avenues II: determiners, quantifiers, context}

Extending the reach of text circuits to determiners and quantifiers appears to require a contextual process theory in which to evaluate and enforce constraints upon the purely syntactic content of text circuits.

\begin{example}[\texttt{Bob drinks the beer}]

\end{example}

\end{example}[\texttt{Bob drinks all the beers}]

\end{example}

\end{example}[\texttt{If bob drinks one beer, he's going to drink all of them}]

\end{example}

\subsection{Avenues III: grammar as geometry, geometry as computation}

The example of noun phrases illustrates the higher principle that the particulars of interpretational choices into string diagrams are inessential: any consistent scheme will do, and the string diagrams will give voice to the computational structure of dynamic semantics.

\newthought{Noun phrases}

\begin{example}[\texttt{The king of France is bald}]

\end{example}


\clearpage