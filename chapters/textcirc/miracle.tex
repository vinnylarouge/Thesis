\begin{fullwidth}

\section{How do we communicate using language?}

\newthought{Speakers and listeners understand one another.} Obviously, natural language involves communication, which involves at minimum a speaker and a listener, or a producer and a parser. Please write to me if you disagree because I haven't found anyone yet that does. The fact that communication happens at all is an everyday miracle that any formal understanding of language must account for. The miracle remains so even if we cautiously hedge to exclude pragmatics and context and only encompass small and boring fragments of factual language like \texttt{Alice sees Bob quickly run to school}. At minimum, we should be able to model a single conversational turn, where a speaker produces a sentence from a thought and a listener parses it to recover the thought. Here is a sequence of diagram equations -- which we will revisit in Section \ref{} -- that demonstrates mathematically how the miracle works for two toy grammars, for the sentence \texttt{Alice sees Bob quickly run to school}. On the left we have a grammatical structure obtained from a context-free grammar, and we have equations from a discrete monoidal fibration all the way to the right, where we obtain a pregroup representation of the same sentence. Going from right to left recovers the correspondence in the other direction.

\[\resizebox{\textwidth}{!}{\tikzfig{tree2gate/workedexample/bigexaltogether}}\]

I know that CFGs and pregroups are syntactic backwoods, which is why I will also show how the story works with generalisations of TAGs (circuit-adjoining grammars) as productive grammars and more powerful pregroup \emph{diagrams} as parsing grammars in Sections \ref{}. Now I will briefly go on the offense. \textbf{\\Start(Offense):} In my view, that picture is so far the only honest standard of what counts as a combined theory of syntax and semantics, if one accepts the following three commitments.

\begin{enumerate}
\item{At some level, semantics is compositional, and syntax directs this composition.}
\item{Speakers produce sentences, and listeners parse sentences.}
\item{Speakers and listeners understand each other, insofar as the compositional structure of their semantic representations are isomorphic.}
\end{enumerate}

If that is unobjectionable, then to the best of my knowledge, that picture is \emph{only} account of both syntax and semantics worth anything that you have ever seen. Mathematical elaboration and coverage of objections will happen in Section \ref{}. Now here is why that picture, for now, is the only game in town for an account of syntax and semantics in natural language in a post-LLM world.\\

\newthought{Theories of grammar by themselves are insufficient to account for communication.} For every grammar that produces sentences, one must also provide a corresponding parsing grammar. A theory of grammar that only produces correct sentences or correct parses is a 'theory' of language outperformed in every respect by an LLM. So we must distinguish between grammars of the speaker and listener, and then investigate how they cohere.\\

\newthought{Coherence of theories of grammar is inextricable from semantics.} We are interested in the ideal of communication, the end result of a single turn of which is that both speaker and listener have the same semantic information, whether that be a logical expression or something else. A consequence of this criterion is that in order to obtain an adequate account of communication, we must seek a relation between grammars and semantics beyond weak and strong equivalence of pairs of theories of grammar.\\

Firstly, "weak equivalence" between grammar formalisms in terms of possible sets of generated sentences is insufficient. Weak equivalence proofs are mathematical busywork that have nothing to do with a unified account of syntax and semantics. For example, merely demonstrating that, e.g. pregroup grammars and context-free grammars can generate the same sentences [] only admits the possibility that a speaker using a context-free grammar and a listener using a pregroup grammar \emph{could} understand each other, without providing any explanation \emph{how}. But we already know that users of language \emph{do} understand one another, so the exercise is pointless.\\

Secondly, "strong equivalence" that seeks equivalence at a structural level between theories of syntax often helps, but is not always necessary. I will explain by analogy. Theories of syntax are like file formats, e.g. \texttt{.png} or \texttt{.jpeg} for images. A model for a particular language is a particular file or photograph. The task here is to show that two photographs in different file formats that both purport to model the same language are really photographs of the same thing from different perspectives. It is overkill to demonstrate that all \texttt{.pngs} and \texttt{.jpegs} are structurally bijectable, just as it is overkill to show that, say, context-free grammars are strongly equivalent to pregroup grammars, because there are context-free and pregroup grammars that generate sets of strings that have nothing to do with natural language. But, the structural agreement of grammars given by strong equivalence can simplify a Montagovian assignment of semantics, since strong agreement means there is essentially a single structure to consider, rather than two.\\

A systematic analysis of communication requires intimacy with specific grammars and a specific semantics. Specific grammars -- and not formats of grammar, such as "all CFGs" -- that model natural languages, even poorly, are the only linguistically relevant objects of study. Once you have a specific grammar that produces sentences in natural language, then to explain communication, you must supply a specific partnered parsing grammar such that on the produced sentences, both grammars yield the same semantic objects by a Montagovian approach, broadly construed as a homomorphism from syntax to semantics. On this account, syntax does not hold a dictatorship over semantics, but we can find duarchies, and in these duumvirates the two syntaxes and the semantics mutually constrain one another.\\

\newthought{A post-LLM theory of language requires coherence between theories of syntax -- one for the speaker and one for the listener -- such that their underlying structures are strongly equivalent and are amenable in principle to distributional or vector semantics.} Anything short of this fails to provide any understanding of language beyond what can be gleaned from an LLM. If you don't have two coherent theories of syntax for speaker and listener, you cannot account for communication. If you don't have a matching theory of semantics that can be learned in principle from data and represented as vectors-and-operations-upon-them, your theory will never be of practical relevance; \textbf{N.B.} go collect an award and dance on Minsky's grave if you figure out how to learn insert-FOL-variant-here representations of dictionary words from data. The stakes are, at worst, the entirety of linguistics as an enterprise of inquiry. At best, it's all made up and nothing really matters so I will subscribe to this standard anyway just because. To the best of my knowledge there isn't any, even partial account of language that satisfies the bare minumum. Perhaps this is due to siloisation, because I don't doubt that all of the ingredients are already present in the literature: there are popular accounts of how grammar composes (albeit practically worthless) truth-theoretic semantics [Heim and Kratzer], and there are too many equivalence proofs between different grammatical formalisms, but I can't find anything that puts the minimum all at once in the same bowl. If it does exist already, my cake is probably prettier and tastier. So in this thesis I will supply a post-LLM theory of language, and I will endeavour to do it as diagrammatically as possible. I only care to get the absolute basics right by my own standard, and I will accordingly treat everything else about language like pragmatics and gesture and whatever else as an afterthought, \emph{as it should be}: consider air resistance only after you know how motion works in a vacuum.

\subsection{Grammars of speakers and listeners}

There is a distinction between \emph{grammars of the speaker} -- which produce sentences -- and \emph{grammars of the listener} -- which deduce sentences. Viewed as mathematical processes, the two kinds of grammars go in opposite directions; speaking grammars (e.g. string-rewrite systems) start with some grammatical structure, and require informational input (e.g. which rule comes next) to produce lists of words -- sentences. Conversely, listening grammars (e.g. typelogical grammars) start with a sentence, and require informational input (e.g. grammatical typing and which proof rule to try next) to deduce or parse a grammatical structure. Since we can understand each other, these two types of grammar must enjoy a systematic correspondence, and if one believes that semantics is compositional according to syntax, then the correspondence must further explain how both speaking and listening grammars manipulate the same underlying semantic expression, whatever that may be.\\

Here are some na\"{i}ve observations on the nature of speaking and listening. Let's suppose that a speaker, Preube, wants to communicate a thought to Fondo. Just as a running example that does not affect the point, let's say we can gloss the thought in first order logic as $\exists a \exists b \exists c \exists f : A(a) \wedge B(b) \wedge C(c) \wedge F(f) \wedge L(a,f) \wedge G(b,c,f)$. In diagrammatic first order logic [], this is equivalently presented as the following diagrams (and any other diagram that agrees up to connectivity)

\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex}}\]

Preube and Fondo cooperate to achieve the miracle; Preube encodes his thoughts -- a structure that isn't a one-dimensional string of symbols -- into a one-dimensional string of symbols. And then Fondo does the reverse, turning a one-dimensional string of symbols into a thought-structure like that of Preube's. What I mean by "like that of Preube's" is that both Preube and Fondo agree on the structure of entities and relations up to the words for those entities and relations. For example, Preube could ask Fondo comprehension questions such as \texttt{WHO GAVE WHAT? TO WHO?}, and if Fondo can always correctly answer -- e.g. \texttt{BOB GAVE FLOWERS. TO CLAIRE.} -- then both Preube and Fondo agree on the relational structure of the communicated thought to the extent permitted by language. It may still be that Preube and Fondo have radically different internal conceptions of what \texttt{FLOWERS} or \texttt{GIVING} or \texttt{BEETLES IN BOXES} are, but that is alright: we only care that the \emph{interacting structure} of the thought-relations in each person's head are the same, not their specific representations.\\

We assume Preube and Fondo speak the same language, so both know how words in their language correspond to putative building blocks of thoughts, and how the order of words in sentences and special grammatical words affect the (de-/re-)construction procedures. Now we have to explain how it is that the two can do this for infinitely many thoughts, and new thoughts never encountered before. Using string diagrams, this is surprisingly easy, because string diagrams are algebraic expressions that are invariant under certain topological manipulations that make it easy to convert between different shapes of language. Here is a sketch example that we will soon have the mathematics to express formally:\\

\begin{example}[\texttt{Alice likes flowers that Bob gives Claire.}]
Let's say Preube is using a context-free grammar to produce sentences, and Fondo a pregroup grammar. The rule of the game is that Preube and Fondo can agree on a string-diagrammatic encoding strategy before having to communicate with each other. Here is one such strategy: Preube might generate the example sentence like so:
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex2a}}\]
Mathematically, it makes no difference if we take the Poincar\'{e} dual of the tree, so that zero-dimensional nodes become one-dimensional wires, and branchings become zero-dimensional points linking wires -- but we can just as well depict those points as boxes to label them more clearly.
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex2b}}\]
Now that Preube can express their grammatical structure string-diagrammatically, they can try to deform their first-order-logic diagram -- representing what they mean to communicate -- subject to the constraint that every one of their branchings (the structure of the CFG) is something recoverable by Fondo using just pregroup reductions. To do so, Preube introduces a formal blue wire to mimic Fondo's sentence-type, and stuffs some complexity inside the labels in the form of internal wirings: a multiwire configuration for \texttt{that}, and a twist for \texttt{gives}. Those internal wirings are the content of Preube and Fondo's shared strategy.
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex2c}}\]
So, when Fondo receives the sentence, Fondo's pregroup derivation yields a pregroup diagram that is connectively equivalent to what Preube stuffed inside the context-free grammar structure. So now the two have strong equivalence between their grammars in the sense that every one of Preube's branches is resolved by one of Fondo's reductions.
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex2d}}\]
Now to fully recover Preube's intended FOL-diagram, Fondo refers to the internal wirings that form their shared strategy, and fills those in.
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex2}}\]
\end{example}

\begin{example}[\texttt{Bob gives Claire flowers. Alice likes flowers.}]
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex3a}}\]
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex3b}}\]
\[\resizebox{\textwidth}{!}{\tikzfig{intro/GFOLex3}}\]
\end{example}

The nature of their challenge can be summarised as an asymmetry of information. The speaker knows the structure of a thought and has to supply information or computation in the form of choices to turn that thought into text. The listener knows only the text, and must supply information or computation to deduce the thought behind it. By this perspective, language is a shared and cooperative strategy to solve this (de/en)coding interaction. I will now outline the constraints imposed by this interaction, illustrating them simply using string-rewrite grammars for the speaker and pregroup grammars for the listener.

\newthought{Speakers choose.} The speaker Preube must supply decisions about phrasing a thought in the process of speaking it. At some point at the beginning of an utterance, Preube has a thought but has not yet decided how to say it. Finding a particular phrasing requires choices to be made, because there are many ways to express even simple relational thoughts. For example, the relational content of our running example might be expressed in at least two ways (glossing over determiners):

\[\texttt{Alice likes flowers that Bob gives Claire.}\]
\[\texttt{Bob gives Claire flowers. Alice likes (those) flowers.}\]

Whether those decisions are made by committee or coinflips, those decisions represent information that must be supplied to Preube in the process of speaking a thought. For this reason, we consider context-free-grammars (and more generally, other string-rewrite systems) to be \emph{grammars of the speaker}. The start symbol $S$ is incrementally expanded and determined by rule-applications that are selected by the speaker. The important aspect here is that the speaker has an initial state of information $S$ that requires more information as input in order to arrive at the final sentence. Note that the concept of grammars of the speaker are not exhausted by string-rewrite systems, merely that string-rewrite systems are a prototype that illustrate the concept well.

\newthought{Listeners deduce.} The listener Fondo must supply decisions about which words are grammatically related, and how. Like right-of-way in driving, sometimes these decisions are settled by convention, for example, subject-verb-object order in English. Sometimes sophisticated decisions need to be made that override or are orthogonal to conventions.

\begin{example}[Garden path sentences]
So-called "garden path" sentences illustrate that listeners have to make choices to resolve lexical ambiguities. One such garden-path sentence is \texttt{The old man the boat}, where typically readers take \texttt{The old man} as a noun-phrase and \texttt{the boat} as another noun-phrase. We can sketch how the readers might think with a (failed) pregroup grammar derivation:

\[
\AxiomC{$\texttt{the} : n \cdot n^{-1}$}
\AxiomC{$\texttt{old} : n \cdot n^{-1}$}
\BinaryInfC{$\texttt{the\textvisiblespace old}: n \cdot n^{-1}$}
\AxiomC{$\texttt{man} : n$}
\BinaryInfC{$\texttt{the\textvisiblespace old\textvisiblespace man}: n$}
\AxiomC{$\texttt{the} : n \cdot n^{-1}$}
\AxiomC{$\texttt{boat} : n$}
\BinaryInfC{$\texttt{the\textvisiblespace boat}: n$}
\BinaryInfC{\textcolor{red}{Not a sentence!}}
\DisplayProof
\]

So the reader has to backtrack, taking \texttt{The old} as a noun-phrase and \texttt{man} as the transitive verb. This yields a sentence as follows:

\[
\AxiomC{$\texttt{the} : n \cdot n^{-1}$}
\AxiomC{$\texttt{old} : n$}
\BinaryInfC{$\texttt{the\textvisiblespace old}: n$}
\AxiomC{$\texttt{man} : ^{-1}n \cdot s \cdot n^{-1}$}
\BinaryInfC{$\texttt{the\textvisiblespace old\textvisiblespace man}: s \cdot n^{-1}$}
\AxiomC{$\texttt{the} : n \cdot n^{-1}$}
\AxiomC{$\texttt{boat} : n$}
\BinaryInfC{$\texttt{the\textvisiblespace old}: n$}
\BinaryInfC{$\texttt{the\textvisiblespace old\textvisiblespace man\textvisiblespace the\textvisiblespace boat\textvisiblespace}: s$}
\DisplayProof
\]

Garden-path sentences illustrate that listeners must make choices about what grammatical roles to assign words. We make these kinds of contextual decisions all the time with lexically ambiguous words or highly homophonic languages like Mandarin; garden-path sentences are special in that they trick the default strategy badly enough that the mental effort for correction is noticeable.
\end{example}

\begin{example}[Ambiguous scoping]
Consider the following sentence:
\[\texttt{Everyone loves someone}\]
The sentence is secretly (at least) two, corresponding to two possible parses. The usual reading is (glossed) $\forall x \exists y : \texttt{loves}(x,y)$. The odd reading is $\exists y \forall x : \texttt{loves}(x,y)$: a situation where there is a single person loved by everyone. We can sketch this difference formally using a simple combinatory categorial grammar.

\[
\AxiomC{$\texttt{everyone} : (n \multimap s) \multimap s$}
\AxiomC{$\texttt{loves} : n \multimap s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\BinaryInfC{$\texttt{everyone\textvisiblespace loves} : s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\AxiomC{$\texttt{someone} : (s \ \rotatebox[origin=c]{180}{$\multimap$} \ n) \multimap s$}
\BinaryInfC{$\texttt{everyone\textvisiblespace loves\textvisiblespace someone} : s$}
\DisplayProof
\]

\[
\AxiomC{$\texttt{everyone} : (n \multimap s) \multimap s$}
\AxiomC{$\texttt{loves} : n \multimap s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\AxiomC{$\texttt{someone} : (s \ \rotatebox[origin=c]{180}{$\multimap$} \ n) \multimap s$}
\BinaryInfC{$\texttt{loves\textvisiblespace someone} : n \multimap s$}
\BinaryInfC{$\texttt{everyone\textvisiblespace loves\textvisiblespace someone} : s$}
\DisplayProof
\]

CCGs have functorial semantics in any cartesian closed category, such as one where morphisms are terms in the lambda calculus and composition is substitution. So we might specify a semantics as follows:

\begin{align}
[\![ \texttt{everyone} ]\!] = \lambda (\lambda x . V(x)) . \forall x : V(x) \\
[\![ \texttt{loves} ]\!] = \lambda x \lambda y . \texttt{loves}(x,y) \\
[\![ \texttt{someone} ]\!] = \lambda (\lambda y . V(y)) . \exists y : V(y)
\end{align}

Now we can plug-in these interpretations to obtain the two different meanings. We decorate with corners just to visually distinguish which bits are partial first-order logic.

\[
\AxiomC{$\lambda (\lambda x . V(x)) . \ulcorner \forall x : V(x) \urcorner : (n \multimap s) \multimap s$}
\AxiomC{$\lambda x \lambda y . \ulcorner \texttt{loves}(x,y) \urcorner : n \multimap s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\BinaryInfC{$ \lambda y . \ulcorner \forall x : \texttt{loves}(x,y) \urcorner : s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\AxiomC{$\lambda (\lambda y . V(y)) . \ulcorner \exists y : V(y) \urcorner : (s \ \rotatebox[origin=c]{180}{$\multimap$} \ n) \multimap s$}
\BinaryInfC{$ \ulcorner \exists y \forall x : \texttt{loves}(x,y) \urcorner : s$}
\DisplayProof
\]

\[
\AxiomC{$\lambda (\lambda x . V(x)) . \ulcorner \forall x : V(x) \urcorner : (n \multimap s) \multimap s$}
\AxiomC{$\lambda x \lambda y . \ulcorner \texttt{loves}(x,y) \urcorner : n \multimap s \ \rotatebox[origin=c]{180}{$\multimap$} \ n$}
\AxiomC{$\lambda (\lambda y . V(y)) . \ulcorner \exists y : V(y) \urcorner : (s \ \rotatebox[origin=c]{180}{$\multimap$} \ n) \multimap s$}
\BinaryInfC{$\lambda x . \ulcorner \exists y : \texttt{loves}(x,y) \urcorner : n \multimap s$}
\BinaryInfC{$ \ulcorner \forall x \exists y : \texttt{loves}(x,y) \urcorner : s$}
\DisplayProof
\]

\end{example}

As is convention for parsing, let's grant that there's a daemon in Fondo's head that makes all these lexical disambiguation choices for them, automatically settling on which sense of \texttt{the old man} or \texttt{somebody} is appropriate. As mathematicians looking for a toy model to get started, we are looking for the simplest kind of choice that Fondo can be trusted to make with only grammatical information available to them.

%\newthought{The speaker's choices and the listener's deductions must be related.} The way the speaker decomposes the thought into words in text in the speaker's grammar must allow the listener to reconstruct the thought in the listener's grammar. Even in simple cases where both parties are aiming for unambiguous communication, the listener still must make choices. This is best illustrated by introducing two toy grammars -- we pick a context-free grammar for the speaker and a pregroup grammar for the listener, because they are simple, planar, and known to be weakly equivalent.

\begin{example}
(grammar pieces -- as simple as it gets)
\texttt{Bob runs.}
\texttt{Bob quickly runs.}
\texttt{Bob drinks beer.}
\texttt{Bob quickly drinks beer.}

\end{example}

\end{fullwidth}

\subsection{Discrete Monoidal Fibrations}

We introduce the concept of a discrete monoidal fibration: a mathematical bookkeeping tool that relates kinds of choices speakers and listeners make when generating and parsing text respectively. We proceed diagrammatically. The first concept is that of a \emph{monoidal functor box}.

\begin{scholium}
Functor boxes are from [meillies]. Expressing the coherence conditions of monoidal functors using equations involving functor boxes as below is not new [meillies].
The idea of a functor being simultaneously monoidal and a fibration is not new [monoidalfibration]. What is new is minor: the express requirement that the lifts of the fibration satisfy interchange, which is in general not guaranteed by just having a functor be monoidal and a (even discrete) fibration [fosco].
\end{scholium}

Suppose we have a functor between monoidal categories $\mathbf{F}: \mathcal{C} \rightarrow \mathcal{D}$. Then we have the following diagrammatic representation of a morphism $\mathbf{F}A \overset{\mathbf{F}f}{\rightarrow} \mathbf{F}B$ in $\mathcal{D}$:

\[\tikzfig{tree2gate/mfunctorbox/mfbox-notation}\]

The use of a functor box is like a window from the target category $\mathcal{D}$ into the source category $\mathcal{C}$; when we know that a morphism in $\mathcal{D}$ is the image under $\mathbf{F}$ of some morphism in $\mathcal{C}$, the functor box notation is just a way of presenting all of that data at once. Since $\mathbf{F}$ is a functor, we must have that $\mathbf{F}f ; \mathbf{F}g = \mathbf{F}(f;g)$. Diagrammatically this equation is represented by freely splitting and merging functor boxes vertically.

\[\tikzfig{tree2gate/mfunctorbox/mfbox-seq}\]

Assume that $\mathbf{F}$ is strict monoidal; without loss of generality by the strictification theorem [], this lets us gloss over the associators and unitors. For $\mathbf{F}$ to be strict monoidal, it has to preserve monoidal units and tensor products on the nose: i.e. $\mathbf{F}I_\mathcal{C} = I_\mathcal{D}$ and $\mathbf{F}A \otimes_\mathcal{D} \mathbf{F}B = \mathbf{F}(A \otimes_\mathcal{C} B)$. Diagrammatically these structural constraints amount to the following equations:

\[\tikzfig{tree2gate/mfunctorbox/mfbox-fibration+structural}\]

What remains is the monoidality of $\mathbf{F}$, which is the requirement $\mathbf{F}f \otimes \mathbf{F}g = \mathbf{F}(f \otimes g)$. Diagrammatically, this equation is represented by freely splitting and merging functor boxes horizontally; analogously to how splitting vertically is the functor-boxes' way of respecting sequential composition, splitting horizontally is how they respect parallel composition.
\[\tikzfig{tree2gate/mfunctorbox/mfbox-tensor}\]

And for when we want $\mathbf{F}$ to be a (strict) symmetric monoidal functor, we are just asking that boxes and twists do not get stuck on one another.
\[\tikzfig{tree2gate/mfunctorbox/mfbox-twist}\]

\begin{remark}
To motivate fibrations, first observe that by the diagrammatic equations of monoidal categories and functor boxes we have so far, we can always "slide out" the contents of a functor box out of the bottom:
\[\tikzfig{tree2gate/mfunctorbox/mfbox-prefibex}\]
When can we do the reverse? That is, take a morphism in $\mathcal{D}$ and \emph{slide it into} a functor box? We know that in general this is not possible, because not all morphisms in $\mathcal{D}$ may be in the image of $\mathbf{F}$. So instead we ask "under what circumstances" can we do this for a functor $\mathbf{F}$? The answer is when $\mathbf{F}$ is a discrete fibration.
\end{remark}

\begin{defn}[Discrete opfibration]
$\mathbf{F}: \mathcal{C} \rightarrow \mathcal{D}$ is a \emph{discrete fibration} when:\\
for all morphisms $f: \mathbf{F}A \rightarrow B$ in $\mathcal{D}$ with domain in the image of $\mathbf{F}$...\\
there exists a unique object $\Phi^A_f$ and a unique morphism $\phi_f: A \rightarrow \Phi^A_f$ in $\mathcal{C}$...\\
such that $f = \mathbf{F}\phi_f$.\\

Diagrammatically, we can present all of the above as an equation reminiscent of sliding a morphism \emph{into} a functor box from below.
\[\tikzfig{tree2gate/mfunctorbox/mfbox-fibration}\]
\end{defn}

\begin{defn}[Monoidal discrete opfibration]
We consider $\mathbf{F}$ to be a \emph{(strict, symmetric) monoidal discrete opfibration} when it is a (strict, symmetric) monoidal functor, a discrete opfibration, and the following equations relating lifts to interchange hold:
\[\scalebox{0.75}{\tikzfig{tree2gate/mfunctorbox/mfbox-fibration+interchange}}\]
\end{defn}
\marginnote{
\begin{remark}
The diagrammatic motivation for the additional coherence equations is that -- if we view the lifts of opfibrations as sliding morphisms into functor boxes -- we do not want the order in which sliding occurs to affect the final result. In this way, lifts behave as 'graphical primitives' in the same manner as interchange isotopies and symmetry twists.
\end{remark}
}

\subsection{Relating the generative grammar to a pregroup grammar via a discrete monoidal fibration}
We merge the monoidal functor-boxes and we slide the bottom edge down using the fibration.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_0}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_1}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_2}}\]
\texttt{quickly} could find itself modifying an intransitive (single noun) or transitive (two noun) verb. Suppose that it is the job of some process $\textcolor{orange}{\texttt{q'}}$ to handle intransitive verbs, and similarly $\textcolor{orange}{\texttt{q''}}$ to handle transitive ones. We use the functor for bookkeeping, by asking it to send both $\textcolor{orange}{\texttt{q'}}$ and $\textcolor{orange}{\texttt{q''}}$ to the dependent label $\textcolor{orange}{\bar{\texttt{q}}}$. Diagrammatically, this assignment is expressed by the following equations:
\[\tikzfig{tree2gate/workedexample/gather_quickly}\]
Since the functor is a monoidal discrete fibration, it introduces the appropriate choice of \texttt{quickly} when we pull the functor-box down, while leaving everything else in parallel alone.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_2}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_3}}\]
Adpositions can apply for verbs of any noun-arity. We again use the fiber of the functor for bookkeeping by asking it to send all of the following partial pregroup diagrams to the adposition generator. We consider the pregroup typing of a verb of noun-arity $k \geq 1$ to be $^{-1} n \cdot s \cdot \underbrace{n^{-1} \cdots n^{-1}}_{(k-1)}$.
\[\tikzfig{tree2gate/workedexample/gather_adp}\]
When we pull down the functor-box, the discrete fibration introduces the appropriate choice of diagram from above, corresponding to the intransitive verb case.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_3}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_4}}\]
Similarly to \texttt{quickly}, we suppose we have a family of processes for the word \texttt{to}, one for each noun-arity of verb.
\[\tikzfig{tree2gate/workedexample/gather_to}\]
Again the discrete fibration introduces the appropriate choice of \texttt{to} when we pull the functor box down.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_4}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_5}}\]
Now we visually simplify the inside of the functor-box by applying yanking equations.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_5}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_6}}\]
Similarly as before, we can pull the functor-box past the intransitive verb node. There is only one pregroup type $^{-1}n \cdot s$ that corresponds to the grammatical category $\textcolor{green}{\texttt{(I)VP}}$.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_6to7}}\]
Proceeding similarly, we can pull the functor-box past the sentential-complement-verb node. There are multiple possible pregroup types for $\textcolor{ForestGreen}{\texttt{SCV}}$, depending on how many noun-phrases are taken as arguments in addition to the sentence. For example, in $\texttt{Alice \textcolor{ForestGreen}{sees} \textcolor{cyan}{[sentence]}}$, $\textcolor{ForestGreen}{\texttt{sees}}$ returns a sentence after taking a noun to the left and a sentence to the right, so it has pregroup typing $^{-1}n \cdot s \cdot s^{-1}$. On the other hand, for something like $\texttt{Alice \textcolor{ForestGreen}{\texttt{tells}} Bob \textcolor{cyan}{[sentence]}}$, $\textcolor{ForestGreen}{\texttt{tells}}$ returns a sentence after taking a noun (the teller) to the left, a noun (the tellee) to the left, and a sentence (the story) to the left, so it has a pregroup typing $^{-1}n \cdot s \cdot n^{-1} \cdot s^{-1}$. These two instances of sentential-complement-verbs are introduced by different nodes. We can record both of these pregroup typings in the functor by asking for the following:
\[\tikzfig{tree2gate/workedexample/gather_scv}\]
Pulling down the functor box:
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_7}}
\quad = \quad
\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_8}}\]
As before, we can ask the functor to send an appropriate partial pregroup diagram to the dependent label $\textcolor{ForestGreen}{\bar{\texttt{see}}}$.
\[\scalebox{0.5}{\tikzfig{tree2gate/workedexample/bigex_8to9}}\]
Now again we can visually simplify using the yanking equation and isotopies, which obtains a pregroup diagram.
\[\tikzfig{tree2gate/workedexample/bigex_a0}\]
The pregroup diagram corresponds to a particular pregroup proof of the syntactic correctness of the sentence \texttt{Alice sees Bob run quickly to school}.
\[
\AxiomC{$\texttt{A} : n$}
\AxiomC{$\textcolor{green}{\texttt{s}} : ^{-1}n \cdot s \cdot s^{-1}$}
\BinaryInfC{$\texttt{A\textvisiblespace \textcolor{green}{s}} : s \cdot s^{-1}$}
\AxiomC{$\texttt{B} : n$}
\AxiomC{$\textcolor{orange}{\texttt{q}} :  (^{-1}n \cdot s) \cdot ( ^{-1}n \cdot s)^{-1} $}
\AxiomC{$\textcolor{green}{\texttt{r}} : ^{-1}n \cdot s$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}} : ^{-1}n \cdot s$}
\AxiomC{$\textcolor{blue}{\texttt{t}} : ^{-1}( ^{-1}n \cdot s) \cdot ( ^{-1}n \cdot s) \cdot n^{-1}$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}} : (^{-1}n \cdot s) \cdot n^{-1}$}
\AxiomC{$\texttt{S} : n$}
\BinaryInfC{$\texttt{\textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : ^{-1}n \cdot s$}
\BinaryInfC{$\texttt{B\textvisiblespace \textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : s$}
\BinaryInfC{$\texttt{A\textvisiblespace \textcolor{green}{s}\textvisiblespace B\textvisiblespace \textcolor{orange}{q}\textvisiblespace \textcolor{green}{r}\textvisiblespace \textcolor{blue}{t}\textvisiblespace S} : s$}
\DisplayProof
\]

\begin{remark}
Technical addendums.\\
The above construction only requires the source category of the functor to be rigid autonomous. Since no braidings are required, the free autonomous completion [Antonificaton] of any monoidal category may be used.\\
To enforce the well-definedness of the functor $\mathbf{F}: \mathcal{P}\mathcal{G} \rightarrow \mathcal{G}$ on objects, we may consider the strictified "category of..." [ghicadiagrams]...
\end{remark}

\subsection{Discrete monoidal fibrations for grammatical functions}

\subsection{Discussion}

\newthought{It is worth noting that in practice, neither grammar nor meaning strictly determines the other.} Clearly there are cases where grammar supercedes: when Fondo hears \texttt{man bites dog}, despite his prior prejudices and associations about which animal is more likely to be biting, he knows that the \texttt{man} is doing the biting and the \texttt{dog} is getting bitten. Going the other way, there are many cases in which the meaning of a subphrase affects grammatical acceptability and structure.

\begin{example}[Exclamations: how meaning affects grammar]
The following examples from [Lakofflecture] illustrate how whether a phrase is an \emph{exclamation} affects what kinds of grammatical constructions are acceptable. By this argument, to know whether something is an exclamation in context is an aspect of meaning, so we have cases where meaning determines grammar. Observe first that the following three phrases are all grammatically acceptable and mean the same thing.
\[\texttt{\textcolor{blue}{nobody knows} how many beers Bob drinks}\]
\[\texttt{\textcolor{blue}{who knows} how many beers Bob drinks}\]
\[\texttt{\textcolor{blue}{God knows} how many beers Bob drinks}\]
The latter two are distinguished when \texttt{God knows} and \texttt{who knows} are exclamations. First, the modularity of grammar and meaning may not match when an exclamation is involved. For example, negating the blue text, we obtain:
\[\texttt{\textcolor{blue}{somebody knows} how many beers Bob drinks}\]
\[\texttt{\textcolor{blue}{who doesn't know} how many beers Bob drinks}\]
\[\texttt{\textcolor{purple}{God doesn't} know how many beers Bob drinks}\]
The first two are acceptable, but mean different things; the latter means to say that everyone knows how many beers Bob drinks, which is stronger than the former. The last sentence is awkward: unlike in the first two cases, the quantified variable in the (gloss) $\cdots \neg \exists x_{Person} \cdots$ of \texttt{God knows} is lost, and what is left is a literal reading $\cdots \neg \texttt{knows}(\texttt{God},\cdots) \cdots$.
Second, whether a sentence is grammatically acceptable may depend on whether an exclamation is involved. \texttt{\textcolor{blue}{God knows}} and \texttt{\textcolor{blue}{who knows}} can be shuffled into the sentence to behave as an intensifier as in:
\[\texttt{Bob drinks \textcolor{blue}{God knows} how many beers}\]
\[\texttt{Bob drinks \textcolor{blue}{who knows} how many beers}\]
But it is awkward to have:
\[\texttt{Bob drank \textcolor{purple}{nobody knows} how many beers}\]
And it is not acceptable to have:
\[\texttt{Bob drank \textcolor{red}{Alice knows} how many beers}\]
\end{example}



\end{fullwidth}