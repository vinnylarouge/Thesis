\section{Interpreting text circuits in \textbf{ContRel}}\label{subsec:toptext}

Recall that there were some mathematically odd choices in conventions for text circuits as syntactic objects, such as the labelling of noun wires and some subtleties with respect to interchange of parallel gates and text order. The explanation of those choices was deferred "to semantics", and since we're here now we have to make good. The aim of this section is to now explain those choices through an interpretation of text circuits in \textbf{ContRel}.

\subsection{Sticky spiders: iconic semantics of nouns}
Without loss of generality we may consider sticky spiders to be set-indexed collections of disjoint open subsets of an ambient space, i.e. a collection of shapes in space that a labelled from a set of names. So particular sticky spiders will be particular models of a collection of nouns; the indexing set names them, and the corresponding shapes in space are a particular iconic representation of those nouns in space.

\subsection{Open sets: concepts}

Apart from enabling us to paint pictures with words, \textbf{ContRel} is worth the trouble because the opens of topological spaces crudely model how we talk about concepts, and the points of a topological space crudely model instances of concepts. We consider these open-set tests to correspond to "concepts", such as redness or quickness of motion. Figure \ref{fig:pointing} generalises to a sketch argument that insofar as we conceive of concepts in (possibly abstractly) spatial terms, the meanings of words are modellable as shared strategies for spatial deixis; absolute precision is communicatively impossible, and the next best thing mathematically requires topology.

\begin{figure}[h!]\label{fig:pointing}
\[\resizebox{\textwidth}{!}{\tikzfig{topology/pointingfinger}}\]
\caption{Points in space are a useful mathematical fiction. Suppose we have a point on a unit interval. Consider how we might tell someone else about where this point is. We could point at it with a pudgy appendage, or the tip of a pencil, or give some finite decimal approximation. But in each case we are only speaking of a vicinity, a neighbourhood, an \emph{open set in the borel basis of the reals} that contains the point. Identifying a true point on a real line requires an infinite intersection of open balls of decreasing radius; an infinite process of pointing again and again, which nobody has the time to do. In the same way, most language outside of mathematics is only capable of offering successively finer, finite approximations.}
\end{figure}

This may explain the asymmetry of why tests are open sets, but why are states allowed to be arbitrary subsets? One could argue that states in this model represent what is conceived or perceived. Suppose we have an analog photograph whether in hand or in mind, and we want to remark on a particular shade of red in some uniform patch of the photograph. As in the case of pointing out a point on the real interval, we have successively finer approximations with a vocabulary of concepts: "red", "burgundy", "hex code \#800021"... but never the point in colourspace itself. If someone takes our linguistic description of the colour and tries to reproduce it, they will be off in a manner that we can in principle detect, cognize, and correct: "make it a little darker" or "add a little blue to it". That is to say, there are in principle differences in mind that we cannot distinguish linguistically in a finite manner; we would have to continue the process of "even darker" and "add a bit less blue than last time" forever. All this is just the mathematical formulation of a very common observation: sometimes you cannot do an experience justice with words, and you eventually give up with "I guess you just had to be there". Yet the experience is there and we can perform linguistic operations on it, and the states accommodate this.

\subsection{Configuration spaces: labelled noun wires}

Whereas sticky spiders correspond to particular iconic representations, we want to interpret text circuits in the configuration spaces of sticky spiders, so that text process-theoretically restricts, expands, and modifies a space of permissible iconic representations\footnote{\textbf{Top} is symmetric monoidal closed with respect to product, why we just work there from the start? Because \textbf{Top} is cartesian monoidal, which in particular means that there is only one test (the map into the terminal singleton topology), and worse, all states are tensor-separable. The latter fact means that we cannot reason natively in diagrams about correlated states, which are extremely useful representing entangled quantum states \citep{coecke_picturing_2017}, and for reasoning about spatial relations \citep{wang-mascianica_talking_2021}. I'll briefly explain the gist of the analogy in prose because it is already presented formally in the cited works and elaborated in \citep{coecke_compositionality_2021}. The Fregean notion of compositionality is roughly that to know a composite system is equivalent to knowing all of its parts, and diagrammatically this amounts to tensor-separability, which arises as a consequence of cartesian monoidality. Schr\"{o}dinger suggests an alternative of compositionality via a lesson from entangled states in quantum mechanics: \emph{perfect knowledge of the whole does not entail perfect knowledge of the parts.} Let's say we have information about a composite system if we can restrict the range of possible outcomes; this is the case for the bell-state, where we know that there is an even chance both qubits measure up or both measure down, and we can rule out mismatched measurements. However, discarding one entangled qubit from a bell-state means we only know that the remaining qubit has a 50/50 of measuring up or down, which is the minimal state of information we can have about a qubit. So we have a case where we can know things about the whole, but nothing about its parts. A more familiar example from everyday life is if I ask you to imagine \texttt{a cup on a table in a room}. There are many ways to envision or realise this scenario in your mind's eye, all drawn from a restricted set of permissable positions of the cup and the table in some room. The spatial locations of the cup and table are entangled, in that you can only consider the positions of both together. If you discard either the cup or the table from your memory, there are no restrictions about where the other object could be in the room; that is, the meaning of the utterance is not localised in any of the parts, it resides in the entangled whole.}. So it is via configuration spaces that we aim for an iconic semantics of general text. Recall from the definition of configuration spaces via split idempotents (Definition \ref{defn:configurationspace}) that the section and retract diagrammatically allow the configuration space wire to open up to guitar strings we can place our circuits in. However, this section and retract pair is not uniquely determined. For example, in the case of configuration spaces of rigid transformations of shapes, one obtains a family of encodings of the configuration space of an $n$-shape sticky spider in the $n$-fold tensor of isometry spaces by choosing a basepoint for each shape and rigidly transforming with respect to that basepoint, which on the plane may lead to different reifications of rotating shapes.
\[\scalebox{0.75}{\tikzfig{topology/labelledwires1}}\]
Interpreting text circuits with respect to a particular split idempotent of configuration space hence gives a good reason to label the noun wires: so that we can remember which space of isometries corresponds to which noun-shape in the absence of knowing how precisely the split idempotent encodes configuration space as the $n$-fold contribution of spatial possibilities for each noun.

\subsection{Copy: stative verbs and adjectives}

\emph{Stative} verbs are those that posit an unchanging state of affairs, such as \texttt{Bob \underline{likes} drinking}. Insofar as stative verbs are restrictions of all possible configurations to a permissible subset, they are conceptually similar to adjectives, such as \texttt{\underline{red} car}, which restricts permissible representations in colourspace. When we interpret concepts as open-set tests, \textbf{ContRel} conspires in our favour by giving us free copy maps on every wire. This allows us to define a family of processes that behave like stative restrictions of possibilities.

\begin{figure}[h!]
\[\resizebox{\textwidth}{!}{\tikzfig{topology/copygatecommute}}\]
\caption{
\begin{example}[Adjectives by analysis of configuration spaces]
The desirable property we obtain is that in the absence of \emph{dynamic} verbs that posit a change in the state of affairs, stative constructions commute in text: if I'm just telling you static properties of the way things are, it doesn't matter in what order I tell you the facts because restrictions commute. Recall that gates of the following form are intersections with respect to open sets, and they commute. These intersections model conjunctive specifications of properties.
\end{example}
}
\end{figure}

\begin{figure}[h!]
\[\resizebox{0.8\textwidth}{!}{\tikzfig{topology/oxfordex}}\]
\caption{
Consider the configuration space of a sticky spider on the unit square with three labelled shapes, which has 6 connected components, depicted. \texttt{Oxford contains Catz.} restricts away configurations where \texttt{Catz} is not enclosed in \texttt{Oxford}. Adding on \texttt{England contains Oxford.} further restricts away incongruent configurations, leaving us only with a single connected component, which contains all spatial configurations that satisfy the text. A similar story holds for abstract conceptual spaces, in which \texttt{fast red car}, \texttt{fast car that is red}, \texttt{car is (red and fast)} all mean the same thing.
}
\end{figure}

\clearpage

\subsection{Homotopies: dynamic verbs and weak interchange}

\begin{figure}[h!]
\[\resizebox{\textwidth}{!}{\tikzfig{topology/collisionparticular}}\]
\caption{
\emph{Dynamic} verbs are those that posit a change in state, such as \texttt{Bob \underline{goes} home}. We want to model these verbs by homotopies, where the unit interval parameter models time. A nice and diagrammatically immediate property is that dynamic verbs are obstacles to the commutation of stative words.
\begin{example}[Dynamic verbs block stative commutations]
States of affairs can change after motion. A simple example is the case of \emph{collision}, where two shapes start off not touching, and then they move rigidly towards one another to end up touching.
\end{example}
}
\end{figure}

\begin{figure}[h!]
\[\resizebox{\textwidth}{!}{\tikzfig{topology/collideinterior}}\]
\caption{
Recalling that homotopies between relations are the unions of homotopies between maps, we have a homotopy that is the union of all collision trajectories, which we mark $\textcolor{orange}{\forall}$. We my define the interior $i(\textcolor{orange}{\forall})$ as the concept of collision; the expressible collection of all particular collisions. But this is not just an open set on the potential configuration of shapes, it is a collection of open sets parameterised by homotopy.
}
\end{figure}

\begin{figure}[h!]
\[\resizebox{\textwidth}{!}{\tikzfig{topology/collisionex}}\]
\caption{\texttt{Collision} blocks the commutation of the statives \texttt{touching} and \texttt{not touching}. In prose, the text \texttt{Alice and Bob are not touching. Alice and Bob collide. Alice and Bob are touching.} is not equal to \texttt{Alice and Bob are (touching and not touching). Alice and Bob collide.}; the latter is an incongruent state of affairs, which is reflected by an empty set of potential models (when \texttt{touching} and \texttt{not touching} intersect.)}
\end{figure}

\begin{figure}
\[\resizebox{\textwidth}{!}{\tikzfig{topology/seqparhom}}\]
\caption{
We can compose multiple motions in parallel by copying the unit interval, allowing it to parameterise multiple gates simultaneously, or compose them sequentially. The sequential composition of dynamic verbs in time explains the weak-interchange subtlety of text diagrams; there is now a diagrammatic distinction between events happening sequentially and in parallel. So now we have noncommuting gates that model \emph{actions}, or verbs.
}
\end{figure}

\begin{figure}[h!]
\[\resizebox{\textwidth}{!}{\tikzfig{topology/inopenout}}\]
\caption{What kinds of actions are there? In our toy setting, in general we can define actions that arbitrarily change states of affairs if we do not restrict ourselves to rigid motions. The trick to doing this is the observation that arbitrary homotopies allow deformations, so our verb gates allow shapes to shrink and open and bend in the process of a homotopy, as long as at the end they arrive at a rigid displacement of their original form.}
\end{figure}

\begin{figure}[h!]
\[\resizebox{0.8\textwidth}{!}{\tikzfig{topology/breaklock}}\]
\caption{
We can further generalise by noting that completely different spiders can be related by homotopy, so we can model a situation where there is a permanent bend, or how a rigid shape might shatter. Construction \ref{cons:morph} shows that dynamic verbs can be constructed to respect a broad range of pre- and postconditions expressed in terms of statives.
}
\end{figure}

\clearpage

\subsection{Coclosure: adverbs and adpositions}

\begin{figure}[h!]
\[\scalebox{0.9}{\tikzfig{topology/adverbex1}}\]
\caption{
Recall that \textbf{ContRel} is coclosed (Proposition \ref{prop:coclosure}), which means that every dynamic verb may be expressed as the composite of a coevaluator and an open set on the space of homotopies. For instance, \texttt{move} is an intransitive dynamic verb, which corresponds to a concept in the space of all movements.
}
\end{figure}

\begin{figure}[h!]
\[\scalebox{0.9}{\tikzfig{topology/adverbex2}}\]
\caption{
Adverb-boxes may be modelled as static restrictions in movement-space. For instance, \texttt{straight} may restrict movements to just those that satisfy some notion of path-length minimality: e.g., given a metric in movement-space on path-lengths, we may construct an open ball (Definition \ref{def:openball}) around the geodesic to model the adverb \texttt{straight}.
}
\end{figure}

\begin{figure}[h!]
\[\scalebox{0.9}{\tikzfig{topology/adverbex3}}\]
\caption{
Similarly, adposition-boxes may be modelled as static restrictions on the product of the spaces of nouns and verbs. For instance, \texttt{towards} may be modelled as an open set that pairs potential positions of the thing-being-moved-towards with movements in movement-space that indeed move towards the target.
}
\end{figure}

\clearpage

\subsection{Turing objects: sentential complementation}

Recall that we also have to explain how untyped-boxes for conjunction and sentential complementation get their semantics. One option for finite models we present here relies on an interesting property of \textbf{ContRel}, which allows \textbf{ContRel} to model untyped-boxes, as well as the linguistic phenomena of \emph{entification} and \emph{general anaphora}. This is because \textbf{ContRel} contains \textbf{FinRel} equipped with a \emph{Turing object}.

\newthought{Entification is the process of turning words and phrases that aren't nouns into nouns.} We are familiar with morphological operations in English, such as \emph{inflections} that turn the singular \texttt{cat} into the plural \texttt{cats}, by adding a suffix \texttt{-s}. Another morphological operation, generally classed \emph{derivation}, turns words from one category into another, for example the adjective \texttt{happy} into the noun \texttt{happiness}. With suffixes such as \texttt{-ness} and \texttt{-ing}, just about any lexical word in English can be turned into a noun, as if lexical words have some semantic content that is independent of the grammatical categories they might wear.

\begin{figure}[h!]
\[\tikzfig{spatialencoding/jono}\]
\caption{
\begin{example}
\[\texttt{Jono was paid minimum wage but he didn't mind \underline{it}}\]
It may be argued that \texttt{it} refers to \texttt{the fact that Jono was paid minimum wage}. Graphically, we might want to depict the gloss as a circuit with a lasso that gives another noun-wire. I'll also call this process \emph{entification}, which extends beyond morphology towards more complicated constructions such as a prefix \texttt{the fact that} that converts sentences into noun-like entities, insofar as these entities are valid anaphoric referents.
\end{example}
}
\end{figure}

\newthought{A mathematical modelling problem for semanticists arises when anything can be a noun wire.} The problem at hand is finding the right mathematical setting to interpret and calculate with such lassos. In principle, any meaningful (possibly composite) part of text can be referred to as if it were a noun. For syntax, this is a boon; having entification around means that there is no need to extend the system to accommodate wires for anything apart from nouns, so long as there is a gadget that can turn anything into a noun and back. For semantics this is a challenge, since this requires noun-wires to "have enough space in them" to accommodate full circuits operating on other noun-wires, which suggests a very structured sort of infinity.

\newthought{Computer science has had a perfectly serviceable model of this kind of noun-wire for a long time.} What separates a computer from other kinds of machine is that a computer can do whatever any other kind of machine could do -- modulo church-turing on computability and the domain of data manipulation -- so long as the computer is running the right \emph{program}. Programs are (for our purposes) processes that manipulate variously formatted -- or typed -- data, such as integers, sounds, and images. They can operate in sequence and in parallel, and wires can be swapped over each other, so programs form a process theory, where we can reason about the extensional equivalence of different programs -- whether two programs behave the same with respect to mapping inputs to outputs. What makes computer programs special is that on real computers, they are specified by \emph{code}.  Programs that are equivalent in their extensional behavior may have many different implementations in code: for example, there are many sorting algorithms, though all of them map the same inputs to the same outputs. Conversely, every possible program in a process theory of programs must have some implementation as code. Importantly, code is just another format of data. So if the code-type is just another wire in the process-theory of computation, what is its process-theoretic characterisation?

\begin{figure}[h!]
\[
\forall A,B \in Ob(\mathcal{C}) \ \exists \text{ev}^{ A}_{B}: A \otimes \Xi \rightarrow B \  \forall f : A \rightarrow B \ \exists \ulcorner f \urcorner : I \rightarrow \Xi\]
\[
\tikzfig{spatialencoding/eval}
\]
\caption{
\begin{defn}[Turing object]\label{defn:turing}
Diagrammatically, we would summarise the situation like this: for every pair of input formats and output formats ($A,B$), there is a computer for that format $ev^{A}_{B}: A \otimes \Xi \rightarrow B$, which takes code-format (which we will just denote $\Xi$ going forward) as an additional input, and for every possible program $f: A \rightarrow B$, there exists a code-state $\ulcorner f \urcorner : I \rightarrow \Xi$ that evaluates to the desired program.
\end{defn}
}
\end{figure}

It would be true but unhelpful to conclude that any programming language is a model for text circuits, using the code data format as the noun wire. In \textbf{ContRel}, the unit square suffices.

\begin{scholium}
Another observation we could have made is that since computers really just manipulate code, every data format is a kind of restricted form of the same code object $\Xi$, but this turns out to be a mathematical consequence of the above equation (and the presence of a few other operations such as copy and compare that form a variant of frobenius algebra), demonstrated in Pavlovic's forthcoming monoidal computer book \citep{pavlovic_programs_2023}, itself a crystallisation of three monoidal computer papers \citep{pavlovic_monoidal_2012,pavlovic_monoidal_2014,pavlovic_monoidal_2018}. I would be remiss to leave out Cockett's work on Turing categories \citep{cockett_introduction_2008}, from which I took the name Turing object. Both approaches to a categorical formulation of computability theory share the common starting ground of a special form of closure (monoidal closure in the case of monoidal computer and exponentiation in Turing categories) where rather than having dependent exponential types $\mathbb{A} \multimap \mathbb{B}$ or $\mathbb{B}^\mathbb{A}$, there is a single "code-object" $\Xi$. They differ in the ambient setting; Pavlovic works in the generic symmetric monoidal category, and Cockett with cartesian restriction categories, which generalise partial functions. I work with Pavlovics' formalism because I prefer string diagrams to commuting diagrams.
\end{scholium}

\marginnote{
\begin{construction}[Sticky spiders on the open unit square model the category relations between countable sets equipped with a code object]
Using the open unit square with its usual topology as the code object, there is a subcategory of \textbf{ContRel} which behaves as the category of countable sets and relations equipped with a Turing object (Construction \ref{cons:unitencoding}.)
\end{construction}
}

\clearpage

\begin{myboxB}
\begin{proposition}[$(0,1) \times (0,1)$ splits through any countable set $X$]
For any countable set $X$, the open unit square $\squarehvfill$ has a sticky spider that splits through $X^\star$.
\begin{proof}
The proof is by construction. We'll assume the sticky-spiders to be mereologies, so that cores and halos agree. Then we only have to highlight the copiable open sets. Take some circle and place axis-aligned open squares evenly along them, one for each element of $X$. The centres of the open squares lie on the circumference of the circle, and we may shrink each square as needed to fit all of them.
\[\scalebox{1}{\tikzfig{spatialencoding/circencodingconstruct}}\]
\end{proof}
\end{proposition}
\end{myboxB}

\begin{myboxB}
\begin{defn}[Morphism of sticky spiders]
A morphism between sticky spiders is any morphism that satisfies the following equation.
\[\scalebox{1}{\tikzfig{spatialencoding/stickymorphismdefn}}\]
\end{defn}
\end{myboxB}

\begin{myboxB}
\begin{proposition}[Morphisms of sticky spiders encode relations]
For arbitrary split idempotents through $A^\star$ and $B^\star$, the morphisms between the two resulting sticky spiders are in bijection with relations $R: A \rightarrow B$.
\[\scalebox{1}{\tikzfig{spatialencoding/arbsetclaim}}\]
\begin{proof}
\[\scalebox{1}{\tikzfig{spatialencoding/arbset}}\]
\end{proof}
\end{proposition}
\end{myboxB}

\begin{myboxB}
\begin{construction}[Representing sets in their various guises within $\squarehvfill$]
We can represent the direct sum of two $\squarehvfill$-representations of sets as follows.
\[\scalebox{0.75}{\tikzfig{spatialencoding/directsumconstruct}}\]
The important bit of technology is the homeomorphism that losslessly squishes the whole unit square into one half of the unit square. The decompressions are partial continuous functions, with domain restricted to the appropriate half of the unit square.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions}}\]
We express the ability of these relations to encode and decode the unit square in just either half by the following graphical equations.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions2}}\]
\end{construction}
Now, to put the two halves together and to take them apart, we introduce the following two relations. In tandem with the squishing and stretching we have defined, these will behave just as the projections and injections for the direct-sum biproduct in \textbf{Rel}.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions3}}\]
The following equation tells us that we can take any two representations in $\squarehvfill$, put them into a single copy of $\squarehvfill$, and take them out again.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions4}}\]
\end{myboxB}

\begin{myboxB}
We encode the tensor product $A \otimes B$ of representations by placing copies of $B$ in each of the open boxes of $A$.
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap}}\]
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap2}}\]
\[\scalebox{0.75}{\tikzfig{spatialencoding/tensorconstruct}}\]
The important bit of technology here is a family of homeomorphisms of $\squarehvfill$ parameterised by axis-aligned open boxes, that allow us to squish and stretch spaces. Thus for every representation of a set in $\squarehvfill$ by a sticky-spider, where each element corresponds to an axis-aligned open box, we can associate each element with a squish-stretch homeomorphism via the parameters of the open box, which we notate with a dot above the name of the element.
\[\scalebox{0.75}{\tikzfig{spatialencoding/compresstogether}}\]
\begin{remark}
The essential idea is that the whole of the unit square is homeomorphic to part of it. In particular this means (modulo a point), we may make copies of shapes outside a container that are in homeomorphic correspondence with shapes within a container, the classic example being thought bubbles in a comic-strip with pictorial contents of the outside world. In our framework, this constitutes a formal and well-typed semantics for certain alethic verbs of cognition such as \texttt{sees} and \texttt{thinks}. For other such verbs, one may by Construction \ref{cons:unitencoding} operate directly on set-theoretic representations of mental contents.
\end{remark}
\end{myboxB}

\begin{myboxB}
Now we can define the "tensor $X$ on the left" relation $\_ \rightarrow X \otimes \_$ and its corresponding cotensor.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/tensordetensor}}\]
The tensor and cotensor behave as we expect from proof nets for monoidal categories.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor2}}\]
And by construction, the (co)tensors and (co)pluses interact as we expect, and they come with all the natural isomorphisms between representations we expect. For example, below we exhibit an explicit associator natural isomorphism between representations.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor3}}\]
\end{myboxB}

\begin{myboxB}
\begin{construction}[Representing relations between sets and their composition within $\squarehvfill$]\label{cons:unitencoding}
With all the above, we can establish a special kind of process-state duality; relations as processes are isomorphic to states of $\squarehvfill$, up to the representation scheme we have chosen. This is part of the condition for Turing objects. What remains to be demonstrated is that the duality coheres with sequential and parallel relational composition.
\end{construction}
\[\scalebox{0.9}{\tikzfig{spatialencoding/relcomp1}}\]
Under this duality, we have continuous relations that perform sequental composition of relations as follows.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/relcomp2}}\]
And similarly, parallel composition. Therefore, we have demonstrated that the unit square behaves as a Turing object for the category of countable sets and relations.
\[\resizebox{\textwidth}{!}{\tikzfig{spatialencoding/relcomp3}}\]
\end{myboxB}
