\section{On entification, general anaphora, computers, and lassos.}

\newthought{Entification is the process of turning words and phrases that aren't nouns into nouns.} We are familiar with morphological operations in English, such as \emph{inflections} that turn the singular \texttt{cat} into the plural \texttt{cats}, by adding a suffix \texttt{-s}. Another morphological operation, generally classed \emph{derivation}, turns words from one category into another, for example the adjective \texttt{happy} into the noun \texttt{happiness}. With suffixes such as \texttt{-ness} and \texttt{-ing}, just about any lexical word in English can be turned into a noun, as if lexical words have some semantic content that is independent of the grammatical categories they might wear. I'll call this process \emph{entification}, which extends beyond morphology towards more complicated constructions such as a prefix \texttt{the fact that} that converts sentences into noun-like entities, insofar as these entities can be referred to by anaphora: for example, in the sentence \texttt{Jono was paid minimum wage but he didn't mind \underline{it}}, it may be argued that \texttt{it} refers to \texttt{the fact that Jono was paid minimum wage}. Graphically, we might want to depict the gloss as a circuit with a lasso that gives another noun-wire:

\[\tikzfig{spatialencoding/jono}\]

\newthought{A mathematical modelling problem for semanticists arises when anything can be a noun wire.} The problem at hand is finding the right mathematical setting to interpret and calculate with such lassos. In principle, any meaningful (possibly composite) part of text can be referred to as if it were a noun. For syntax, this is a boon; having entification around means that there is no need to extend the system to accommodate wires for anything apart from nouns, so long as there is a gadget that can turn things into nouns and back. For semantics this is a challenge, since this requires noun-wires to "have enough space in them" to accommodate full circuits operating on other noun-wires, which suggests a very structured sort of infinity.

\newthought{Computer science has had a perfectly serviceable model of this kind of noun-wire for a long time.} What separates a computer from other kinds of machine is that a computer can do whatever any other kind of machine could do -- modulo church-turing on computability and the domain of data manipulation -- so long as the computer is running the right \emph{program}. Programs are (for our purposes) processes that manipulate variously formatted -- or typed -- data, such as integers, sounds, and images. They can operate in sequence and in parallel, and wires can be swapped over each other, so programs form a process theory, where we can reason about the extensional equivalence of different programs -- whether two programs behave the same with respect to mapping inputs to outputs. This aim of reasoning about programs in an implementation-independent fashion contributed to the birth of computer \emph{science} from programming, which was attended by the independent discovery of proto-string-diagrams in the form of flowcharts.

\[placeholder\]

\newthought{What makes computer programs special is that on real computers, they are specified by code}.} Code is just another format of data. Programs that are equivalent in their extensional behavior may have many different implementations in code: for example, there are many sorting algorithms, though all of them map the same inputs to the same outputs. Conversely, every possible program in a process theory of programs must have some implementation as code. Diagrammatically, we would summarise the situation like this: for every pair of input formats and output formats ($\mathbb{A},\mathbb{B}$), there is a computer for that format $evˆ{\mathbb{A}}_{\mathbb{B}}: \mathbb{A} \otimes \Xi \rightarrow \mathbb{B}$, which takes code-format (which we will just denote $\Xi$ going forward) as an additional input, and for every possible program $f: \mathbb{A} \rightarrow \mathbb{B}$, there exists a state $\ulcorner f \urcorner : I \rightarrow \Xi$ such that:

\begin{align}
\forall \mathbb{A},\mathbb{B} \in Ob(\mathcal{C}) \exists \mathtext{ev}ˆ{ \mathbb{A}}_{\mathbb{B}}: \mathbb{A} \otimes \Xi \rightarrow \mathbb{B} \forall f : \mathbb{A} \exists \ulcorner f \urcorner : I \rightarrow \Xi \\
\tikzfig{spatialencoding/eval}
\end{align}

The above equation, which characterises computers as code-evaluators, provides a plan of attack for the semantic modelling problem of entification: if we take noun-wires to be the code object in a monoidal computer, we have restricted the candidate symmetric monoidal categories to model text-circuits in a way that allows for entification.

\begin{scholium}
Another observation we could have made is that since computers really just manipulate code, every data format is a kind of restricted form of the same code object $\Xi$, but this turns out to be a mathematical consequence of the above equation (and the presence of a few other operations such as copy and compare that form a variant of frobenius algebra), demonstrated in Pavlovic's forthcoming monoidal computer book [], itself a crystallisation of three monoidal computer papers []. I would be remiss to leave out Cockett's work on Turing categories []. Both approaches to a categorical formulation of computability theory share the common starting ground of a special form of closure (monoidal closure in the case of monoidal computer and exponentiation in Turing categories) where rather than having dependent exponential types $\mathbb{A} \multimap \mathbb{B}$ or $\mathbb{B}ˆ\mathbb{A}$, there is a single "code-object" $\Xi$. They differ in the ambient setting; Pavlovic works in the generic symmetric monoidal category, and Cockett with cartesian restriction categories, which generalise partial functions. I work in Pavlovics' formalism because I prefer string diagrams to commuting diagrams.
\end{scholium}

It would be true but unhelpful to conclude that any programming language is a model for text circuits, using the code data format as the noun wire. Since semanticists like to work with sets, I provide the following construction.

\begin{construction}[Sticky spiders on the open unit square model the category relations between countable sets equipped with a code object]
Using the open unit square with its usual topology as the code object, there is a subcategory of \textbf{ContRel} which behaves as the category of countable sets and relations equipped with universal evaluators.
\end{construction}

\begin{proposition}[$(0,1) \times (0,1)$ splits through any countable set $X$]
For any countable set $X$, the open unit square $\squarehvfill$ has a sticky spider that splits through $X^\star$.
\begin{proof}
The proof is by construction. We'll assume the sticky-spiders to be mereologies, so that cores and halos agree. Then we only have to highlight the copiable open sets. Take some circle and place axis-aligned open squares evenly along them, one for each element of $X$. The centres of the open squares lie on the circumference of the circle, and we may shrink each square as needed to fit all of them.
\[\scalebox{1}{\tikzfig{spatialencoding/circencodingconstruct}}\]
\end{proof}
\end{proposition}

\begin{defn}[Morphism of sticky spiders]
A morphism between sticky spiders is any morphism that satisfies the following equation.
\[\scalebox{1}{\tikzfig{spatialencoding/stickymorphismdefn}}\]
\end{defn}

\begin{proposition}[Morphisms of sticky spiders encode relations]
For arbitrary split idempotents through $A^\star$ and $B^\star$, the morphisms between the two resulting sticky spiders are in bijection with relations $R: A \rightarrow B$.
\[\scalebox{1}{\tikzfig{spatialencoding/arbsetclaim}}\]
\begin{proof}
\[\scalebox{1}{\tikzfig{spatialencoding/arbset}}\]
\end{proof}
\end{proposition}

\begin{construction}[Representing sets in their various guises within $\squarehvfill$]
We can represent the direct sum of two $\squarehvfill$-representations of sets as follows.
\[\scalebox{1}{\tikzfig{spatialencoding/directsumconstruct}}\]
The important bit of technology is the homeomorphism that losslessly squishes the whole unit square into one half of the unit square. The decompressions are partial continuous functions, with domain restricted to the appropriate half of the unit square.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions}}\]
We express the ability of these relations to encode and decode the unit square in just either half by the following graphical equations.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions2}}\]
Now, to put the two halves together and to take them apart, we introduce the following two relations. In tandem with the squishing and stretching we have defined, these will behave just as the projections and injections for the direct-sum biproduct in \textbf{Rel}.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions3}}\]
The following equation tells us that we can take any two representations in $\squarehvfill$, put them into a single copy of $\squarehvfill$, and take them out again. Banach and Tarski would approve.
\[\scalebox{1}{\tikzfig{spatialencoding/leftrightcompressions4}}\]
We encode the tensor product $A \otimes B$ of representations by placing copies of $B$ in each of the open boxes of $A$.
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap}}\]
%\[\scalebox{0.75}{\tikzfig{spatialencoding/directsummap2}}\]
\[\scalebox{1}{\tikzfig{spatialencoding/tensorconstruct}}\]
The important bit of technology here is a family of homeomorphisms of $\squarehvfill$ parameterised by axis-aligned open boxes. We depict the parameters outside the body of the homeomorphism for clarity. The squish is on the left, the stretch on the right.
\[\scalebox{1}{\tikzfig{spatialencoding/boxcompression}}\]
Now, for every representation of a set in $\squarehvfill$ by a sticky-spider, where each element corresponds to an axis-aligned open box, we can associate each element with a squish-stretch homeomorphism via the parameters of the open box, which we notate with a dot above the name of the element.
\[\scalebox{1}{\tikzfig{spatialencoding/obtainboxposition}}\]
Now we can define the "tensor $X$ on the left" relation $\_ \rightarrow X \otimes \_$ and its corresponding cotensor.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor}}\]
The tensor and cotensor behave as we expect from proof nets for monoidal categories.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor2}}\]
And by construction, the (co)tensors and (co)pluses interact as we expect, and they come with all the natural isomorphisms between representations we expect. For example, below we exhibit an explicit associator natural isomorphism between representations.
\[\scalebox{1}{\tikzfig{spatialencoding/tensordetensor3}}\]
\end{construction}

\begin{construction}[Representing relations between sets and their composition within $\squarehvfill$]
With all the above, we can establish a special kind of process-state duality; relations as processes are isomorphic to states of $\squarehvfill$, up to the representation scheme we have chosen.
\[\scalebox{1}{\tikzfig{spatialencoding/relcomp1}}\]
Moreover, we have continuous relations that perform sequental composition of relations.
\[\scalebox{1}{\tikzfig{spatialencoding/relcomp2}}\]
And we also know how to take the parallel composition of relations by tensors.
\[\scalebox{1}{\tikzfig{spatialencoding/relcomp3}}\]
\end{construction}