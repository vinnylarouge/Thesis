\section{Process Theories}

\marginnote{
    There are a lot of definitions to get started, but as with programming languages, preloading the work makes it easier to scale. This is the mathematical source code of string diagrams, which is only necessary if we need to show that something new is a symmetric monoidal category or if we are tinkering deeply, so it can be skipped for now. The important takeaway is that string diagrams are syntax, with morphisms in symmetric monoidal categories as semantics.

\begin{defn}[Category]
A \emph{category} $\mathcal{C}$ consists of the following data
\begin{itemize}
\item{A collection $\text{Ob}(\mathcal{C})$ of \emph{objects}}
\item{For every pair of objects $A,B \in \text{Ob}(\mathcal{C})$, a set $\mathcal{C}(A,B)$ of \emph{morphisms} from $a$ to $b$.}
\item{Every object $a \in \text{Ob}(\mathcal{C})$ has a specified morphism $1_a$ in $\mathcal{C}(a,a)$ called \emph{the identity morphism} on $a$.}
\item{Every triple of objects $A,B,C \in \text{Ob}(\mathcal{C})$, and every pair of morphisms $f \in \mathcal{C}(A,B)$ and $g \in \mathcal{C}(b,c)$ has a specified morphism $(f;g) \in \mathcal{C}(a,c)$ called \emph{the composite} of $f$ and $g$.}
\end{itemize}
This data has to satisfy two coherence conditions, which are:

\newthought{Unitality:} For any morphism $f: a \rightarrow b$, $1_a;f = f = f;1_b$

\newthought{Associativity:} For any four objects $A,B,C,D$ and three morphisms $f: A \rightarrow B$, $g: B \rightarrow C$, $h: C \rightarrow D$, $(f;g);h = f;(g;h)$
\end{defn}
}

\marginnote{
\begin{defn}[Categorical Product]
In a category $\mathcal{C}$, given two objects $a,b \in \text{Ob}(\mathcal{C})$, the \emph{product} $A \times B$, if it exists, is an object with projection morphisms $\pi_0: A \times B \rightarrow A$ and $\pi_1: A \times B \rightarrow B$ such that for any object $x \in \text{Ob}(\mathcal{C})$ and any pair of morphisms $f: X \rightarrow A$ and $g: x \rightarrow b$, there exists a unique morphism $f \times g: X \rightarrow A \times B$ such that $f = (f \times g) ; \pi_0$ and $g = (f \times g); \pi_1$. This is a mouthful which is easier expressed as a commuting diagram as below. The dashed arrow indicates uniqueness. $A \times B$ is a product when every path through the diagram following the arrows between two objects is an equality.
\[\begin{tikzcd}[ampersand replacement=\&]
	\&\& x \\
	\\
	a \&\& {a \times b} \&\& b
	\arrow["{\langle f,g \rangle}"{description}, dashed, from=1-3, to=3-3]
	\arrow["f"{description}, from=1-3, to=3-1]
	\arrow["g"{description}, from=1-3, to=3-5]
	\arrow["{\pi_1}"{description}, from=3-3, to=3-5]
	\arrow["{\pi_0}"{description}, from=3-3, to=3-1]
\end{tikzcd}\]
\end{defn}
}

\marginnote{
The idea behind the definition of product is simple: instead of explicitly constructing the cartesian product of sets from within, let's say \emph{a product is as a product does}. For objects, the cartesian product of sets $A \times B$ is a set of pairs, and we may destruct those pairs by extracting or projecting out the first and second elements, hence the projection maps $\pi_0,\pi_1$. Another thing we would like to do with pairs is construct them; whenever we have some $A$-data and $B$-data, we can pair them in such a way that construction followed by destruction is lossless and doesn't add anything. In category-theoretic terms, we select `arbitrary' $A$- and $B$-data by arrows $f: X \rightarrow A$ and $g: X \rightarrow B$, and we declare that $f \times g: X \rightarrow A \times B$ is the unique way to select corresponding tuples in $A \times B$. This design-pattern of "for all such-and-such there exists a unique such-and-such" is an instance of a so-called \emph{universal property}, the purpose of which is to establish isomorphism between operationally equivalent implementations.
}

\marginnote{
To understand what this style of definition give us, let's revisit Kuratowski's and Wiener's definitions of cartesian product, which are, respectively:
\[A \overset{K}{\times} B := \bigg\{ \{\{a\},\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
\[A \overset{W}{\times} B := \bigg\{ \{\{a,\varnothing\},b\} \ | \ a \in A \ , \ b \in B \bigg\}\]
Keeping overset-labels and using maplet notation, the associated projections are:
\[\overset{K}{\pi_0} := \{\{a\},\{a,b\}\} \mapsto a\]
\[\overset{K}{\pi_1} := \{\{a\},\{a,b\}\} \mapsto b\]
\[\overset{W}{\pi_0} := \{\{a,\varnothing\},b\} \mapsto a\]
\[\overset{W}{\pi_1} := \{\{a,\varnothing\},b\} \mapsto b\]
And maps $f,g$ into $A$ and $B$ are tupled by the following:
\[f \overset{K}{\times} g := x \mapsto \{\{f(x)\},\{f(x),g(x)\}\}\]
\[f \overset{W}{\times} g := x \mapsto \{\{f(x),\varnothing\},g(x)\}\]
Both satisfy the commutative diagram defining the product.
Something neat happens when we pick $A \overset{K}{\times} B$ to be the arbitrary $X$ for the product definition of $A \overset{W}{\times} B$ and vice versa. We get to mash the commuting diagrams together:
% https://q.uiver.app/?q=WzAsNSxbNCwxLCJCIl0sWzAsMSwiQSJdLFsyLDAsIkEgXFxvdmVyc2V0e0t9e1xcdGltZXN9IEIiXSxbMSwzLCJBIFxcb3ZlcnNldHtXfXtcXHRpbWVzfSBCIl0sWzMsMywiQSBcXG92ZXJzZXR7V317XFx0aW1lc30gQiJdLFsyLDEsIlxcb3ZlcnNldHtLfXtcXHBpXzB9IiwxXSxbMiwwLCJcXG92ZXJzZXR7S317XFxwaV8xfSIsMV0sWzMsMCwiXFxvdmVyc2V0e1d9e1xccGlfMX0iLDFdLFszLDEsIlxcb3ZlcnNldHtXfXtcXHBpXzB9IiwxXSxbMiwzLCIiLDEseyJjdXJ2ZSI6LTIsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dLFszLDIsIiIsMSx7ImN1cnZlIjotMiwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzQsMSwiXFxvdmVyc2V0e1d9e1xccGlfMH0iLDEseyJsYWJlbF9wb3NpdGlvbiI6ODB9XSxbNCwwLCJcXG92ZXJzZXR7V317XFxwaV8xfSIsMV0sWzMsNCwiaWQiLDEseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJhcnJvd2hlYWQifSwiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV1d
\[\begin{tikzcd}[ampersand replacement=\&]
	\&\& {A \overset{K}{\times} B} \\
	A \&\&\&\& B \\
	\\
	\& {A \overset{W}{\times} B} \&\& {A \overset{W}{\times} B}
	\arrow["{\overset{K}{\pi_0}}"{description}, from=1-3, to=2-1]
	\arrow["{\overset{K}{\pi_1}}"{description}, from=1-3, to=2-5]
	\arrow["{\overset{W}{\pi_1}}"{description}, from=4-2, to=2-5]
	\arrow["{\overset{W}{\pi_0}}"{description}, from=4-2, to=2-1]
	\arrow[curve={height=-12pt}, dashed, from=1-3, to=4-2]
	\arrow[curve={height=-12pt}, dashed, from=4-2, to=1-3]
	\arrow["{\overset{W}{\pi_0}}"{description, pos=0.8}, from=4-4, to=2-1]
	\arrow["{\overset{W}{\pi_1}}"{description}, from=4-4, to=2-5]
	\arrow["id"{description}, dashed, tail reversed, from=4-2, to=4-4]
\end{tikzcd}\]
}

\marginnote{
The two unique arrows between $\overset{K}{\times}$ and $\overset{W}{\times}$ are format-conversions, and we know by definition that the unique arrow that performs format conversion from $\overset{W}{\times}$ to itself in the bottom face is the identity. In maplet notation, the conversion from $A \overset{K}{\times} B \rightarrow A \overset{W}{\times} B$ is $\{\{a\},\{a,b\}\} \mapsto \{\{a,\varnothing\},b\}$, and similarly for the other direction. Because these conversions are uniquely determined arrows, their composite is also uniquely determined, and we know their composite is equal to the identity. So, the nontrivial conversions witness an \emph{isomorphism} between $A \overset{K}{\times} B$ and $A \overset{W}{\times} W$; a pair of maps $X \rightarrow Y$ and $Y \rightarrow X$ such that their loop-composites equal identities. This in a nutshell is the category-theoretic approach to overcoming the bureaucracy of syntax: use universal properties (or whatever else) encode your intents and purposes, establish isomorphisms, and then treat isomorphic things as "the same for all intents and purposes". The idea of treating isomorphic objects as the same is ingrained in category theory, so isomorphism notation $\simeq$ is often just written as equality $=$; going forward we will use equality notation unless there are good reasons to remember that we only have isomorphisms.
}

\marginnote{
\begin{defn}[Functor]
A functor $F: \mathcal{C} \rightarrow \mathcal{D}$ (read: with domain a category $\mathcal{C}$ and codomain a category $\mathcal{D}$) consists of two suitably related functions. An object function $F_0 : \text{Ob}(\mathcal{C}) \rightarrow \text{Ob}(\mathcal{D})$ and a morphism function (equivalently viewed as a family of functions indexed by pairs of objects of $\mathcal{C}$) $F_1(X,Y) : \mathcal{C}(X,Y) \rightarrow \mathcal{D}(F_0 X,F_0 Y )$. $F_1$ must map identities to identities -- i.e., be such that for all $X \in \mathcal{C}$, $F_1(1_X) = 1_{F_0 X}$ -- and $F_1$ must map composites to composites -- i.e., for all $X,Y,Z \in \text{Ob}(\mathcal{C})$ and all $f: X \rightarrow Y$ and $g: Y \rightarrow Z$, $F_1 (f;g) = F_1 f ; F_1 g$.
\end{defn}

Functors in short map categories to categories, preserving the structure of identities and composition. They are the essence of "structure preserving transformation". Insofar as semantics is the science of finding structure-preserving transformations that tell us when syntactic things are equal, functors are just that. They are incredibly useful and mysterious and worth internalising in a way I am not adept enough to impress by example in this margin. For us, for now, they are just stepping stones to define transformations \emph{between functors}.
}

\marginnote{
\begin{defn}[Natural Transformation]
A natural transformation $\theta: F \rightarrow G$ for (co)domain-aligned functors $F,G: \mathcal{C} \rightarrow \mathcal{D}$ is a family of morphisms in $\mathcal{D}$ indexed by objects $X \in \mathcal{C}$ such that for all $f: X \rightarrow Y$ in $\mathcal{C}$, the following commuting diagram holds in $\mathcal{D}$:
% https://q.uiver.app/?q=WzAsNCxbMCwwLCJGWCJdLFsyLDAsIkZZIl0sWzAsMiwiR1giXSxbMiwyLCJHWSJdLFswLDIsIlxcdGhldGFfWCIsMV0sWzAsMSwiRmYiLDFdLFsxLDMsIlxcdGhldGFfWSIsMV0sWzIsMywiR2YiLDFdXQ==
\[\begin{tikzcd}[ampersand replacement=\&]
	FX \&\& FY \\
	\\
	GX \&\& GY
	\arrow["{\theta_X}"{description}, from=1-1, to=3-1]
	\arrow["Ff"{description}, from=1-1, to=1-3]
	\arrow["{\theta_Y}"{description}, from=1-3, to=3-3]
	\arrow["Gf"{description}, from=3-1, to=3-3]
\end{tikzcd}\]
\end{defn}
}

\marginnote{
\begin{defn}[$\mathbf{Cat}$]
$\mathbf{Cat}$ is a (2-)category where the objects are (1-)categories as defined above, the morphisms are functors, and the (2-)morphisms are natural transformations. (2-)morphisms are morphisms between morphisms that we discuss in more detail in Section \ref{ncat}. There's no "set of all sets" paradox here by construction; $\mathbf{Cat}$ is slightly more than a category as we have seen so far because of the (2-)morphisms. We're introducing this just to state that the definition of product also works here so that we can consider product categories $\mathcal{C} \times \mathcal{D}$, whose objects are pairs of objects and morphisms pairs of morphisms.
\end{defn}
}

\marginnote{
\begin{defn}[Monoidal Category]
A monoidal category consists of a category $\mathcal{C}$, a functor $\otimes: \mathcal{C} \times \mathcal{C} \rightarrow C$, a monoidal unit object $I \in \text{Ob}(\mathcal{C})$, and the following natural isomorphisms -- i.e. natural transformations with inverses, where multiple bar notation indicates variable object argument positions: an associator $\alpha: ((- \ \otimes =) \otimes \equiv) \mapsto (- \ \otimes (= \otimes \equiv))$, a right unitor $\rho - \otimes I \mapsto -$, and a left unitor $\lambda I \otimes - \mapsto -\big)$. These natural isomorphisms must in addition satisfy certain \emph{coherence} diagrams, to be displayed shortly.
\end{defn}
}

\marginnote{
\begin{theorem}[Coherence for monoidal categories]
The following pentagon and triangle diagrams are conditions in the definition of a monoidal category. When they hold, all composites of associators and unitors (and their inverses) are isomorphisms \cite{}. $1$ denotes identities.
% https://q.uiver.app/?q=WzAsNSxbMSwwLCIoKFcgXFxvdGltZXMgWCkgXFxvdGltZXMgKFkgXFxvdGltZXMgWikpIl0sWzAsMSwiKFcgXFxvdGltZXMgKFggXFxvdGltZXMgKFkgXFxvdGltZXMgWikpKSJdLFsxLDIsIigoKFcgXFxvdGltZXMgWCkgXFxvdGltZXMgWSkgXFxvdGltZXMgWikiXSxbMCwzLCIoVyBcXG90aW1lcyAoKFggXFxvdGltZXMgWSkgXFxvdGltZXMgWikpIl0sWzEsNCwiKChXIFxcb3RpbWVzIChYIFxcb3RpbWVzIFkpKSBcXG90aW1lcyBaKSJdLFsxLDAsIlxcYWxwaGEiLDFdLFswLDIsIlxcYWxwaGEiLDFdLFs0LDIsIlxcYWxwaGEgXFxvdGltZXMgMSIsMV0sWzMsNCwiXFxhbHBoYSIsMV0sWzEsMywiMSBcXG90aW1lcyBcXGFscGhhIiwxXV0=
\[\begin{tikzcd}[ampersand replacement=\&]
	\& {((W \otimes X) \otimes (Y \otimes Z))} \\
	{(W \otimes (X \otimes (Y \otimes Z)))} \\
	\& {(((W \otimes X) \otimes Y) \otimes Z)} \\
	{(W \otimes ((X \otimes Y) \otimes Z))} \\
	\& {((W \otimes (X \otimes Y)) \otimes Z)}
	\arrow["\alpha"{description}, from=2-1, to=1-2]
	\arrow["\alpha"{description}, from=1-2, to=3-2]
	\arrow["{\alpha \otimes 1}"{description}, from=5-2, to=3-2]
	\arrow["\alpha"{description}, from=4-1, to=5-2]
	\arrow["{1 \otimes \alpha}"{description}, from=2-1, to=4-1]
\end{tikzcd}\]

% https://q.uiver.app/?q=WzAsMyxbMCwwLCIoWCBcXG90aW1lcyAoSSBcXG90aW1lcyBZKSkiXSxbMSwxLCIoKFggXFxvdGltZXMgSSkgXFxvdGltZXMgWSkiXSxbMCwyLCIoWCBcXG90aW1lcyBZKSJdLFswLDEsIlxcYWxwaGEiLDFdLFswLDIsIjEgXFxvdGltZXMgXFxsYW1iZGEiLDFdLFsxLDIsIlxccmhvIFxcb3RpbWVzIDEiLDFdXQ==
\[\begin{tikzcd}[ampersand replacement=\&]
	{(X \otimes (I \otimes Y))} \\
	\& {((X \otimes I) \otimes Y)} \\
	{(X \otimes Y)}
	\arrow["\alpha"{description}, from=1-1, to=2-2]
	\arrow["{1 \otimes \lambda}"{description}, from=1-1, to=3-1]
	\arrow["{\rho \otimes 1}"{description}, from=2-2, to=3-1]
\end{tikzcd}\]

\end{theorem}
}

\marginnote{
\begin{remark}[Coherence]
Coherence is about getting rid of syntactic bureaucracy. Addition for example is a binary associative operation, and knowing that $(x + (y + z)) = ((x + y) + z)$ is what allows us to safely drop all possible bracketings and just write $1+2+3$. We further know that addition is a monoid with unit 0, so we can always write $x + 0 = x = 0 + x$. Now the situation is that we have replaced the associativity equation with associator natural transformations $((X \otimes Y) \otimes Z) \overset{\alpha_{XYZ}}{\rightarrow} (X \otimes (Y \otimes Z))$, and unit equations with left and right unitors $(X \otimes I) \overset{\rho_X}{\rightarrow} X \overset{\lambda_X}\rightarrow (I \otimes X)$. Recalling that we're happy with isomorphism in place of equality, we would like to know that every possible composite of these structural operations is an isomorphism.
\end{remark}
}

\marginnote{
\begin{defn}[Symmetric Monoidal Category]
A symmetric monoidal category is a monoidal category with an additional natural isomorphism $\theta: - \ \otimes = \ \mapsto \ = \otimes \ -$. Coherence requires the following pair of hexagons \cite{}.
% https://q.uiver.app/?q=WzAsNixbMCwwLCIoWCBcXG90aW1lcyAoWSBcXG90aW1lcyBaKSkiXSxbMCwxLCIoWCBcXG90aW1lcyAoWiBcXG90aW1lcyBZKSkiXSxbMSwxLCIoKFogXFxvdGltZXMgWCkgXFxvdGltZXMgWSkiXSxbMCwyLCIoQSBcXG90aW1lcyAoWiBcXG90aW1lcyBYKSkiXSxbMSwyLCIoKFggXFxvdGltZXMgWikgXFxvdGltZXMgWSkiXSxbMSwwLCIoWiBcXG90aW1lcyAoWCBcXG90aW1lcyBZKSkiXSxbMCw1LCJcXHRoZXRhIl0sWzUsMiwiXFxhbHBoYSJdLFsyLDQsIlxcdGhldGFeey0xfSJdLFswLDEsIlxcYWxwaGFeey0xfSIsMl0sWzEsMywiMSBcXG90aW1lcyBcXHRoZXRhIiwyXSxbMyw0LCJcXGFscGhhIiwyXV0=
\[\begin{tikzcd}[ampersand replacement=\&]
	{(X \otimes (Y \otimes Z))} \& {(Z \otimes (X \otimes Y))} \\
	{(X \otimes (Z \otimes Y))} \& {((Z \otimes X) \otimes Y)} \\
	{(X \otimes (Z \otimes Y))} \& {((X \otimes Z) \otimes Y)}
	\arrow["\theta", from=1-1, to=1-2]
	\arrow["\alpha", from=1-2, to=2-2]
	\arrow["{\theta^{-1}}", from=2-2, to=3-2]
	\arrow["{\alpha^{-1}}"', from=1-1, to=2-1]
	\arrow["{1 \otimes \theta}"', from=2-1, to=3-1]
	\arrow["\alpha"', from=3-1, to=3-2]
\end{tikzcd}\]

% https://q.uiver.app/?q=WzAsNixbMCwwLCIoWCBcXG90aW1lcyAoWSBcXG90aW1lcyBaKSkiXSxbMCwxLCIoKFggXFxvdGltZXMgWSkgXFxvdGltZXMgWikiXSxbMSwxLCIoWSBcXG90aW1lcyAoWiBcXG90aW1lcyBYKSkiXSxbMCwyLCIoKFkgXFxvdGltZXMgWCkgXFxvdGltZXMgWikiXSxbMSwyLCIoWiBcXG90aW1lcyAoWCBcXG90aW1lcyBZKSkiXSxbMSwwLCIoKFkgXFxvdGltZXMgWikgXFxvdGltZXMgWCkiXSxbMCw1LCJcXHRoZXRhIl0sWzUsMiwiXFxhbHBoYV57LTF9Il0sWzIsNCwiMVxcb3RpbWVzIFxcdGhldGEiXSxbMyw0LCJcXGFscGhhXnstMX0iLDJdLFsxLDMsIlxcdGhldGFeey0xfSIsMl0sWzAsMSwiXFxhbHBoYSIsMl1d
\[\begin{tikzcd}[ampersand replacement=\&]
	{(X \otimes (Y \otimes Z))} \& {((Y \otimes Z) \otimes X)} \\
	{((X \otimes Y) \otimes Z)} \& {(Y \otimes (Z \otimes X))} \\
	{((Y \otimes X) \otimes Z)} \& {(Z \otimes (X \otimes Y))}
	\arrow["\theta", from=1-1, to=1-2]
	\arrow["{\alpha^{-1}}", from=1-2, to=2-2]
	\arrow["{1\otimes \theta}", from=2-2, to=3-2]
	\arrow["{\alpha^{-1}}"', from=3-1, to=3-2]
	\arrow["{\theta^{-1}}"', from=2-1, to=3-1]
	\arrow["\alpha"', from=1-1, to=2-1]
\end{tikzcd}\]
\end{defn}
}

\marginnote{
\begin{remark}[Coherence for symmetric monoidal categories]
Going back to addition, we can rearrange sums by commutativity of the addition monoid: $x + y = y + x$. In the monoidal setting, the natural isomorphism $\theta$ is the twisting of wires. Just as we would like to have addition equations such as $(x + (y + z)) = ((z + y) + x)$, we would like the twists $\theta$ to behave well with respect to the bundling of objects and morphisms with the associator $\alpha$.
\end{remark}
}

\marginnote{

\begin{theorem}[Strictification]

\end{theorem}
}

\marginnote{
\begin{theorem}[Graphical?]

\end{theorem}
}


This section seeks to give an introduction to process theories. The main body will work by examples towards process-theoretic mathematics of linguistic spatial relations -- words like "to the left of" and "between" -- which are a common ground of competence we all possess. The margin material will provide the formal mathematics of string diagrams from the bottum-up. Here we only focus on geometric relations in two dimensional Euclidean space equipped with notions of metric and distance. This section provides adequate foundations to follow \citep{}talkspace, in which I demonstrate how text circuits can be obtained from sentences and how such text circuits interpreted in the category of sets and relations \textbf{Rel} provides a semantics for such sentences. Moreover, this section motivates the question of how to express the (arguably more primitive \citep{}piaget) linguistic topological concepts -- such as "touching" and "inside" -- the mathematics of which will be in Chapter \ref{}; the reader may skip straight to that chapter after this section if they are uninterested in syntax. We close this section with a brief aside on how process theories relate to mathematical foundations and computer science.

A \emph{process} is something that transforms some number of input system types to some number of output system types. We depict systems as wires, labelled with their type, and processes as boxes. We read processes from left to right.
\[\tikzfig{proctheory/process}\]
Processes may compose in parallel, which we depict as vertically stacking boxes.
\[\tikzfig{proctheory/processpar}\]
Processes may compose sequentially, which we depict as connecting wires of the same type from left to right.
\[\tikzfig{proctheory/processseq}\]
In these diagrams only input-output connectivity matters: so we may twist wires and slide boxes along wires to obtain different diagrams that still refer to the same process. So the diagram below is equal to the diagram above.
\[\tikzfig{proctheory/processeq}\]
Some processes have no inputs; we call these \emph{states}. 
\[\tikzfig{proctheory/state}\]
Some processes have no outputs; we call these \emph{tests}.
\[\tikzfig{proctheory/test}\]
A process with no inputs and no outputs is a \emph{number}; the number tells us the outcome of applying tests to a composite of states modified by processes.
\[\tikzfig{proctheory/number}\]

A process theory is given by the following data:
\begin{itemize}
    \item A collection of systems
    \item A collection of processes along with their input and output systems
    \item A methodology to compose systems and processes sequentially and in parallel, and a specification of the unit of parallel composition.
    \item A collection of equations between composite processes
\end{itemize}

\begin{example}[Linear maps with direct sum]
Systems are finite-dimensional vector spaces over $\mathbb{R}$. Processes are linear maps, expressed as matrices with entries in $\mathbb{R}$.\\
Sequential composition is matrix multiplication. Parallel composition of systems is the direct sum of vector spaces $\oplus$. The parallel composition of matrices $\mathbf{A}, \mathbf{B}$ is the block-diagonal matrix
$$\begin{bmatrix}
\mathbf{A} & \mathbf{0} \\
\mathbf{0} & \mathbf{B}
\end{bmatrix}$$
The unit of parallel composition is the singleton 0-dimensional vector space.
States are row vectors. Tests are column vectors. The numbers are $\mathbb{R}$. Usually the monoidal product is written with the symbol $\otimes$, which denotes hadamard product for linear maps. The process theory we have just described takes the direct sum $\oplus$ to be the monoidal product. To avoid confusion we will use the linear algebraic notation when linear algebra is concerned.
\end{example}

\begin{example}[Sets and functions with cartesian product]
Systems are sets $A,B$. Processes are functions between sets $f: A \rightarrow B$. Sequential composition is function composition. Parallel composition of systems is the cartesian product of sets: the set of ordered pairs of two sets.
\[A \otimes B = A \times B := \{(a,b) \ | \ a \in A, b \in B\}\]
The parallel composition $f \otimes g : A \times C \rightarrow B \times D$ of functions $f: A \rightarrow B$ and $g: C \rightarrow D$ is defined:
\[f \otimes g := (a,c) \mapsto (f(a),g(c))\]
The unit of parallel composition is the singleton set $\{\star\}$. There are many singletons, but this presents no problem for the later formal definition because they are all equivalent up to unique isomorphism. States of a set $A$ correspond to elements $a \in A$ --  we forgo the usual categorical definition of points from the terminal object in favour of generalised points from the monoidal perspective. Every system $A$ has only one test $a \mapsto \star$; this is since the singleton is terminal in \textbf{Set}. So there is only one number.
\end{example}

\begin{example}[Sets and relations with cartesian product]
Systems are sets $A,B$. Processes are relations between sets $\Phi \subseteq A \times B$, which we may write in either direction $\Phi^*: A \nrightarrow B$ or $\Phi_*: B \nrightarrow A$. Relations between sets are equivalently matrices with entries from the boolean semiring. Relation composition is matrix multiplication with the boolean semiring. $\Phi^*,\Phi_*$ are the transposes of one another. Sequential composition is relation composition:
\[A \overset{\Phi}{\nrightarrow} B \overset{\Psi}{\nrightarrow} C := \{(a,c) \ | \  a \in A, \ c \in C, \ \exists b_{\in B}: (a,b) \in \Phi \wedge (b,c) \in \Psi  \}\]
Parallel composition of systems is the cartesian product of sets. The parallel composition of relations $A \otimes C \overset{\Phi \otimes \Psi}{\nrightarrow} B \otimes D$ of relations $A \overset{\Phi}{\nrightarrow} B$ and $C \overset{\Psi}{\nrightarrow} D$ is defined:
\[\Phi \otimes \Psi := \{\big( (a,c) , (b,d) \big) \ | \ (a,b) \in \Phi \wedge (c,d) \in \Psi\}\]
The unit of parallel composition is the singleton. States of a set $A$ are subsets of $A$. Tests of a set $A$ are also subsets of $A$.
\end{example}

\subsection{What does it mean to copy and delete?}

Now we discuss how we might define the properties and behaviour of processes by positing equations between diagrams. Let's begin simply with two intuitive processes \emph{copy} and \emph{delete}:
\[\tikzfig{proctheory/copydelete}\]

\begin{example}[Linear maps]
Consider a vector space $\mathbf{V}$, which we assume includes a choice of basis. The copy map is the rectangular matrix made of two identity matrices:
\[\Delta_\mathbf{V}: \mathbf{V} \rightarrow \mathbf{V} \oplus \mathbf{V} := \begin{bmatrix} \mathbf{1}_\mathbf{V} & \mathbf{1}_\mathbf{V} \end{bmatrix}\]
The delete map is the column vector of 1s:
\[\epsilon_\mathbf{V}: \mathbf{V} \rightarrow \mathbf{0} := \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}\]
\end{example}

\begin{example}[Sets and functions]
Consider a set $A$. The copy function is defined:
\[\Delta_A : A \rightarrow A \times A := a \mapsto (a,a)\]
The delete funtion is defined:
\[\epsilon_A : A \rightarrow \{\star\} := a \mapsto \star\]
\end{example}

\begin{example}[Sets and relations]\label{relcopy}
Consider a set $A$. The copy relation is defined:
\[\Delta_A : A \nrightarrow A \times A := \{\big(a , (a,a) \big) \ | \ a \in A\}\]
The delete relation is defined:
\[\epsilon_A : A \nrightarrow \{\star\} := \{(a,\star) \ | \ a \in A\}\]
\end{example}

We may verify that, no matter the concrete interpretation of the diagram in terms of linear maps, functions or relations, the following equations characterise a cocommutative comonoid internal to a monoidal category.
\begin{equation}\label{cocom}
\resizebox{\textwidth}{!}{\tikzfig{bestiary/basicrelations}}
\end{equation}

It is worth pausing here to think about how one might characterise the process of copying in words; it is challenging to do so for such an intuitive process. The diagrammatic equations in the margin, when translated into prose, provide an answer.
\begin{description}
\item[\textbf{Coassociativity}:] says there is no difference between copying copies.
\item[\textbf{Cocommutativity}:] says there is no difference between the outputs of a copy process.
\item[\textbf{Counitality}:] says that if a copy is made and one of the copies is deleted, the remaining copy is the same as the original.
\end{description}

Insofar as we think this is an acceptable characterisation of copying, rather than specify concretely what a copy and delete does for each system $X$ we encounter, we can instead posit that so long as we have processes $\Delta_X: X \otimes X \rightarrow X$ and $\epsilon_X: X \rightarrow I$ that obey all the equational constraints above, $\Delta_X$ and $\epsilon_X$ are as good as a copy and delete.

\begin{example}[Not all states are copyable]\label{ex:copyablestate}
Call a state \emph{copyable} when it satisfies the following diagrammatic equation:
\[\tikzfig{proctheory/copyable}\]
In the process theory of sets and functions, all states are copyable. Not all states are copyable in the process theories of sets and relations and of linear maps. For example, consider the two element set $\mathbb{B} := \{0,1\}$, and let $\top : \{\star\} \nrightarrow \mathbb{B} := \{(\star,0),(\star,1)\} \simeq \{0,1\}$. Consider the composite of $\top$ with the copy relation:
\[\top;\Delta_{\mathbb{B}} := \{\big(\star,(0,0)\big),\big(\star,(1,1)\big)\} \simeq \{(0,0),(1,1)\}\]
This is a perfectly correlated bipartite state, and it is not equal to $\{0,1\} \times \{0,1\}$, so $\top$ is not copyable.
\end{example}


\begin{remark}\label{ft:determinism}
The copyability of states is a special case of a more general form of interaction with the copy relation:
\[\scalebox{0.75}{\tikzfig{proctheory/copyablefunc}}\]
A cyan map that satisfies this equation is said to be a homomorphism with respect to the commutative comonoid. In the process theory of relations, those relations that satisfy this equation are precisely the functions; in other words, this diagrammatic equation expresses \emph{determinism}.
\end{remark}

Here is an unexpected consequence. Suppose we insist that \emph{to copy} in principle also implies the ability to copy \emph{anything} -- arbitrary states. From Example \ref{ex:copyablestate} and Remark \ref{ft:determinism}, we know that this demand is incompatible with certain process theories. In particular, this demand would constrain a process theory of sets and relations to a subtheory of sets and functions. The moral here is that process theories are flexible enough to meet ontological needs. A classical computer scientist who works with perfectly copyable data and processes might demand universal copying along with Equations \ref{cocom}, whereas a quantum physicist who wishes to distinguish between copyable classical data and non-copyable quantum data might taxonomise copy and delete as a special case of a more generic quasi-copy and quasi-delete that only satisfies equations \ref{cocom}. In fact, quantum physicists \emph{do} do this; see Dodo: [].

\subsection{What is an update?}\label{ss:update}

In the previous section we have seen how we can start with concrete examples of copying in distinct process theories, and obtain a generic characterisation of copying by finding diagrammatic equations copying satisfies in each concrete case. In this section, we show how to go in the opposite direction: we start by positing diagrammatic equations that characterise the operational behaviour of a particular process -- such as \emph{updating} -- and it will turn out that any concrete process that satisfies the equational constraints we set out will \emph{by our own definition} be an update.\\

Perhaps the most familiar setting for an update is a database. In a database, an \bM entry\e often takes the form of pairs of \bB fields\e and \bO values\e. For example, where a database contains information about employees, a typical entry might look like:
\[\texttt{\bM < \bB\textbf{NAME}\e:\bO Jono Doe\e, \bB\textbf{AGE}\e:\bO 69\e, \bB\textbf{JOB}\e:\bO CONTENT CREATOR\e, \bB\textbf{SALARY}\e:\bO\$420\e, ... >\e}\]
There are all kinds of reasons one might wish to update the value of a field: Jono might legally change their name, a year might pass and Jono's age must be incremented, Jono might be promoted or demoted or get a raise and so on. It was the concern of database theorists to formalise and axiomatise the notion of updating the value of a field -- \emph{independently of the specific programming language implementation of a database} -- so that they had reasoning tools to ensure program correctness []. The problem is reducible to axiomatising a \emph{rewrite}: we can think of updating a value as first calculating the new value, then \emph{putting} the new value in place of the old. Since often the new value depends in some way on the old value, we also need a procedure to \emph{get} the current value.\\

That was a flash-prehistory of \emph{bidirectional transformations} []. Following the monoidal generalisation of lenses in [], a rewrite as we have described above is specified by system diagrammatic equations in the margin, each of which we introduce in prose.

\begin{description}
\item[\textbf{PutPut}:] Putting in one value and then a second is the same as deleting the first value and just putting in the second.
\[\scalebox{0.5}{\tikzfig{proctheory/putputs}}\]
\item[\textbf{GetPut}:] Getting a value from a field and putting it back in is the same as not doing anything.
\[\scalebox{0.5}{\tikzfig{proctheory/getput}}\]
\item[\textbf{PutGet}:] Putting in a value and getting a value from the field is the same as first copying the value, putting in one copy and keeping the second.
\[\scalebox{0.5}{\tikzfig{proctheory/putget}}\]
\item[\textbf{GetGet}:] Getting a value from a field twice is the same as getting the value once and copying it.
\[\scalebox{0.5}{\tikzfig{proctheory/getget}}\]
\end{description}


These diagrammatic equations do two things. First, they completely specify what it means to get and put values in a field in an implementation independent manner; it doesn't matter whether database entries are encoded as bitstrings, qubits, slips or paper or anything else, what matters is the interaction of get and put. Second, the diagrammatic equations give us the right to call our processes \emph{get} and \emph{put} in the first place: we define what it means to get and put by outlining the mutual interactions of get, put, copy, and delete. These two points are worth condensing and rephrasing:
\[
\textbf{A \emph{kind of process} is determined by patterns of interaction with other kinds of processes.}
\]

Now we can diagrammatically depict the process of updating Jono's age, by \bB getting\e Jono's \bO age value\e from their \bM entry\e, incrementing it by 1, and \bB putting\e it back in.

\[\tikzfig{proctheory/incrementage}\]

\subsection{Spatial predicates}

The following simple inference is what we will try to capture process-theoretically:

\begin{itemize}
\item \texttt{Oxford is north of London}
\item \texttt{Vincent is in Oxford}
\item \texttt{Philipp is in London}
\end{itemize}

How might it follow that \texttt{Philipp is south of Vincent}?\\

One way we might approach such a problem computationally is to assign a global coordinate system, for instance interpreting `north' and `south' and `is in' using longitude and latitude. Another coordinate system we might use is a locally flat map of England. The fact that either coordinate system would work is a sign that there is a degree of implementation-independence.\\

This coordinate/implementation-independence is true of most spatial language: we specify locations only by relative positions to other landmarks or things in space, rather than by means of a coordinate system. This is necessarily so for the communication of spatial information between agents who may have very different reference frames.\\

So the process-theoretic modelling aim is to specify how relations between entities can be \emph{updated} and \emph{classified} without requiring individual spatial entities to intrinsically possess meaningful or determinate spatial location.\\

So far we have established how to update properties of individual entities. We can build on what we have so far by observing that a relation between two entities can be considered a property of the pair.

\[placeholder\]

Spatial relations obey certain compositional constraints, such as transitivity in the case of `north of':

\[placeholder\]

Or the equivalence between 'north of' and 'south of' up to swapping the order of arguments:

\[\]

There are other general constraints on spatial relations, such as order-independence: the order in which spatial relations are updated does not (at least for perfect reasoners) affect the resultant presentation. This is depicted diagrammatically as commuting gates:

\[placeholder\]

\subsection{Deterministic Neural Nets and Closed Monoidal Categories}

For those unfamiliar, it is worth understanding the fundamentals of how neural nets work, which illuminates how they are able to transmute large amounts of input data into strong and "theory-free" [] performance at tasks. The ability to convert data (in which we are drowning), along with the universal approximation theorem, explain why data-driven learning methods have recently outperformed handcrafted rules-based approaches to artificial intelligence. I will focus only on the basic deep neural net -- so the reader is warned that the state of the art in terms of learning architectures is far more sophisticated, such as transformers [] used in large language models. There are only two important takeaways from this section. First is that the reader must understand why it is that data-driven learning methods outperform human axiomatising when dealing with complex problem domains, and are therefore (with the usual caveats of explainability) preferable. Second is an understanding of the statement of the universal approximation theorem, which will reappear in Section \ref{sec:learn}, where it will be of service in an attempt to introduce compositionality via interacting ensembles of neural nets.

\newthought{Neural nets arise from a toy model of biological neurons.} At a glance, biological neurons have many receptors and one output, and the neuron fires a signal out if its combined inputs exceed an activation threshold. As a simplification, McCulloch-Pitts neurons are a sum of $n$ inputs passed through an activation function $\sigma: \mathbf{R}^n \rightarrow \mathbf{R}$ that is permitted to be nonlinear, but traditionally monotone increasing and sigmoidal, which bounds the range of the function $\exists a_{\mathbb{R}} b_{\mathbb{R}} \forall x_{\mathbb{R}} : a \leq \sigma(x) \leq b$, and asks that $\sigma$ approaches the lower and upper bounds in the limit as $x$ goes to $-\infty$ and $\infty$ respectively. Using the diagrammatic calculus for linear algebra [] equipped with a nonlinear activation function -- all of which is interpretable in \textbf{TopRel}, we can immediately grasp a visual resemblance between the designs of nature and man:

\[placeholder\]

\newthought{The first use of neural nets was in application to the problem of machine vision.} These first, single-layer neural nets were called \emph{perceptrons}. Mimicking the neuronal organisation of the visual cortex, it was a sensible idea to stack these layers on top of one another [] -- these layers are the original reason for the word "deep" in "deep learning", but words change in meaning over time.

\[placeholder\]

\newthought{The modern ubiquity of neural nets is due to several factors.} First is Hinton's backpropagation algorithm [] (which may be obsolete when you are reading this by Hinton's forward-forward second salvo [].) Observe that even after one has decided on the shape of the neural net in terms of neuronal connectivity, there are still many degrees of freedom in the parameters of the activation functions, in particular their horizontal shift (bias) and vertical stretching (weights). Borrowing diagrammatic notation for parameters as orthogonal wires from [], we can depict the degrees of freedom for a single neuron like this:

\[placeholder\]

\newthought{There is a massive space of parameters to set for even a moderately sized neural net.} So how do we set the parameters in such a way that the neural net computes something useful? Backpropagation solves this problem by leveraging the shape of a neural net. There are many easily searchable resources that cover backpropagation for the interested reader, including category-theoretic ones []. The simple explanation goes like this. Let's just focus on the weight parameter of each neuron. By analogy each neuron is a shitty person, and their weight is how strongly they hold a binary opinion. A neural net by analogy is a shitty rigid hierachical society with voters in the back and decision makers in the front. As a simple example, Alice and Bob each make a recommendation to Claire based on what they receive as input.

\[placeholder\]

Suppose that Claire's decision is wrong. She revises her own opinion then meets with her confidantes. Alice's recommendation was faulty, so Claire blames her; as a narcissistic defense, the viciousness of the blame is proportional to how wrong Claire was. Alice revises her own opinion proportional how mean Claire is being, and then Alice goes to seek out her confidantes to perpetuate a vicious cycle of psychological violence. Bob on the other hand was right, Claire tells him this with sheepishness proportional to her error, and he starts gloating "I told you so!" with glee proportional to how much cleverer he feels than Claire. So Bob becomes slightly more entrenched in his opinion, and then he goes to seek out his confidantes to either congratulate or belittle them, again proportional to how right he was. When all of the blame and backpatting has backpropagated throughout society, all the shitty people have adjusted their opinions, and their shitty society will be less prone to making the same mistake again. In human terms, this process is repeated for all of recorded history or longer, and then you have a neural net that can recognise handwritten digits.

\[placeholder\]

All this process needs to get started is a lot of labelled pairs of data, input along with the desired output for that input. The formal terminology for the scenario above that converts data into performance is "training", which is a computationally intensive process when lots of data is involved for big neural nets. So the second factor of the ubiquity of neural nets is Moore's law and analogues, which have overseen exponential growth in computational power and digital data storage capacity. Neural nets convert data and compute power as fuel into practical applications, and we live in an era of increasingly plentiful data and compute. Hence, the bitter lesson []; clever theories are no match for stupid methods with lots of data and a big computer. But why the hell should any of this work in the first place? Surely there are limits to what neural nets can do. Now the third factor; Moore's law and the bitter lesson might be cheated, but the third factor is a law backed by mathematics.

\begin{theorem}[Universal Approximation Theorem]

\end{theorem}

\newthought{Any problem that can be encoded as a continuous transformation of lists of real numbers into other lists of real numbers is potential prey for a big enough neural net.} A little creative thought will show that many practical problems can be treated in this way. The litigious can easily spot problems in neural nets outside of this law. For example, to the best of our knowledge there is no known lower bound for how much data is required -- as a function of desired accuracy within a desired confidence -- for a neural net to learn its target, so for all we know, any big neural net could suddenly fail on an easy input instance for no good reason. The universal approximation theorem is a double-edged sword, and the side that cuts the holder is that for complex problems, the input data cannot span the whole problem domain, so there will be many neural nets that agree perfectly on the training data but will perform differently out-of-distribution -- this common phenomenon is called "overfitting". Moreover, a major component of explainability from the human perspective is having mental representations and rules for their manipulation, whereas it is in general very difficult to determine how or even whether the neural net has internal representations and rules for its inputs. This introduces an ethical criterion that most data-driven algorithms fail: it is not a big deal to be recommended a show we do not like or have a machine play a game more innovatively than a human could, but if the stakes are higher, such as operating a vehicle or on a patient, we would prefer safety guarantees predicated upon internal representations and rules that we can understand and negotiate in human terms. In Section \ref{sec:learn} we will try to blunt the painful edges by using the universal approximation theorem and overfitting to our advantage in search of compositional, explainable internal representations.

\newthought{What is the diagrammatic content of the Universal Approximation Theorem?}

\subsection{Pregroup diagrams and correlations}

Let's revisit the copy and delete maps for a moment. Suppose our process theory is such that for every wire $X$, there is a unique copy map $\delta_X$ such that every state on $X$ is copyable and deletable. A consequence of this assumption is that every state is $\otimes$-separable (read \emph{tensor separable}):

\[placeholder\]

But there are certainly process theories in which we don't want this. For example, if states are random variables and parallel composition is the product of independent random variables (as is the case in Markov categories for probability theory []) then copying a random variable gives a perfectly correlated pair of variables, which cannot be expressed as the product of a pair of independent random variables.

\subsection{Equational Constraints and Frobenius Algebras}

\subsection{Processes, Sets, and Computers}

\newthought{\texttt{Objection:} But what are the \emph{things} that the processes operate on?}

This is a common objection from philosophers who want their ontologies tidy. The claim roughly goes that you can't really reason about processes without knowing the underlying objects that participate on those processes, and since set theory is the only way we know how to spell out objects intensionally in this way, we should stick to sets. In simpler terms, if we're drawing only functions as (black)-boxes in our diagrams, how will we know what they do to the elements of the underlying sets?\\

The short answer is that -- perhaps surprisingly -- reasoning process-theoretically is mathematically equivalent to reasoning about sets and elements for all practical purposes; it is as if whatever is going on \emph{out there} is indifferent to whether we describe using a language of only nouns or only verbs.\\

In the case of set theory, let's suppose that instead of encoding functions as sets, we treat functions as primitive, so that we have a process theory where wires are labelled with sets, and functions are process boxes that we draw. The problem we face now is that it is not immediately clear how we would access the elements of any set using only the diagrammatic language. The solution is the observation that the elements $\{x \ | \ X\}$ of a set $X$ are in bijective correspondence with the functions from a singleton into $X$: $\{ f(\star) \mapsto x \ | \ \{\star\} \overset{f}{\rightarrow} X $. In prose, for any element $x$ in a set $X$, we can find a function that behaves as a pointer to that element $\{\star\} \rightarrow X$. So the states we have been drawing, when interpreted in the category of sets and function, are precisely elements of the sets that label their output wires.\\

The full and formal answer will require the reader to see Section \ref{} which spells out the category theory underpinning process theories. The caveat here is that process theories work for all \emph{practical} purposes, so I make no promises about how diagrams work for the kind of set theories that deals with hierarchies of infinities that set theorists do. For other issues concerning for instance the set of all functions between two sets, that requires symmetric monoidal closure, for which there exist string-diagrammatic formalisms \citep{}.

\newthought{\texttt{Objection:} But if they're expressively the same, what's the point?}

The following rebuttal draws on Harold Abelson's introductory lecture to computer science \citep{} (in which string diagrams appear to introduce programs without being explicitly named as such).\\

There is a distinction between declarative and imperative knowledge. Declarative knowledge is \emph{knowing-that}, for example, 6 is the square root of 36, which we might write $6 = \sqrt{36}$. Imperative knowledge is \emph{knowing-how}, for example, to obtain the square root of a positive number, for instance, by Heron's iterative method: to obtain the square root of $Y$, make a guess $X$, and take the average of $X$ and $\frac{Y}{X}$ until your guess is good enough.\\

Computer science concerns imperative knowledge. An obstacle to the study of imperative knowledge is complexity, which computer scientists manage by black-box abstraction -- suppressing irrelevant details, so that for instance once a square root procedure is defined, the reasoner outside the system does not need to know whether the procedure inside is an iterative method by Heron or Newton, only that it works and has certain properties. These black-boxes can be then composed into larger processes and procedures within human cognitive load.\\

Abstraction also yields generality. For example, in the case of addition, it is not only numbers we may care to add, but perhaps vectors, or the waveforms of signals. So there is an abstract notion of addition which we concretely instantiate for different domains that share a common interface; we may decide for example that all binary operations that are commutative monoids are valid candidates for what it means to be an addition operation.\\

In this light, string diagrams are a natural metalanguage for the study of imperative knowledge; string diagrams in fact independently evolved within computer science from flowcharts describing processes. Process theories, which are equations or logical sentences about processes, allow us to reason declaratively about imperative knowledge. Moreover, string diagrams as syntactic objects can be interpreted in various concrete settings, so that the same diagram serves as the common interface for a process like addition, with compliant implementation details for each particular domain spelled out separately.