\section{Process Theories}

\marginnote{
There are a lot of definitions to get started, but as with programming languages, preloading the work makes it easier to scale. This is the mathematical source code of string diagrams, which is only necessary if we need to show that something new is a symmetric monoidal category or if we are tinkering deeply, so it can be skipped for now. The important takeaway is that string diagrams are syntax, with morphisms in symmetric monoidal categories as semantics.
}

\marginnote[2cm]{
\begin{defn}[Category]
A \emph{category} $\mathcal{C}$ consists of the following data
\end{defn}
}

\marginnote{
\begin{itemize}
\item{A collection $\text{Ob}(\mathcal{C})$ of \emph{objects}}
\item{For every pair of objects $A,B \in \text{Ob}(\mathcal{C})$, a set $\mathcal{C}(A,B)$ of \emph{morphisms} from $a$ to $b$.}
\item{Every object $a \in \text{Ob}(\mathcal{C})$ has a specified morphism $1_a$ in $\mathcal{C}(a,a)$ called \emph{the identity morphism} on $a$.}
\item{Every triple of objects $A,B,C \in \text{Ob}(\mathcal{C})$, and every pair of morphisms $f \in \mathcal{C}(A,B)$ and $g \in \mathcal{C}(b,c)$ has a specified morphism $(f;g) \in \mathcal{C}(a,c)$ called \emph{the composite} of $f$ and $g$.}
\end{itemize}
}

\marginnote{
This data has to satisfy two coherence conditions, which are:\\
\textbf{Unitality:} For any morphism $f: a \rightarrow b$, $1_a;f = f = f;1_b$\\
\textbf{Associativity:} For any four objects $A,B,C,D$ and three morphisms $f: A \rightarrow B$, $g: B \rightarrow C$, $h: C \rightarrow D$, $(f;g);h = f;(g;h)$
}

\marginnote{
\begin{defn}[Categorical Product]
In a category $\mathcal{C}$, given two objects $a,b \in \text{Ob}(\mathcal{C})$, the \emph{product} $A \times B$, if it exists, is an object with projection morphisms $\pi_0: A \times B \rightarrow A$ and $\pi_1: A \times B \rightarrow B$ such that for any object $x \in \text{Ob}(\mathcal{C})$ and any pair of morphisms $f: X \rightarrow A$ and $g: x \rightarrow b$, there exists a unique morphism $f \times g: X \rightarrow A \times B$ such that $f = (f \times g) ; \pi_0$ and $g = (f \times g); \pi_1$. This is a mouthful which is easier expressed as a commuting diagram as below. The dashed arrow indicates uniqueness. $A \times B$ is a product when every path through the diagram following the arrows between two objects is an equality.
\[\begin{tikzcd}[ampersand replacement=\&]
	\&\& x \\
	\\
	a \&\& {a \times b} \&\& b
	\arrow["{\langle f,g \rangle}"{description}, dashed, from=1-3, to=3-3]
	\arrow["f"{description}, from=1-3, to=3-1]
	\arrow["g"{description}, from=1-3, to=3-5]
	\arrow["{\pi_1}"{description}, from=3-3, to=3-5]
	\arrow["{\pi_0}"{description}, from=3-3, to=3-1]
\end{tikzcd}\]
\end{defn}
}

\marginnote[2cm]{
The idea behind the definition of product is simple: instead of explicitly constructing the cartesian product of sets from within, let's say \emph{a product is as a product does}. For objects, the cartesian product of sets $A \times B$ is a set of pairs, and we may destruct those pairs by extracting or projecting out the first and second elements, hence the projection maps $\pi_0,\pi_1$. Another thing we would like to do with pairs is construct them; whenever we have some $A$-data and $B$-data, we can pair them in such a way that construction followed by destruction is lossless and doesn't add anything. In category-theoretic terms, we select `arbitrary' $A$- and $B$-data by arrows $f: X \rightarrow A$ and $g: X \rightarrow B$, and we declare that $f \times g: X \rightarrow A \times B$ is the unique way to select corresponding tuples in $A \times B$. This design-pattern of "for all such-and-such there exists a unique such-and-such" is an instance of a so-called \emph{universal property}, the purpose of which is to establish isomorphism between operationally equivalent implementations.
}

\marginnote{
To understand what this style of definition gives us, let's revisit Kuratowski's and Wiener's definitions of cartesian product, which are, respectively:
\[A \overset{K}{\times} B := \bigg\{ \{\{a\},\{a,b\}\} \ | \ a \in A \ , \ b \in B \bigg\}\]
\[A \overset{W}{\times} B := \bigg\{ \{\{a,\varnothing\},b\} \ | \ a \in A \ , \ b \in B \bigg\}\]
Keeping overset-labels and using maplet notation, the associated projections are:
\[\overset{K}{\pi_0} := \{\{a\},\{a,b\}\} \mapsto a\]
\[\overset{K}{\pi_1} := \{\{a\},\{a,b\}\} \mapsto b\]
\[\overset{W}{\pi_0} := \{\{a,\varnothing\},b\} \mapsto a\]
\[\overset{W}{\pi_1} := \{\{a,\varnothing\},b\} \mapsto b\]
And maps $f,g$ into $A$ and $B$ are tupled by the following:
\[f \overset{K}{\times} g := x \mapsto \{\{f(x)\},\{f(x),g(x)\}\}\]
\[f \overset{W}{\times} g := x \mapsto \{\{f(x),\varnothing\},g(x)\}\]
Both satisfy the commutative diagram defining the product.
Something neat happens when we pick $A \overset{K}{\times} B$ to be the arbitrary $X$ for the product definition of $A \overset{W}{\times} B$ and vice versa. We get to mash the commuting diagrams together:
% https://q.uiver.app/?q=WzAsNSxbNCwxLCJCIl0sWzAsMSwiQSJdLFsyLDAsIkEgXFxvdmVyc2V0e0t9e1xcdGltZXN9IEIiXSxbMSwzLCJBIFxcb3ZlcnNldHtXfXtcXHRpbWVzfSBCIl0sWzMsMywiQSBcXG92ZXJzZXR7V317XFx0aW1lc30gQiJdLFsyLDEsIlxcb3ZlcnNldHtLfXtcXHBpXzB9IiwxXSxbMiwwLCJcXG92ZXJzZXR7S317XFxwaV8xfSIsMV0sWzMsMCwiXFxvdmVyc2V0e1d9e1xccGlfMX0iLDFdLFszLDEsIlxcb3ZlcnNldHtXfXtcXHBpXzB9IiwxXSxbMiwzLCIiLDEseyJjdXJ2ZSI6LTIsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dLFszLDIsIiIsMSx7ImN1cnZlIjotMiwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzQsMSwiXFxvdmVyc2V0e1d9e1xccGlfMH0iLDEseyJsYWJlbF9wb3NpdGlvbiI6ODB9XSxbNCwwLCJcXG92ZXJzZXR7V317XFxwaV8xfSIsMV0sWzMsNCwiaWQiLDEseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJhcnJvd2hlYWQifSwiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV1d
\[\begin{tikzcd}[ampersand replacement=\&]
	\&\& {A \overset{K}{\times} B} \\
	A \&\&\&\& B \\
	\\
	\& {A \overset{W}{\times} B} \&\& {A \overset{W}{\times} B}
	\arrow["{\overset{K}{\pi_0}}"{description}, from=1-3, to=2-1]
	\arrow["{\overset{K}{\pi_1}}"{description}, from=1-3, to=2-5]
	\arrow["{\overset{W}{\pi_1}}"{description}, from=4-2, to=2-5]
	\arrow["{\overset{W}{\pi_0}}"{description}, from=4-2, to=2-1]
	\arrow[curve={height=-12pt}, dashed, from=1-3, to=4-2]
	\arrow[curve={height=-12pt}, dashed, from=4-2, to=1-3]
	\arrow["{\overset{W}{\pi_0}}"{description, pos=0.8}, from=4-4, to=2-1]
	\arrow["{\overset{W}{\pi_1}}"{description}, from=4-4, to=2-5]
	\arrow["id"{description}, dashed, tail reversed, from=4-2, to=4-4]
\end{tikzcd}\]
}

\marginnote{
The two unique arrows between $\overset{K}{\times}$ and $\overset{W}{\times}$ are format-conversions, and we know by definition that the unique arrow that performs format conversion from $\overset{W}{\times}$ to itself in the bottom face is the identity. In maplet notation, the conversion from $A \overset{K}{\times} B \rightarrow A \overset{W}{\times} B$ is $\{\{a\},\{a,b\}\} \mapsto \{\{a,\varnothing\},b\}$, and similarly for the other direction. Because these conversions are uniquely determined arrows, their composite is also uniquely determined, and we know their composite is equal to the identity. So, the nontrivial conversions witness an \emph{isomorphism} between $A \overset{K}{\times} B$ and $A \overset{W}{\times} W$; a pair of maps $X \rightarrow Y$ and $Y \rightarrow X$ such that their loop-composites equal identities. This in a nutshell is the category-theoretic approach to overcoming the bureaucracy of syntax: use universal properties (or whatever else) encode your intents and purposes, establish isomorphisms, and then treat isomorphic things as "the same for all intents and purposes". The idea of treating isomorphic objects as the same is ingrained in category theory, so isomorphism notation $\simeq$ is often just written as equality $=$; going forward we will use equality notation unless there are good reasons to remember that we only have isomorphisms.
}

\marginnote[1cm]{
\begin{defn}[Product of categories]
The product of categories $\mathcal{C} \times \mathcal{D}$ has ordered pairs of objects $(C,D)$ and pairs of morphisms $(f,g)$ with elementwise composition and pairs of identities for identity morphisms. It is also, up to unique isomorphism, the categorical product in \textbf{Cat}, the category of categories and functors.
\end{defn}
}

\marginnote[1cm]{
\begin{defn}[Functor]
A functor $F: \mathcal{C} \rightarrow \mathcal{D}$ (read: with domain a category $\mathcal{C}$ and codomain a category $\mathcal{D}$) consists of two suitably related functions. An object function $F_0 : \text{Ob}(\mathcal{C}) \rightarrow \text{Ob}(\mathcal{D})$ and a morphism function (equivalently viewed as a family of functions indexed by pairs of objects of $\mathcal{C}$) $F_1(X,Y) : \mathcal{C}(X,Y) \rightarrow \mathcal{D}(F_0 X,F_0 Y )$. $F_1$ must map identities to identities -- i.e., be such that for all $X \in \mathcal{C}$, $F_1(1_X) = 1_{F_0 X}$ -- and $F_1$ must map composites to composites -- i.e., for all $X,Y,Z \in \text{Ob}(\mathcal{C})$ and all $f: X \rightarrow Y$ and $g: Y \rightarrow Z$, $F_1 (f;g) = F_1 f ; F_1 g$.
\end{defn}

Functors in short map categories to categories, preserving the structure of identities and composition. They are the essence of "structure preserving transformation". Insofar as semantics is the science of finding structure-preserving transformations that tell us when syntactic things are equal, functors are just that. They are incredibly useful and mysterious and worth internalising in a way I am not adept enough to impress by example in this margin. For us, for now, they are just stepping stones to define transformations \emph{between functors}.
}

\marginnote[2cm]{
\begin{defn}[Natural Transformation]
A natural transformation $\theta: F \rightarrow G$ for (co)domain-aligned functors $F,G: \mathcal{C} \rightarrow \mathcal{D}$ is a family of morphisms in $\mathcal{D}$ indexed by objects $X \in \mathcal{C}$ such that for all $f: X \rightarrow Y$ in $\mathcal{C}$, the following commuting diagram holds in $\mathcal{D}$:
% https://q.uiver.app/?q=WzAsNCxbMCwwLCJGWCJdLFsyLDAsIkZZIl0sWzAsMiwiR1giXSxbMiwyLCJHWSJdLFswLDIsIlxcdGhldGFfWCIsMV0sWzAsMSwiRmYiLDFdLFsxLDMsIlxcdGhldGFfWSIsMV0sWzIsMywiR2YiLDFdXQ==
\[\begin{tikzcd}[ampersand replacement=\&]
	FX \&\& FY \\
	\\
	GX \&\& GY
	\arrow["{\theta_X}"{description}, from=1-1, to=3-1]
	\arrow["Ff"{description}, from=1-1, to=1-3]
	\arrow["{\theta_Y}"{description}, from=1-3, to=3-3]
	\arrow["Gf"{description}, from=3-1, to=3-3]
\end{tikzcd}\]
\end{defn}
}

\clearmargin

\marginnote{
\begin{defn}[Monoidal Category]
A monoidal category consists of a category $\mathcal{C}$, a functor $\otimes: \mathcal{C} \times \mathcal{C} \rightarrow C$, a monoidal unit object $I \in \text{Ob}(\mathcal{C})$, and the following natural isomorphisms -- i.e. natural transformations with inverses, where multiple bar notation indicates variable object argument positions: an associator $\alpha: ((- \ \otimes =) \otimes \equiv) \mapsto (- \ \otimes (= \otimes \equiv))$, a right unitor $\rho - \otimes I \mapsto -$, and a left unitor $\lambda I \otimes - \mapsto -\big)$. These natural isomorphisms must in addition satisfy certain \emph{coherence} diagrams, to be displayed shortly.
\end{defn}
}

\marginnote[2cm]{
\begin{theorem}[Coherence for monoidal categories]
The following pentagon and triangle diagrams are conditions in the definition of a monoidal category. When they hold, all composites of associators and unitors (and their inverses) are isomorphisms \citep{maclane_natural_1963,benabou_algebre_1964}.
% https://q.uiver.app/?q=WzAsNSxbMSwwLCIoKFcgXFxvdGltZXMgWCkgXFxvdGltZXMgKFkgXFxvdGltZXMgWikpIl0sWzAsMSwiKFcgXFxvdGltZXMgKFggXFxvdGltZXMgKFkgXFxvdGltZXMgWikpKSJdLFsxLDIsIigoKFcgXFxvdGltZXMgWCkgXFxvdGltZXMgWSkgXFxvdGltZXMgWikiXSxbMCwzLCIoVyBcXG90aW1lcyAoKFggXFxvdGltZXMgWSkgXFxvdGltZXMgWikpIl0sWzEsNCwiKChXIFxcb3RpbWVzIChYIFxcb3RpbWVzIFkpKSBcXG90aW1lcyBaKSJdLFsxLDAsIlxcYWxwaGEiLDFdLFswLDIsIlxcYWxwaGEiLDFdLFs0LDIsIlxcYWxwaGEgXFxvdGltZXMgMSIsMV0sWzMsNCwiXFxhbHBoYSIsMV0sWzEsMywiMSBcXG90aW1lcyBcXGFscGhhIiwxXV0=
\[\begin{tikzcd}[ampersand replacement=\&]
	\& {((W \otimes X) \otimes (Y \otimes Z))} \\
	{(W \otimes (X \otimes (Y \otimes Z)))} \\
	\& {(((W \otimes X) \otimes Y) \otimes Z)} \\
	{(W \otimes ((X \otimes Y) \otimes Z))} \\
	\& {((W \otimes (X \otimes Y)) \otimes Z)}
	\arrow["\alpha"{description}, from=2-1, to=1-2]
	\arrow["\alpha"{description}, from=1-2, to=3-2]
	\arrow["{\alpha \otimes 1}"{description}, from=5-2, to=3-2]
	\arrow["\alpha"{description}, from=4-1, to=5-2]
	\arrow["{1 \otimes \alpha}"{description}, from=2-1, to=4-1]
\end{tikzcd}\]

% https://q.uiver.app/?q=WzAsMyxbMCwwLCIoWCBcXG90aW1lcyAoSSBcXG90aW1lcyBZKSkiXSxbMSwxLCIoKFggXFxvdGltZXMgSSkgXFxvdGltZXMgWSkiXSxbMCwyLCIoWCBcXG90aW1lcyBZKSJdLFswLDEsIlxcYWxwaGEiLDFdLFswLDIsIjEgXFxvdGltZXMgXFxsYW1iZGEiLDFdLFsxLDIsIlxccmhvIFxcb3RpbWVzIDEiLDFdXQ==
\[\begin{tikzcd}[ampersand replacement=\&]
	{(X \otimes (I \otimes Y))} \\
	\& {((X \otimes I) \otimes Y)} \\
	{(X \otimes Y)}
	\arrow["\alpha"{description}, from=1-1, to=2-2]
	\arrow["{1 \otimes \lambda}"{description}, from=1-1, to=3-1]
	\arrow["{\rho \otimes 1}"{description}, from=2-2, to=3-1]
\end{tikzcd}\]
\end{theorem}
}

\clearmargin

This section seeks to introduce process theories via string diagrams. The margin material will provide the formal mathematics of string diagrams from the bottum-up. The main body develops process theories via string diagrams by example, through which we develop towards a model of linguistic spatial relations -- words like "to the left of" and "between" -- which are a common ground of competence we all possess. Here we only focus on geometric relations between points in two dimensional Euclidean space equipped with the usual notions of metric and distance, providing adequate foundations to follow \citep{wang-mascianica_talking_2021}, in which I demonstrate how text circuits can be obtained from sentences and how such text circuits interpreted in the category of sets and relations \textbf{Rel} provides a semantics for such sentences. This motivates the question of how to express the (arguably more primitive \citep{jean_childs_1967}) linguistic topological concepts -- such as "touching" and "inside" -- the mathematics of which will be in Chapter \ref{sec:concepts}; the reader may skip straight to that chapter after this section if they are uninterested in syntax. We close this section with a brief note on how process theories relate to mathematical foundations and computer science.\\

A \emph{process} is something that transforms some number of input system types to some number of output system types. We depict systems as wires, labelled with their type, and processes as boxes. Unless otherwise specified, we read processes from left to right.
\[\scalebox{0.5}{\tikzfig{proctheory/process}}\]
Processes may compose in parallel, depicted as placing boxes next to each other.
\[\scalebox{0.5}{\tikzfig{proctheory/processpar}}\]
Processes may compose sequentially, depicted as connecting wires of the same type.
\[\scalebox{0.5}{\tikzfig{proctheory/processseq}}\]
In these diagrams only input-output connectivity matters: so we may twist wires and slide boxes along wires to obtain different diagrams that still refer to the same process. So the diagram below is equal to the diagram above.
\[\scalebox{0.5}{\tikzfig{proctheory/processeq}}\]
Some processes have no inputs; we call these \emph{states}. 
\[\scalebox{0.5}{\tikzfig{proctheory/state}}\]
Some processes have no outputs; we call these \emph{tests}.
\[\scalebox{0.5}{\tikzfig{proctheory/test}}\]
A process with no inputs and no outputs is a \emph{number}; the number tells us the outcome of applying tests to a composite of states modified by processes.
\[\scalebox{0.5}{\tikzfig{proctheory/number}}\]

A process theory is given by the following data:
\begin{itemize}
    \item A collection of systems
    \item A collection of processes along with their input and output systems
    \item A methodology to compose systems and processes sequentially and in parallel, and a specification of the unit of parallel composition.
    \item A collection of equations between composite processes
\end{itemize}

\begin{example}[Linear maps with direct sum]
Systems are finite-dimensional vector spaces over $\mathbb{R}$. Processes are linear maps, expressed as matrices with entries in $\mathbb{R}$.\\
Sequential composition is matrix multiplication. Parallel composition of systems is the direct sum of vector spaces $\oplus$. The parallel composition of matrices $\mathbf{A}, \mathbf{B}$ is the block-diagonal matrix
$$\begin{bmatrix}
\mathbf{A} & \mathbf{0} \\
\mathbf{0} & \mathbf{B}
\end{bmatrix}$$
The unit of parallel composition is the singleton 0-dimensional vector space.
States are row vectors. Tests are column vectors. The numbers are $\mathbb{R}$. Usually the monoidal product is written with the symbol $\otimes$, which clashes with notation for the hadamard product for linear maps, while the process theory we have just described takes the direct sum $\oplus$ to be the monoidal product.
\end{example}

\begin{example}[Sets and functions with cartesian product]
Systems are sets $A,B$. Processes are functions between sets $f: A \rightarrow B$. Sequential composition is function composition. Parallel composition of systems is the cartesian product of sets: the set of ordered pairs of two sets.
\[A \otimes B = A \times B := \{(a,b) \ | \ a \in A, b \in B\}\]
The parallel composition $f \otimes g : A \times C \rightarrow B \times D$ of functions $f: A \rightarrow B$ and $g: C \rightarrow D$ is defined:
\[f \otimes g := (a,c) \mapsto (f(a),g(c))\]
The unit of parallel composition is the singleton set $\{\star\}$. There are many singletons, but this presents no problem for the later formal definition because they are all equivalent up to unique isomorphism. States of a set $A$ correspond to elements $a \in A$ --  we forgo the usual categorical definition of points from the terminal object in favour of generalised points from the monoidal perspective. Every system $A$ has only one test $a \mapsto \star$; this is since the singleton is terminal in \textbf{Set}. So there is only one number.
\end{example}

\begin{example}[Sets and relations with cartesian product]
Systems are sets $A,B$. Processes are relations between sets $\Phi \subseteq A \times B$, which we may write in either direction $\Phi^*: A \nrightarrow B$ or $\Phi_*: B \nrightarrow A$. Relations between sets are equivalently matrices with entries from the boolean semiring. Relation composition is matrix multiplication with the boolean semiring. $\Phi^*,\Phi_*$ are the transposes of one another. Sequential composition is relation composition:
\[A \overset{\Phi}{\nrightarrow} B \overset{\Psi}{\nrightarrow} C := \{(a,c) \ | \  a \in A, \ c \in C, \ \exists b_{\in B}: (a,b) \in \Phi \wedge (b,c) \in \Psi  \}\]
Parallel composition of systems is the cartesian product of sets. The parallel composition of relations $A \otimes C \overset{\Phi \otimes \Psi}{\nrightarrow} B \otimes D$ of relations $A \overset{\Phi}{\nrightarrow} B$ and $C \overset{\Psi}{\nrightarrow} D$ is defined:
\[\Phi \otimes \Psi := \{\big( (a,c) , (b,d) \big) \ | \ (a,b) \in \Phi \wedge (c,d) \in \Psi\}\]
The unit of parallel composition is the singleton. States and tests of a set $A$ are subsets of $A$.
\end{example}

\clearpage

\subsection{What does it mean to copy and delete?}

Now we discuss how we might define the properties and behaviour of processes by positing equations between diagrams. Let's begin simply with two intuitive processes \emph{copy} and \emph{delete}:
\[\tikzfig{proctheory/copydelete}\]

\begin{example}[Linear maps]
Consider a vector space $\mathbf{V}$, which we assume includes a choice of basis. The copy map is the rectangular matrix made of two identity matrices:
\[\Delta_\mathbf{V}: \mathbf{V} \rightarrow \mathbf{V} \oplus \mathbf{V} := \begin{bmatrix} \mathbf{1}_\mathbf{V} & \mathbf{1}_\mathbf{V} \end{bmatrix}\]
The delete map is the column vector of 1s:
\[\epsilon_\mathbf{V}: \mathbf{V} \rightarrow \mathbf{0} := \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}\]
\end{example}

\begin{example}[Sets and functions]
Consider a set $A$. The copy function is defined:
\[\Delta_A : A \rightarrow A \times A := a \mapsto (a,a)\]
The delete function is defined:
\[\epsilon_A : A \rightarrow \{\star\} := a \mapsto \star\]
\end{example}

\begin{example}[Sets and relations]\label{relcopy}
Consider a set $A$. The copy relation is defined:
\[\Delta_A : A \nrightarrow A \times A := \{\big(a , (a,a) \big) \ | \ a \in A\}\]
The delete relation is defined:
\[\epsilon_A : A \nrightarrow \{\star\} := \{(a,\star) \ | \ a \in A\}\]
\end{example}

We may verify that, no matter the concrete interpretation of the diagram in terms of linear maps, functions or relations, the following equations characterise a cocommutative comonoid internal to a monoidal category.
\[\resizebox{\textwidth}{!}{\tikzfig{bestiary/basicrelations_old}}\]

It is worth pausing here to think about how one might characterise the process of copying in words; it is challenging to do so for such an intuitive process. The diagrammatic equations, when translated into prose, provide an answer.
\begin{description}
\item[\textbf{Coassociativity}:] says there is no difference between copying copies.
\item[\textbf{Cocommutativity}:] says there is no difference between the outputs of a copy process.
\item[\textbf{Counitality}:] says that if a copy is made and one of the copies is deleted, the remaining copy is the same as the original.
\end{description}

Insofar as we think this is an acceptable characterisation of copying, rather than specify concretely what a copy and delete does for each system $X$ we encounter, we can instead posit that so long as we have processes $\Delta_X: X \otimes X \rightarrow X$ and $\epsilon_X: X \rightarrow I$ that obey all the equational constraints above, $\Delta_X$ and $\epsilon_X$ are as good as a copy and delete.

\begin{example}[Not all states are copyable]\label{ex:copyablestate}
Call a state \emph{copyable} when it satisfies the following diagrammatic equation:
\[\tikzfig{proctheory/copyable}\]
In the process theory of sets and functions, all states are copyable. Not all states are copyable in the process theories of sets and relations and of linear maps. For example, consider the two element set $\mathbb{B} := \{0,1\}$, and let $\top : \{\star\} \nrightarrow \mathbb{B} := \{(\star,0),(\star,1)\} \simeq \{0,1\}$. Consider the composite of $\top$ with the copy relation:
\[\top;\Delta_{\mathbb{B}} := \{\big(\star,(0,0)\big),\big(\star,(1,1)\big)\} \simeq \{(0,0),(1,1)\}\]
This is a perfectly correlated bipartite state, and it is not equal to $\{0,1\} \times \{0,1\}$, so $\top$ is not copyable.
\end{example}


\begin{remark}\label{ft:determinism}
The copyability of states is a special case of a more general form of interaction with the copy relation:
\[\scalebox{0.75}{\tikzfig{proctheory/copyablefunc}}\]
A cyan map that satisfies this equation is said to be a homomorphism with respect to the commutative comonoid. In the process theory of relations, those relations that satisfy this equation are precisely the functions; in other words, this diagrammatic equation expresses \emph{determinism}.
\end{remark}

Here is an unexpected consequence. Suppose we insist that \emph{to copy} in principle also implies the ability to copy \emph{anything} -- arbitrary states. From Example \ref{ex:copyablestate} and Remark \ref{ft:determinism}, we know that this demand is incompatible with certain process theories. In particular, this demand would constrain a process theory of sets and relations to a subtheory of sets and functions. The moral here is that process theories are flexible enough to meet ontological needs. A classical computer scientist who works with perfectly copyable data and processes might demand universal copying along with the commutative comonoid equations, whereas a quantum physicist who wishes to distinguish between copyable classical data and non-copyable quantum data might taxonomise copy and delete as a special case of a more generic quasi-copy and quasi-delete that is only a commutative comonoid. In fact, quantum physicists \emph{do} do this; see Dodo: \citep{coecke_picturing_2017}.

\subsection{What is an update?}\label{ss:update}

In the previous section we have seen how we can start with concrete examples of copying in distinct process theories, and obtain a generic characterisation of copying by finding diagrammatic equations copying satisfies in each concrete case. In this section, we show how to go in the opposite direction: we start by positing diagrammatic equations that characterise the operational behaviour of a particular process -- such as \emph{updating} -- and it will turn out that any concrete process that satisfies the equational constraints we set out will \emph{by our own definition} be an update.\\

Perhaps the most familiar setting for an update is a database. In a database, an \bM entry\e often takes the form of pairs of \bB fields\e and \bO values\e. For example, where a database contains information about employees, a typical entry might look like:
\[\texttt{\bM < \bB\textbf{NAME}\e:\bO Jono Doe\e, \bB\textbf{AGE}\e:\bO 69\e, \bB\textbf{JOB}\e:\bO CONTENT CREATOR\e, \bB\textbf{SALARY}\e:\bO\$420\e, ... >\e}\]
There are all kinds of reasons one might wish to update the value of a field: Jono might legally change their name, a year might pass and Jono's age must be incremented, Jono might be promoted or demoted or get a raise and so on. It was the concern of database theorists to formalise and axiomatise the notion of updating the value of a field independently of the specific programming language implementation of a database. The problem is reducible to axiomatising a \emph{rewrite}: we can think of updating a value as first calculating the new value, then \emph{putting} the new value in place of the old. Since often the new value depends in some way on the old value, we also need a procedure to \emph{get} the current value. That was a flash-prehistory of \emph{bidirectional transformations} \citep{czarnecki_bidirectional_2009}, which then met applied category theory in e.g. \citep{gibbons_relating_2012}. Following the monoidal generalisation of lenses in \citep{wilson_safari_2021,hefford_categories_2020}, a rewrite as we have described above is specified by system diagrammatic equations in the margin, each of which we introduce in prose.

\clearpage

\marginnote[3cm]{
\begin{defn}[Symmetric Monoidal Category]
A symmetric monoidal category is a monoidal category with an additional natural isomorphism
\[\theta: - \ \otimes = \ \mapsto \ = \otimes \ -\] 
Which satisfies the following pair of hexagons \citep{nlab_authors_symmetric_nodate}.

\[\begin{tikzcd}[ampersand replacement=\&]
    {(X \otimes (Y \otimes Z))} \& {(Z \otimes (X \otimes Y))} \\
    {(X \otimes (Z \otimes Y))} \& {((Z \otimes X) \otimes Y)} \\
    {(X \otimes (Z \otimes Y))} \& {((X \otimes Z) \otimes Y)}
    \arrow["\theta", from=1-1, to=1-2]
    \arrow["\alpha", from=1-2, to=2-2]
    \arrow["{\theta^{-1}}", from=2-2, to=3-2]
    \arrow["{\alpha^{-1}}"', from=1-1, to=2-1]
    \arrow["{1 \otimes \theta}"', from=2-1, to=3-1]
    \arrow["\alpha"', from=3-1, to=3-2]
\end{tikzcd}\]

\[\begin{tikzcd}[ampersand replacement=\&]
    {(X \otimes (Y \otimes Z))} \& {((Y \otimes Z) \otimes X)} \\
    {((X \otimes Y) \otimes Z)} \& {(Y \otimes (Z \otimes X))} \\
    {((Y \otimes X) \otimes Z)} \& {(Z \otimes (X \otimes Y))}
    \arrow["\theta", from=1-1, to=1-2]
    \arrow["{\alpha^{-1}}", from=1-2, to=2-2]
    \arrow["{1\otimes \theta}", from=2-2, to=3-2]
    \arrow["{\alpha^{-1}}"', from=3-1, to=3-2]
    \arrow["{\theta^{-1}}"', from=2-1, to=3-1]
    \arrow["\alpha"', from=1-1, to=2-1]
\end{tikzcd}\]

\end{defn}
}

\begin{description}
\item[\textbf{PutPut}:] Putting in one value and then a second is the same as deleting the first value and just putting in the second.
\[\scalebox{1}{\tikzfig{proctheory/putputs}}\]
\item[\textbf{GetPut}:] Getting a value from a field and putting it back in is the same as not doing anything.
\[\scalebox{1}{\tikzfig{proctheory/getput}}\]
\item[\textbf{PutGet}:] Putting in a value and getting a value from the field is the same as first copying the value, putting in one copy and keeping the second.
\[\scalebox{1}{\tikzfig{proctheory/putget}}\]
\item[\textbf{GetGet}:] Getting a value from a field twice is the same as getting the value once and copying it.
\[\scalebox{1}{\tikzfig{proctheory/getget}}\]
\end{description}

\clearpage

\marginnote[2cm]{
Coherence is about getting rid of syntactic bureaucracy. Addition for example is a commutative monoid, which satisfies equations that let us forget about bracketings, zeroes, and the ordering of summation.
\[(x + (y + z)) = ((x + y) + z)\]
\[x + 0 = x = 0 + x\]
\[x + y = y + x\]
Now the situation is that we have replaced the associativity equation with associator natural transformations, unit equations with unitors (and commutation with natural isomorphisms $\theta$ that are depicted as twisting wires in string diagrams for symmetric monoidal categories.)
\[((X \otimes Y) \otimes Z) \overset{\alpha_{XYZ}}{\rightarrow} (X \otimes (Y \otimes Z))\]
\[(X \otimes I) \overset{\rho_X}{\rightarrow} X \overset{\lambda_X}\rightarrow (I \otimes X)\]
Recalling that we're happy with isomorphism in place of equality, coherence conditions ask that every possible composite of these structural operations is an isomorphism. In terms of the graphical language for monoidal categories, this means that string diagrams up-to-(processive)-planar-isotopy (and connectivity of wires in the case of symmetric monoidal categories) represent equivalent-up-to-isomorphism morphisms in an appropriate monoidal category. The reader is referred to \citep{selinger_survey_2010,joyal_geometry_1991,joyal_geometry_nodate} for details.
}

These diagrammatic equations do two things. First, they completely specify what it means to get and put values in a field in an implementation independent manner; it doesn't matter whether database entries are encoded as bitstrings, qubits, slips or paper or anything else, what matters is the interaction of get and put. Second, the diagrammatic equations give us the right to call our processes \emph{get} and \emph{put} in the first place: we define what it means to get and put by outlining the mutual interactions of get, put, copy, and delete. These two points are worth condensing and rephrasing:
\[
\textbf{A \emph{kind of process} is determined by patterns of interaction with other kinds of processes.}
\]

Now we can diagrammatically depict the process of updating Jono's age, by \bB getting\e Jono's \bO age value\e from their \bM entry\e, incrementing it by 1, and \bB putting\e it back in.

\[\tikzfig{proctheory/incrementage}\]

\newthought{But what are the \emph{things} that the processes operate on?}

This is a common objection from philosophers who want their ontologies tidy. The claim roughly goes that you can't really reason about processes without knowing the underlying objects that participate, and since set theory is the only way we know how to spell out objects intensionally in this way, we should stick to sets. In simpler terms, if we're drawing (black)-boxes in our diagrams, how will we know what they do to the elements of the underlying sets?\\

The short answer is that -- perhaps surprisingly -- reasoning process-theoretically is mathematically equivalent to reasoning about sets and elements for all practical purposes; it is as if whatever is going on \emph{out there} is indifferent to whether we describe using a language of only nouns or only verbs.\\

In the case of set theory (the practical kind, not the one with crazy infinities), let's suppose that instead of encoding functions as sets, we treat functions as primitive, so that we have a process theory where wires are labelled with sets, and functions are process boxes that we draw. The problem we face now is that it is not immediately clear how we would access the elements of any set using only the diagrammatic language. The solution is the observation that the elements $\{x \ | \ X\}$ of a set $X$ are in bijective correspondence with the functions from a singleton into $X$: $\{ f(\star) \mapsto x \ | \ \{\star\} \overset{f}{\rightarrow} X $. In prose, for any element $x$ in a set $X$, we can find a function that behaves as a pointer to that element $\{\star\} \rightarrow X$. So the states we have been drawing, when interpreted in the category of sets and function, are precisely elements of the sets that label their output wires.

\newthought{But if they're expressively the same, what's the point?}

The following rebuttal draws on Harold Abelson's introductory lecture to computer science \citep{harold_abelson_lecture_2019} (in which string diagrams appear to introduce programs without being explicitly named as such). There is a distinction between declarative and imperative knowledge. Declarative knowledge is \emph{knowing-that}, for example, 6 is the square root of 36, which we might write $6 = \sqrt{36}$. Imperative knowledge is \emph{knowing-how}, for example, to obtain the square root of a positive number, for instance, by Heron's iterative method: to obtain the square root of $Y$, make a guess $X$, and take the average of $X$ and $\frac{Y}{X}$ until your guess is good enough.\\

Computer science concerns imperative knowledge. An obstacle to the study of imperative knowledge is complexity, which computer scientists manage by black-box abstraction -- suppressing irrelevant details, so that for instance once a square root procedure is defined, the reasoner outside the system does not need to know whether the procedure inside is an iterative method by Heron or Newton, only that it works and has certain properties. These black-boxes can be then composed into larger processes and procedures within human cognitive load.\\

Abstraction also yields generality. For example, in the case of addition, it is not only numbers we may care to add, but perhaps vectors, or the waveforms of signals. So there is an abstract notion of addition which we concretely instantiate for different domains that share a common interface; we may decide for example that all binary operations that are commutative monoids are valid candidates for what it means to be an addition operation.\\

In this light, string diagrams are a natural metalanguage for the study of imperative knowledge; string diagrams in fact independently evolved within computer science from flowcharts describing processes. Process theories, which are equations or logical sentences about processes, allow us to reason declaratively about imperative knowledge. Moreover, string diagrams as syntactic objects can be interpreted in various concrete settings, so that the same diagram serves as the common interface for a process like addition, with compliant implementation details for each particular domain spelled out separately.\\

\newthought{Now we define the one structure we really care about} Spiders -- or special frobenius algebras (introduced in \citep{paquette_categorical_2008}) -- will be the master structure we work with.

\clearpage

\begin{myboxB}
\begin{defn}[Spiders]
I will describe the behaviour of a spider as a PROP \citep{nlab_authors_prop_nodate}, which as far as we care is just a way to list out processes and equations that their composites satisfy in a symmetric monoidal category. We say that an object is \emph{equipped} with a spider when it has the following processes:
\[\scalebox{0.5}{\tikzfig{proctheory/spiders0}}\]
That satisfy the following equations:
\[\resizebox{\textwidth}{!}{\tikzfig{proctheory/spiders1}}\]
Spiders imply strong compact closure; self-dual cups and caps may be viewed as composite spiders. These are the cups which we use in pregroup diagrams to come. The equations for cups and caps are derived from spider rules, taking advantage of equality up to connectivity of wires.
\[\resizebox{\textwidth}{!}{\tikzfig{proctheory/strongcompactclosed}}\]
\end{defn}
\end{myboxB}

\begin{myboxB}
\begin{term}
We call a category in which every object has a spider a \emph{hypergraph category} \citep{fong_hypergraph_2018}. 
\end{term}

\begin{remark}[Spiders are easy]
All connected configurations of spiders with the same number of inputs and outputs are equal. Intuitively, whereas wires connect end-to-end in string diagrams, spiders give us \emph{multiwires} that we may freely split and connect.
\[\tikzfig{proctheory/multiwires}\]
\end{remark}

\begin{remark}[Only connectivity matters]
In hypergraph categories, by recovery of strong compact closure, we may freely reason without a convention for reading direction of processes. By the multiwire property, all diagrams with the same connectivity are equal. Hence, \emph{only connectivity matters}.
\end{remark}
\end{myboxB}

\clearpage


