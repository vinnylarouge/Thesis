\begin{fullwidth}

\section{(Im)possibility results for learning text circuits from data}

So far, we have been working process-theoretically, using equations between processes to specify their behaviour. It is natural to ask whether it is possible to realise each process in a process theory as a neural net, using the equations as training criteria so that the neural nets jointly model a process theory. This approach is worth pursuing to combine the benefits and ease of data-driven learning with the modularity and explainability benefits of process theories. Moreover, the onus is on us to demonstrate that text circuits can be learnt in this way, or else we would be no better off in terms of a practical theory of language for the age of big data.\\

In this sketch, we briefly introduce neural nets diagrammatically, along with the \emph{Universal Approximation Theorem}, which, along with variants for different architectures, states that for any dimension $m$ and any $\epsilon > 0$, there exists a neural net that approximates any continuous function $\mathbb{R}^m \rightarrow \mathbb{R}$ on a compact subset of the domain $\mathbb{R}^m$ within a discrepancy of $\epsilon$. Then we introduce the notion of approximability for PROPs, and we observe that not all PROPs are approximable in terms of smooth functions of the form given by the universal approximation theorem. So we restrict our attention to PROPs for basic text circuits, which we demonstrate are suitable for certain learning tasks. We prove that basic text circuit PROPs of bounded depth and width -- a notion we will define -- are approximable; in other words, that text circuits work in principle alongside data-driven techniques. We close with a discussion of limitations and extensions. We give a corollary that finitely generated subcategories of \textbf{FinSet} are realisable as ensembles of deterministic neural nets, and we show how introducing probabilistic states extends the situation to \textbf{FinRel}. We formalise an observed tension between the space-resource demands of deterministic representations and unbounded compositionality by a no-go conjecture.

\subsection{Approximating Text Circuits with deterministic neural nets}

There is a lot to be gained from a process-theoretic view of interacting ensembles of neural nets. For a simple example, consider that an autoencoder is precisely a pair of neural nets trained cooperatively encode a large input space into a small latent space and decode the original input from the latent space. Diagrammatically, this amounts to asking for the equations of a split idempotent to be treated as training conditions for a pair of processes.

\[autoencoder\]

If that's what we can do with a pair of equations, what can we do with an arbitrary PROP? We first need to decide what qualifies as a valid interpretation the generators of a PROP in terms of neural nets. Not just any functor will do, because we want to rule out trivial solutions that map all processes to constant functions. We also need to put in some work to interpret what equality of processes should mean in the setting of neural nets.

\begin{defn}[Approximating a (coloured) PROP]
An $(\epsilon^{=},\epsilon^{\neq})$-approximation of a finitely presented coloured PROP $\mathfrak{P}$ is a strict symmetric monoidal functor $\mathcal{T}$ that interprets $\mathfrak{P}$ in the (cartesian) symmetric monoidal subcategory of \textbf{Top} generated by Euclidean spaces with the usual metric as wires equipped with cartesian copy and delete, along with neural nets as processes. As a nontriviality condition, $\mathcal{T}$ must send each wire colour in $\mathfrak{P}$ to a Euclidean space of finite positive dimension. Equality relations presented in $\mathfrak{P}$ are interpreted as $\epsilon^{=}$-closeness by $\mathcal{T}$, i.e. if $\mathfrak{P}$ stipulates that $f = g$ for $f,g: A \rightarrow B$, then we have the following inequality in the metric of $\mathcal{T}B$:
\[\forall \mathbf{x}_{\in\mathcal{T}A} : d_{\mathcal{T}B}\big(\mathcal{T}f(\mathbf{x}),\mathcal{T}g(\mathbf{x})\big) \leq \epsilon^{=}\]
Any PROP that equates generators directly is redundant, and we can without loss of generality restrict consideration to PROPs where each generator is implicitly assumed to be distinct. We interpret inequality as $\epsilon^{\neq}$ farness, i.e., for all pairs of generators $f,g$ of the same type $A \rightarrow B$, we ask that $\mathcal{T}$ satisfies:
\[\exists \mathbf{y}_{\in\mathcal{T}A} : d_{\mathcal{T}B}\big(\mathcal{T}f(\mathbf{y}),\mathcal{T}g(\mathbf{y})\big) \geq \epsilon^{\neq} \]
\end{defn}

Since the category is cartesian monoidal, states are points in euclidean space, and the above definition specialise to treating points as "equal" if they are $\epsilon^{=}$-close and "inequal" if they are $\epsilon^{\neq}$-far. We choose to treat the determination of equality and inequality as separate semidecidable procedures because "equality" as we have defined it is not necessarily transitive, but we can recover a form of bounded transitivity by making $\epsilon^{=}$ very small compared to $\epsilon^{\neq}$, so that equality is testable within a tolerance of $\epsilon^{\neq}$, granting $\frac{\epsilon^{\neq}}{\epsilon^{=}}$-fold transitivity. We can always recover decidable "equality" at the expense of transitivity by setting $\epsilon^{=} = \epsilon^{\neq}$. With that out of the way, we observe that since the target category of deterministic neural nets is cartesian monoidal, not all PROPs are approximable.

\begin{example}[Not all PROPs are approximable]
We take the snake equations as an example. The PROP generating the snake equation is as follows:
\[snakeprop\]
Since we are dealing with a cartesian monoidal category, the cup can only be interpreted as a pair of points, and the cap can only be a pair of deletes []. The only Euclidean space in which the identity is equal to a constant map is the singleton zero-dimensional space.
\[triviality proof\]
\end{example}

So nondeterminism is a necessary but possibly insufficient condition for the realisation of general PROPs. Not all is lost; if we restrict our consideration to well-behaved PROPs, such as those of simple text circuits, then we can get somewhere eventually.

\begin{defn}[Basic Text Circuit PROP]
A \emph{basic text circuit PROP} has two colours of wires, $N$ for "noun" and $A$ for "answer". The generators fall under four main families. \emph{Nouns} have type $1 \rightarrow N$. \emph{Gates} have type $\bigotimes^k N \rightarrow \bigotimes^k N$ for some positive $k$. \emph{Queries} have type $\bigotimes^k N \rightarrow A$ for some positive $k$. \emph{Answers} have type $1 \rightarrow A$.
\[states, gates, queries, answers\]
The relations of a text circuit fall under three families. \emph{Axioms} are equations between pairs of nonempty composites of gates; the only kind we disallow is an equality between two generators.
\[axiom\]
\emph{Instances} are equations between a composite of nouns and gates and a single query on the left -- a \emph{datum} -- and an answer on the right -- a \emph{label}.
\[instance\]
In addition, we ask for a special generator with a non-finite family of rules to enforce a coherence condition; we decide that distinct nouns should maintain their identity no matter what relations they participate in. The generator is \emph{Name}, of type $N \rightarrow N$, and its relations are such that applying \emph{Name} to any noun-wire that traces back to a noun will return that noun.
\[name\]
\end{defn}

\begin{example}[How basic text circuits PROPs may be used in practice]

\end{example}

A limitation that arises from the interaction of the definition of approximability and the universal approximation theorem is that any compact subset of euclidean space can be covered by finitely many $\epsilon$-balls. This means that the universal approximation theorem is unable to provide us guarantees if we want potentially unboundedly large text circuits, but we might reasonably expect compositional behaviour up to some notion of text circuits with bounded width and depth, which we state as follows.

\begin{defn}[Bounded width and depth]
We say that a basic text circuit PROP $\mathfrak{T}$ is \emph{terminating} if all of its axiom relations can be equipped with directions so that applying directed equality rewrites to any diagram (necessarily finite) yields a finite set of equal diagrams. As an easy example, a basic text circuit PROP with a single idempotent gate is terminating when we equip the idempotence relation with a direction that reduces the number of gates; we don't want to deal with cases where equalities explode to infinity.
\[idempotentreduce\]
Observe that if $\mathfrak{T}$ is terminating, then applying axioms to the datum of any instance relation will also yield a finite set of equal diagrams, and each instance diagram may be rewritten by isotopies so that parallel gates are displaced so that each gate occupies a distinct level, sandwiched by a layer of nouns and query, which we also count as layers. We say that $\mathfrak{T}$ has \emph{bounded depth} $d \in \mathbb{N}$ if the maximum depth in layers obtained in this way from any datum in $\mathfrak{T}$ is bounded above by $d$. The case of width is simpler, because axioms cannot change the number of wires in a datum, which is the same as the number of nouns; we say that $\mathfrak{T}$ has \emph{bounded width} $w \in \mathbb{N}$ if the maximum number of nouns that occur in any datum is bounded above by $w$.
\end{defn}

Even this is not enough. Another issue arises from the interaction of approximability with the nature of cartesian monoidal categories.

\begin{theorem}[Fox's Theorem]

\end{theorem}

A consequence of Fox's theorem is that in any cartesian monoidal category, we can equip every wire with canonical cocommutative comonoids (copy and delete), and every morphism in a cartesian monoidal category is a cohomomorphism with respect to copy and delete; recall from Section \ref{sec:proctheory} that this is the diagrammatic definition of deterministic maps. This consequence means that for some text circuit PROPs, approximable-equalities may hold in \emph{any} of their interpretations as deterministic neural nets that are not equalities licensed by the original PROP.

\begin{example}

\end{example}

The deeper essence of this issue is that in a cartesian monoidal category, every wire can at most carry data about its diagrammatic causal past, since equalities of the following sort always hold.

\[placeholder\]

This conflicts with the nature of updating representations with text, where later information may force revisions of earlier representations. For a crude example, consider this transcription of a meme about a morning routine:

\[\texttt{Wake up. Take a shit. Eat. Get out of bed. Have breakfast.}\]

Now I will kill the joke by overthinking it. What happens in our mind's eye if we are constructing a little vignette of events? In this example, the first three clauses take a pedestrian interpretation -- \texttt{Taking a shit} normally happens in toilets, and the inferred object of \texttt{Eat} is breakfast. Clauses four and five require belief revisions. The internal representation of the sequence of events up to those points must be retroactively changed in a manner that is not representable as gates updating entities locally:

\[cartoon\]

So in the deterministic setting, after every local update, we require a \emph{global} coordination gate that gives all representations access to each other's data and a chance for them to revise themselves. To summarise our restrictions so far, we are only dealing with text circuit PROPs, of bounded depth to remain within the guarantees of the universal approximation theorem, and of fixed width as a diagrammatic consequence of having a single global coordinator gate.

\begin{theorem}[Basic Text Circuit PROPs of bounded depth and fixed width are approximable using a global coordinator]
\begin{proof}

We sketch a construction. The essential idea is to have each noun represent all of the gates it has passed through. The representation is (the monoid analogue of) a Cayley graph of a free group in $\mathbb{R}^{|\mathbf{G}|}$, where if $w \in \mathbb{N}$ is the fixed width, $\mathbf{G}$ is the set of gates where every wire but one is assigned: $\{ g(i,j,\cdots,-,\cdots,k) \ | \ g \in \text{Gates} \ , \ 1 \leq i,j,\cdots k \leq w\}$. In the illustration below, we can for instance record that a noun wire has passed through a unary gate $\textcolor{magenta}{h}$ as the only argument, or that it has passed through three gates, first a binary $\textcolor{cyan}{f}$ gate as the first argument, sharing the gate with the third wire, then a binary $\textcolor{orange}{g}$ gate as the second argument, sharing the gate with the first wire, and then a unary $\textcolor{magenta}{h}$ gate. Each distinct generator has its own dimension in Euclidean space.

\[\scalebox{0.5}{\tikzfig{learnable/freetree}}\]

By asking for each node to be an $\epsilon$-ball, we can test for equality by a spike function.

\[\scalebox{1}{\tikzfig{learnable/spike}}\]

We don't want equal steps in each direction in the graph, or else all pairs of steps would commute. To reflect the order in which steps occur, we want subsequent steps to be smaller than earlier ones. So for each colour of node in the graph, we add the value of a move function in the desired dimension, parameterised by the initial position of the noun, so that each subsequent step is half the distance of the previous one. To make life easier, we have each move function use the taxicab metric. By bounded depth, there is some $d \in \mathbb{N}$ after which we don't need to carry representations, so we modify the ends of the move function to hit zero early. Setting $R >> 2^d \epsilon$, each move function acts to translate all graph nodes within an $R$-ball to their destination graph nodes after uniformly applying some generator. We ask that each wire is initialised at least $2R$-far apart, so each wire has their own $R$-ball neighbourhood in which to encode data as a graph. 

\[\scalebox{0.5}{\tikzfig{learnable/shift}}\]

Now we can jointly specify representations of nouns and gates. For example, suppose $f$ is a binary gate, and that $w=3$. Then there are six shift functions $$\mathbf{M}_f(-,1),\mathbf{M}_f(-,2),\mathbf{M}_f(-,3),\mathbf{M}_f(1,-),\mathbf{M}_f(2,-),\mathbf{M}_f(3,-)$$ Set an ansatz dimension $\mathcal{A}$ aside, which distinguishes noun wires by indicator-spike functions $\mathbf{1}_{1},\mathbf{1}_{2},\mathbf{1}_{3}$ -- a continuous analog of one-hot encoding -- then let $f$ be:

\[f(x,y) := \begin{cases}
x \mapsto x + \sum\limits^{w}_{i^x,i^y =1}\big(\mathbf{1}_{i^x}(x)\mathbf{1}_{i^y}(y) \mathbf{M}_f(-,i^y)  \big) \\
y \mapsto y + \sum\limits^{w}_{i^x,i^y =1}\big(\mathbf{1}_{i^x}(x)\mathbf{1}_{i^y}(y) \mathbf{M}_f(i^x,-)  \big)
\end{cases}
\]

In prose, the coefficients obtained by the indicator functions of each summand behave as a case guard, identifying the wire-numbers of $x,y$ by sending all other cases to zero. This generalises to representations of gates with higher arity. At this point, we have obtained representations for a free theory of just the gates of bounded-depth and fixed-width text circuits -- we have not yet incorporated the potential equalities of the theory. First we deal with the axiom-type equalities. Here is where we make use of a global coordinator gate; where width is $w$ and each noun is represented in some space $\mathbb{R}^J$, the global coordinator is typed $\prod\limits^w \mathbb{R}^J \rightarrow \prod\limits^w \mathbb{R}^J$. The global coordinator looks at the local data recorded by each wire, and when it spots an equality in the theory, it pinches free graph nodes together. The pinch function is a modification of the spike and shift functions, moving only points within a tolerance $\beta$ into a target $\epsilon$-ball.

\[\scalebox{0.5}{\tikzfig{learnable/pinch}}\]

The global coordinator waits to see if the product state of all the wires satisfies an equality, and when it does, it pinches the agreeing free representations on each wire to a new node along a new dimension -- we ask for one dimension for each equivalence class of expressions quotiented by equality in the theory, and here the bounded-depth and fixed-width conditions mean that we only have to add finitely more dimensions in this way.

\[\scalebox{0.5}{\tikzfig{learnable/theorypinch}}\]

The target location of each pinch can be chosen to be compatible with the shift functions that generate the graphs -- i.e. each equality-class is treated as if it were its own generator with its own shift function. The global coordinator is the sum of all such pinches, and the way to use it is to sandwich it between each gate.

\[placeholder\]

Finally, the query morphisms in the instance-type rules of the text circuit PROP are continuous functions that assign each $\epsilon$-ball node a value in a Euclidean answer-space, which can mimick a finite discrete number of cases by spike-indicator functions. All of this is, by construction, an $(\epsilon,R)$-approximation of a text circuit PROP.
\end{proof}
\end{theorem}

\subsection{Text circuits with unbounded depth and width}

For a better understanding of what is possible, we might approach "from above", seeking first a mathematical setting in which we have unrestricted compositionality, and then gradually applying sensible constraints. One such mathematical setting is \textbf{Rel}, which we show can faithfully model any finitely presented coloured PROP.

\begin{defn}[Faithful models and expressive completeness]
Given a coloured prop $\mathfrak{P}$, a symmetric monoidal category $\mathcal{M}$, and a symmetric monoidal functor $T: \mathfrak{P} \rightarrow \mathcal{M}$, we say that $T$ is a \emph{faithful model of} $\mathfrak{P}$ \emph{in} $\mathcal{M}$ if, for all diagrams $f,g \in \mathfrak{P}$, $f =_\mathfrak{P} g \iff T(f) =_\mathcal{M} T(g)$. Given a collection of props $\{\mathfrak{P}\}$, we say that $\mathcal{M}$ is \emph{expressively complete for} $\{\mathfrak{P}_i\}$ when each $\mathfrak{P}_i$ has a faithful model in $\mathcal{M}$.
\end{defn}

\begin{theorem}[$\textbf{Rel}^\times$ is expressively complete for all finitely presented coloured PROPs]
\begin{proof}
Our strategy has two main ideas. First is to take the Lindenbaum-Tarski algebra of diagrams for an arbitrary $\mathfrak{P}$, quotiented by the equivalence relation $=_\mathfrak{P}$; this will give faithfulness. Second is to treat the sought functor $T$ for $\mathfrak{P}$ as a category of elements construction, adapted to the symmetric monoidal setting. Let $\mathfrak{P}^\star$ denote the finitely presented coloured PROP $\mathfrak{P}$ augmented with new generators that do not obey any relations: a state $\rotatebox[origin=c]{180}{$\multimap$}$ and effect $\multimap$ for each colour $C$ in $\mathfrak{P}$. Let $\mathfrak{P}^\star_{\mathbf{LT}}$ denote the set of diagrams of $\mathfrak{P}^\star$ quotiented by the equivalence relation obtained by equality $=_\mathfrak{P}$ in $\mathfrak{P}$. Let $T(0)$ be the singleton. For each colour $C \in \mathfrak{P}$, let $T(C)$ be the subset of $\mathfrak{P}^\star_{\mathbf{LT}}$ that selects all states on $C$. For each nonempty list of colours $[C_i]$, let $T([C_i])$ be the subset of $\mathfrak{P}^\star_{\mathbf{LT}}$ that selects all states on $C_1 \otimes_\mathfrak{P} C_2 \otimes_\mathfrak{P} \cdots C_i$. For $f: C \rightarrow D$ in $\mathfrak{P}$, let $T(f) := \{(\ulcorner \bra{c} \urcorner, \ulcorner \bra{c};f \urcorner) \ | \ c_{\in T(C)}\}$, where corner notation denotes an equivalence class of states under $=_\mathfrak{P}$; as a particular consequence, for a state $\bra{C}$ on $C$, $T(\bra{c}) = \ulcorner \bra{c} \urcorner \subseteq T(C)$. Note that applying $f$ after any state $\ulcorner \bra{c} \urcorner \in T(C)$ yields a state $\ulcorner \bra{c} ; f \urcorner \in T(D)$, and that these states include actual states native to $\mathfrak{P}$ and formal states from $\mathfrak{P}^\star$ where diagrams from $\mathfrak{P}$ with an output wire typed $C$ have their other wires "capped off" by $\rotatebox[origin=c]{180}{$\multimap$}$ and $\multimap$.\\

$T$ is a functor; $T(1_C) = 1_{T(C)}$ since $\{(\ulcorner \bra{c} \urcorner, \ulcorner \bra{c};1_C \urcorner) \ | \ \ulcorner \bra{c} \urcorner_{\in T(C)}\}$ is the identity relation on $T(C)$; $T(f) \ ;_\textbf{Rel} \ T(g) = T(f \ ;_{\mathfrak{P} \ g)}$ since the relational composite is
$$T(f) \ ;_\textbf{Rel} \ T(g) := \{(\ulcorner \bra{c} \urcorner, \ulcorner \bra{e} \urcorner) \ | \ c_{\in T(C)} \ , \ \exists \ulcorner \bra{d} \urcorner_{\in T(D)} : \ulcorner \bra{c};f \urcorner = \ulcorner \bra{d} \urcorner \& \ulcorner \bra{d};g \urcorner = \ulcorner \bra{e} \urcorner\}$$ We observe that $\ulcorner \bra{d} \urcorner_{\in T(D)} : (\ulcorner \bra{c};f \urcorner = \ulcorner \bra{d} \urcorner \ \& \ \ulcorner \bra{d};g \urcorner = \ulcorner \bra{e} \urcorner)$ implies that $\ulcorner \bra{c};f;g \urcorner = \ulcorner \bra{e} \urcorner$, so we have $T(f \ ;_{\mathfrak{P}} \ g \subseteq T(f) \ ;_\textbf{Rel} \ T(g)$. For the other inclusion, we observe that $\ulcorner (\bra{c};f);g \urcorner$ yields the state $\ulcorner \bra{c};f \urcorner \in T(D)$ in the bracketed expression, thus satisfying the existential quantifier.\\

The unitors, associators, braidings, and coherences are tedious to write but conceptually trivial, so I will skip them. The tricky part of showing that $T$ is monoidal is providing isomorphisms $T(C) \times T(D) \simeq T(C \otimes_\mathfrak{P} D)$. We present them first and comment later.

\[T(C) \times T(D) \rightarrow T(C \otimes_\mathfrak{P} D) := \{ \big( \begin{pmatrix} \ulcorner \bra{c} \urcorner \\ \ulcorner \bra{d} \urcorner \end{pmatrix} \ , \ \begin{cases}\centering \ulcorner \bra{x} \urcorner & \text{if } \exists \bra{x}_{\in T(C \otimes_\mathfrak{P} D)} : \ulcorner \bra{c};\multimap_C \urcorner = \ulcorner \bra{x};(\multimap_C \otimes_\mathfrak{P} \multimap_{D}) \urcorner = \ulcorner \bra{d};\multimap_D \urcorner \\ \ulcorner \bra{c} \otimes_\mathfrak{P} \bra{d} \urcorner & \text{otherwise} \end{cases} \big) \ | \ \ulcorner \bra{c} \urcorner_{\in T(C)}, \ulcorner \bra{d} \urcorner_{\in T(D)} \}\]

\[T(C \otimes_\mathfrak{P} D) \rightarrow T(C) \times T(D) := \{ \ulcorner \bra{x} \urcorner \ , \ \begin{cases} \begin{pmatrix} \ulcorner \bra{c} \urcorner \\ \ulcorner \bra{d} \urcorner \end{pmatrix} & \text{if }\exists \ulcorner \bra{c} \urcorner_{\in T(C)} \ulcorner \bra{d} \urcorner_{\in T(D)}: \ulcorner \bra{x} \urcorner = \ulcorner \bra{c} \ \otimes_\mathfrak{P} \ \bra{d} \urcorner \\ \begin{pmatrix} \ulcorner \bra{x};(1_C \ \otimes_\mathfrak{P} \ \multimap_D) \urcorner \\ \ulcorner \bra{x};(\multimap_C \ \otimes_\mathfrak{P} \ 1_D) \urcorner \end{pmatrix} & \text{otherwise} \end{cases} \big) \ | \ \ulcorner \bra{x} \urcorner_{\in T(C \otimes_\mathfrak{P} D)} \}\]

We walk through the construction case-by-case. For the first case top-to-bottom, if two states $\bra{c},\bra{d}$ are obtainable by formally deleting the $C$ and $D$ outputs (using $\multimap$) of some state $\bra{x}$ on $C \otimes_\mathfrak{P} D$, then we send the pair $(\ulcorner \bra{c} \urcorner,\ulcorner \bra{d} \urcorner)$ to $\ulcorner \bra{x} \urcorner$. Note that in the first case, even if $\bra{x}$ is tensor separable as $\bra{c} \otimes_\mathfrak{P} \bra{d}$, formal deletion creates new formal scalars which must also agree by the case guard. The total effect of the first case is to identify when $(\bra{c},\bra{d})$ arise as partial diagrams (where ends are truncated with $\multimap$) of some state $\bra{x}$. The second case is where $(\bra{c},\bra{d})$ are not partial diagrams of some $\bra{x}$ in this way, in which case we send the pair to their tensor product. The third case is the inverse of the second, sending all $\bra{x}$ that are equivalent to a tensor-separable composite state $\bra{c}$ and $\bra{d}$ to that pair of states. The fourth case is the inverse of the first; if $\bra{x}$ has no tensor-separable equivalent, then we project $\bra{x}$ to states on $C$ and $D$ by formally deleting the appropriate outputs. The effect of taking equivalence classes makes the first relation as a whole an injective function, and likewise for the second relation.\\

All that remains is to show that $T$ is a faithful model, i.e., for all lists of colours $[C],[D]$ and for all $f,g : [C] \rightarrow [D]$, $f =_\mathfrak{P} g \iff T(f) =_\textbf{Rel} T(g)$. For the forward direction $\Rightarrow$, if $f =_\mathfrak{P} g$, then for any state $\bra{x} : 0 \rightarrow [C]$, we have that $\bra{x};f = \bra{x};g$. Because $T(\bra{x}) = \ulcorner \bra{x} \urcorner \in T([C])$, we have $\ulcorner \bra{x};f = \bra{x};g \urcorner \in T([D])$, thus $T(f) =_\textbf{Rel} T(g)$. For the converse direction $\Leftarrow$, if $f \neq_\mathfrak{P} g$, then by the Lindenbaum-Tarski construction and the relational-freeness of the generator $\rotatebox[origin=c]{180}{$\multimap$}$, $\ulcorner \rotatebox[origin=c]{180}{$\multimap$};f \urcorner \neq \ulcorner \rotatebox[origin=c]{180}{$\multimap$};g \urcorner$, so $T(f)$ as a relation contains the pair $(\ulcorner \rotatebox[origin=c]{180}{$\multimap$}_{[C]} \urcorner, \ulcorner \rotatebox[origin=c]{180}{$\multimap$}_{[C]};f \urcorner)$ that is not present in $T(g)$, and symmetrically for $(\ulcorner \rotatebox[origin=c]{180}{$\multimap$}_{[C]} \urcorner, \ulcorner \rotatebox[origin=c]{180}{$\multimap$}_{[C]};g \urcorner)$, thus $T(f) \neq T(g)$.
\end{proof}
\end{theorem}

\begin{corollary}[FinRel will work for bounded-size compositionality]
Just limit consideration to diagrams with upper bounds on the maximal number of generators among diagrams in their equivalence classes, or whatever reasonable finiteness constraint you want, I'm not a cop.
\end{corollary}

\subsection{Text circuits of unbounded width in noncartesian settings}

\newthought{What's special about \textbf{Rel} that distinguishes it from a category of deterministic processes?} For one, \textbf{Rel} is not cartesian monoidal; while the monoidal product in \textbf{Rel} is the categorical product of sets, and every object in \textbf{Rel} has a copy-delete comonoid, not every relation is a cohomomorphism with respect to this comonoid (in fact, only the functions are.) It turns out that this property is what allows the existence of tests that differ from deleting. In a cartesian monoidal category, every test is equal to deleting.

\[placeholder\]

In a noncartesian monoidal category such as \textbf{Rel} or \textbf{Stoch} -- the category of stochastic maps between measureable sets, we may have tests corrresponding to postselection, which greatly increases our expressive capacity for composing constraints. Let's consider an example in \textbf{Stoch}, where states are probability distributions, for example a coinflip as a state on the space $\{H,T\}$. Observe that copying the outcome of a coinflip gives a probability distribution on $\{H,T\} \times \{H,T\}$ that is different from what we obtain by flipping two independent coins, as the first is perfectly correlated and the second is not. So \textbf{Stoch} is not cartesian monoidal, while it does have a copy-delete comonoid for every object.

\[placeholder\]

The scalars in \textbf{Stoch} correspond to probabilities in the range $[0,1]$. Another way we could generate perfectly correlated coinflips is to flip two coins and throw away any outcomes that are not either $H \times H$ or $T \times T$; diagrammatically, this corresponds to copying the outcomes of the two coins and applying a test to one set of copies. Postselecting will yield outcomes will equal in distribution to copying a single coinflip, but only half of the time.

\[placeholder\]

Postselection gates are commutative, so in a circuit made up entirely of postselection gates, the only relevant size measure is the width of the circuit.

\[placeholder\]

Although it is a very stupid method, one way to achieve unbounded-width compositionality using postselection gates is to just continue sampling randomly until you find a sample that passes all the postselection tests. For example, suppose we have a group of people $\{\texttt{Alice}, \texttt{Bob}, \texttt{Claire}, \cdots\}$ standing in a queue modelled as a unit interval, and we have one binary linguistic relation $X \texttt{is-in-front-of} Y$, which postselects with condition $X > Y$ for $X,Y \in [0,1]$. Assume every person's initial state is the uniform random distribution on $[0,1]$. Then we can model the text \texttt{Alice is in front of Bob. Claire is in front of Alice.} as:

\[placeholder\]

Any postselected sample in this case is a satisfying instance of our linguistic positions: an assignment of possible positions in the queue to people such that Alice really is in front of Bob and Claire is really in front of Alice. In this case, we can have any finite number of people and it will remain true that if we manage to obtain a postselected sample, it will be a configuration that satisfies our constraints. Moreover, no globally coordinating gate is required in this example. So by introducing random variables as initial states and postselection, we have shown that it is (in principle, and in some cases) possible to obtain approximations for text circuit PROPs using deterministic neural nets without global coordination gates and without the restriction of fixed width.

\subsection{A value proposition for quantum machine learning}

Since I am from the quantum group in my department, I should probably write something extolling quantum computers, so here is the token gesture. A practicality issue arises for the random sampling approach in the form of postselection, which is the procedure of throwing away samples that do not adhere to the constraints we have set and rerolling the dice. For very specific constraints that jointly independent samples are very unlikely to satisfy, we may have to reroll many times before we obtain a satisfying instance, which may take longer than we are willing to wait. While I am sure there are clever ways to mitigate the problem of postselection on a classical computer beyond my current knowledge, let me suggest a \emph{possibility} to avoid postselection altogether; that is, every measurement will always return a satisfying instance. The main ingredient is a parameterised quantum circuit, which consists of unitary gates controlled by classical parameters; just as a neural net is a classically-parameterised process where the parameters determine weights and biases per neuron, classical parameters can determine the preparation of e.g. phase states within a unitary quantum gate. We only want to traffic in causal quantum processes so we never have to worry about postselection, i.e. the only test we ever want to apply is discarding. So without loss of generality we express each parameterised gate as the following composite by Stinespring dilation.

\[placeholder\]

That is, we deal with gates that consist of classically-parameterised unitaries and a classically-parameterised ansatz state on ancilla wires that we discard. Since the only test we ever apply is discarding, we do not have to postselect. \textbf{If} we can find classical parameters for each gate such that the equations of a text circuit PROP are satisfied, then we we would have a faithful model that would in principle keep the benefit of width-unbounded compositionality as in the probabilistic case, but without the drawbacks of postselection. To be balanced, I should qualify how big of an "if" we are dealing with. There are at least three obstacles that spring to mind. First is practical: the space of parameters for parameterised quantum circuits are not easily differentiable against a loss function unlike their classical counterparts and batching is difficult when dealing with ensembles of parameterised processes in different configurations, so training takes more time. Second is theoretical: there is more work to be done to understand what kinds of text circuit PROPs may be modelled faithfully in principle by quantum and classical gates, and in particular whether bounded-depth compositionality transfers from \textbf{FinRel} to \textbf{FdHilb} with quantum processes. Third is mystical-heuristical: we can approximate that finding a satisfying instance by random sampling on a classical computer takes something like $\mathcal{O}(e^N)$ samples where $N$ is the number of gates, so if we believe in a complexity-theory analogue of "no free lunches", finding appropriate parameters for an ensemble of quantum or classical gates may be around that hard as well.

\newthought{But wouldn't it be nice?}

\end{fullwidth}